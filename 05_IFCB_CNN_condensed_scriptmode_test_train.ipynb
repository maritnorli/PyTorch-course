{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObopxWTs4cFmZcbs2VDOwV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "22552054f5cf4a3aba19df1d6d766f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e82f35ba49e048dd9040c28a24022b19",
              "IPY_MODEL_df0ccf2e74c0453eb77ed4e6bee55295",
              "IPY_MODEL_54e4cd2728df4e2a9824e49130955fc5"
            ],
            "layout": "IPY_MODEL_b89a0df7bb4b40f4802dee5df78c1501"
          }
        },
        "e82f35ba49e048dd9040c28a24022b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c271d5f5c5564cb39c998024737feb84",
            "placeholder": "​",
            "style": "IPY_MODEL_49ea8eae9d424c7d8c0a75ea7986dc90",
            "value": "100%"
          }
        },
        "df0ccf2e74c0453eb77ed4e6bee55295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8fce04fd4374e47a31470527edb813b",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f36037e59923408680c7b061bb0f01bc",
            "value": 5
          }
        },
        "54e4cd2728df4e2a9824e49130955fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41dfcfe4fbb64793b412002d09616b5a",
            "placeholder": "​",
            "style": "IPY_MODEL_db12b9148dbe45dba62caf289e83fc26",
            "value": " 5/5 [00:10&lt;00:00,  1.89s/it]"
          }
        },
        "b89a0df7bb4b40f4802dee5df78c1501": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c271d5f5c5564cb39c998024737feb84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49ea8eae9d424c7d8c0a75ea7986dc90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8fce04fd4374e47a31470527edb813b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f36037e59923408680c7b061bb0f01bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41dfcfe4fbb64793b412002d09616b5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db12b9148dbe45dba62caf289e83fc26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69e53e04a7c54b99985af754aa55be14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f47a7a3a2b947b4ac3383582eff92ef",
              "IPY_MODEL_47ee4c9442184207adacefdf5d5da32e",
              "IPY_MODEL_f5ac750f49f24036a12bfa5da2f72f27"
            ],
            "layout": "IPY_MODEL_774830d23c0a4b24ae5d4b005dc0b0c5"
          }
        },
        "9f47a7a3a2b947b4ac3383582eff92ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6b44949c4444c8a9d87453aa81e661e",
            "placeholder": "​",
            "style": "IPY_MODEL_8884a505b2a74d8e98b37c943b1849d3",
            "value": "100%"
          }
        },
        "47ee4c9442184207adacefdf5d5da32e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be86f25a13cd4e9fb9c9528f450bc291",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d01dca085db541a9b6a4dcee35b64d99",
            "value": 5
          }
        },
        "f5ac750f49f24036a12bfa5da2f72f27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb95e8fd5f414e1d91f1f7b698b26688",
            "placeholder": "​",
            "style": "IPY_MODEL_20913245b6ae425fb230569f44cce306",
            "value": " 5/5 [00:11&lt;00:00,  2.47s/it]"
          }
        },
        "774830d23c0a4b24ae5d4b005dc0b0c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6b44949c4444c8a9d87453aa81e661e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8884a505b2a74d8e98b37c943b1849d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be86f25a13cd4e9fb9c9528f450bc291": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d01dca085db541a9b6a4dcee35b64d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb95e8fd5f414e1d91f1f7b698b26688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20913245b6ae425fb230569f44cce306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maritnorli/IFCB_CNN_Classify/blob/main/05_IFCB_CNN_condensed_scriptmode_test_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05. Going Modular: Part 2 (script mode)\n",
        "\n",
        "This notebook is part 2/2 of section [05. Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n",
        "\n",
        "For reference, the two parts are:\n",
        "1. [**05. Going Modular: Part 1 (cell mode)**](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/05_pytorch_going_modular_cell_mode.ipynb) - this notebook is run as a traditional Jupyter Notebook/Google Colab notebook and is a condensed version of [notebook 04](https://www.learnpytorch.io/04_pytorch_custom_datasets/).\n",
        "2. [**05. Going Modular: Part 2 (script mode)**](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/05_pytorch_going_modular_script_mode.ipynb) - this notebook is the same as number 1 but with added functionality to turn each of the major sections into Python scripts, such as, `data_setup.py` and `train.py`.\n",
        "\n",
        "Why two parts?\n",
        "\n",
        "Because sometimes the best way to learn something is to see how it *differs* from something else.\n",
        "\n",
        "If you run each notebook side-by-side you'll see how they differ and that's where the key learnings are."
      ],
      "metadata": {
        "id": "3iHTyE5CJhev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is script mode?\n",
        "\n",
        "**Script mode** uses [Jupyter Notebook cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html) (special commands) to turn specific cells into Python scripts.\n",
        "\n",
        "For example if you run the following code in a cell, you'll create a Python file called `hello_world.py`:\n",
        "\n",
        "```\n",
        "%%writefile hello_world.py\n",
        "print(\"hello world, machine learning is fun!\")\n",
        "```\n",
        "\n",
        "You could then run this Python file on the command line with:\n",
        "\n",
        "```\n",
        "python hello_world.py\n",
        "\n",
        ">>> hello world, machine learning is fun!\n",
        "```\n",
        "\n",
        "The main cell magic we're interested in using is `%%writefile`.\n",
        "\n",
        "Putting `%%writefile filename` at the top of a cell in Jupyter or Google Colab will write the contents of that cell to a specified `filename`.\n",
        "\n",
        "> **Question:** Do I have to create Python files like this? Can't I just start directly with a Python file and skip using a Google Colab notebook?\n",
        ">\n",
        "> **Answer:** Yes. This is only *one* way of creating Python scripts. If you know the kind of script you'd like to write, you could start writing it straight away. But since using Jupyter/Google Colab notebooks is a popular way of starting off data science and machine learning projects, knowing about the `%%writefile` magic command is a handy tip."
      ],
      "metadata": {
        "id": "seap-hlHttcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What has script mode got to do with PyTorch?\n",
        "\n",
        "If you've written some useful code in a Jupyter Notebook or Google Colab notebook, chances are you'll want to use that code again.\n",
        "\n",
        "And turning your useful cells into Python scripts (`.py` files) means you can use specific pieces of your code in other projects.\n",
        "\n",
        "This practice is not PyTorch specific.\n",
        "\n",
        "But it's how you'll see many different online PyTorch repositories structured."
      ],
      "metadata": {
        "id": "t0EQ8mq1ugny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch in the wild\n",
        "\n",
        "For example, if you find a PyTorch project on GitHub, it may be structured in the following way:\n",
        "\n",
        "```\n",
        "pytorch_project/\n",
        "├── pytorch_project/\n",
        "│   ├── data_setup.py\n",
        "│   ├── engine.py\n",
        "│   ├── model.py\n",
        "│   ├── train.py\n",
        "│   └── utils.py\n",
        "├── models/\n",
        "│   ├── model_1.pth\n",
        "│   └── model_2.pth\n",
        "└── data/\n",
        "    ├── data_folder_1/\n",
        "    └── data_folder_2/\n",
        "```\n",
        "\n",
        "Here, the top level directory is called `pytorch_project` but you could call it whatever you want.\n",
        "\n",
        "Inside there's another directory called `pytorch_project` which contains several `.py` files, the purposes of these may be:\n",
        "* `data_setup.py` - a file to prepare data (and download data if needed).\n",
        "* `engine.py` - a file containing various training functions.\n",
        "* `model_builder.py` or `model.py` - a file to create a PyTorch model.\n",
        "* `train.py` - a file to leverage all other files and train a target PyTorch model.\n",
        "* `utils.py` - a file dedicated to helpful utility functions.\n",
        "\n",
        "And the `models` and `data` directories could hold PyTorch models and data files respectively (though due to the size of models and data files, it's unlikely you'll find the *full* versions of these on GitHub, these directories are present above mainly for demonstration purposes).\n",
        "\n",
        "> **Note:** There are many different ways to structure a Python project and subsequently a PyTorch project. This isn't a guide on *how* to structure your projects, only an example of how you *might* come across PyTorch projects in the wild. For more on structuring Python projects, see Real Python's [*Python Application Layouts: A Reference*](https://realpython.com/python-application-layouts/) guide."
      ],
      "metadata": {
        "id": "fX3zuSYLul9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's the difference between this notebook (Part 2) and the cell mode notebook (Part 1)?\n",
        "\n",
        "This notebook, 05 Going Modular: Part 2 (script mode), creates Python scripts out of the cells created in part 1.\n",
        "\n",
        "Running this notebook end-to-end will result in having a directory structure very similar to the `pytorch_project` structure above.\n",
        "\n",
        "You'll notice each section in Part 2 (script mode) has an extra subsection (e.g. 2.1, 3.1, 4.1) for turning cell code into script code."
      ],
      "metadata": {
        "id": "St-Eoq8FuwBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What we're going to cover\n",
        "\n",
        "By the end of this notebook you should finish with a directory structure of:\n",
        "\n",
        "```\n",
        "going_modular/\n",
        "├── going_modular/\n",
        "│   ├── data_setup.py\n",
        "│   ├── engine.py\n",
        "│   ├── model_builder.py\n",
        "│   ├── train.py\n",
        "│   └── utils.py\n",
        "├── models/\n",
        "│   ├── 05_going_modular_cell_mode_tinyvgg_model.pth\n",
        "│   └── 05_going_modular_script_mode_tinyvgg_model.pth\n",
        "└── data/\n",
        "    └── pizza_steak_sushi/\n",
        "        ├── train/\n",
        "        │   ├── pizza/\n",
        "        │   │   ├── image01.jpeg\n",
        "        │   │   └── ...\n",
        "        │   ├── steak/\n",
        "        │   └── sushi/\n",
        "        └── test/\n",
        "            ├── pizza/\n",
        "            ├── steak/\n",
        "            └── sushi/\n",
        "```\n",
        "\n",
        "Using this directory structure, you should be able to train a model from within a notebook with the command:\n",
        "\n",
        "```\n",
        "!python going_modular/train.py\n",
        "```\n",
        "\n",
        "Or from the command line with:\n",
        "\n",
        "```\n",
        "python going_modular/train.py\n",
        "```\n",
        "\n",
        "In essence, we will have turned our helpful notebook code into **reusable modular code**."
      ],
      "metadata": {
        "id": "JubX_28Wu55e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where can you get help?\n",
        "\n",
        "You can find the book version of this section [05. PyTorch Going Modular on learnpytorch.io](https://www.learnpytorch.io/05_pytorch_going_modular/).\n",
        "\n",
        "The rest of the materials for this course [are available on GitHub](https://github.com/mrdbourke/pytorch-deep-learning).\n",
        "\n",
        "If you run into trouble, you can ask a question on the course [GitHub Discussions page](https://github.com/mrdbourke/pytorch-deep-learning/discussions).\n",
        "\n",
        "And of course, there's the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) and [PyTorch developer forums](https://discuss.pytorch.org/), a very helpful place for all things PyTorch."
      ],
      "metadata": {
        "id": "17cIlASevCHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Creating a folder for storing Python scripts\n",
        "\n",
        "Since we're going to be creating Python scripts out of our most useful code cells, let's create a folder for storing those scripts.\n",
        "\n",
        "We'll call the folder `going_modular` and create it using Python's [`os.makedirs()`](https://docs.python.org/3/library/os.html) method."
      ],
      "metadata": {
        "id": "Yac0ocFlvIKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"going_modular\", exist_ok=True)"
      ],
      "metadata": {
        "id": "9K_WuvyDvKl2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Get data\n",
        "We're going to start by downloading the same data we used in notebook 04."
      ],
      "metadata": {
        "id": "gVGPfGAGJzqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"IFCB_test_train\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download pizza, steak, sushi data\n",
        "with open(data_path / \"IFCB_test_train.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/maritnorli/IFCB_CNN_Classify/raw/main/data/IFCB_test_train.zip\")\n",
        "    print(\"Downloading IFCB data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "# Unzip pizza, steak, sushi data\n",
        "with zipfile.ZipFile(data_path / \"IFCB_test_train.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping IFCB_test_train data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "# Remove zip file\n",
        "os.remove(data_path / \"IFCB_test_train.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCl8frgxJ0YG",
        "outputId": "54ba0b01-b02d-4da4-d58e-47771d227d5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data/IFCB_test_train directory, creating one...\n",
            "Downloading IFCB data...\n",
            "Unzipping IFCB_test_train data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turn this code into a getdata.py script"
      ],
      "metadata": {
        "id": "3eExZV2HB4qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile get_data.py\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"IFCB_test_train\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download pizza, steak, sushi data\n",
        "with open(data_path / \"IFCB_test_train.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/maritnorli/IFCB_CNN_Classify/raw/main/data/IFCB_test_train.zip\")\n",
        "    print(\"Downloading IFCB data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "# Unzip pizza, steak, sushi data\n",
        "with zipfile.ZipFile(data_path / \"IFCB_test_train.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping IFCB_test_train data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "# Remove zip file\n",
        "os.remove(data_path / \"IFCB_test_train.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lHmuVLQB4O8",
        "outputId": "c8ba3428-bf77-4ed4-b489-9761aaa7bc11"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing get_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup train and testing paths\n",
        "train_dir = image_path / \"Train\"\n",
        "test_dir = image_path / \"Test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYZERVCxJ0UB",
        "outputId": "5b53b506-429c-49e2-f30c-fa5002c30e4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/IFCB_test_train/Train'),\n",
              " PosixPath('data/IFCB_test_train/Test'))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "\n",
        "Now we'll turn the image dataset into PyTorch `Dataset`'s and `DataLoader`'s."
      ],
      "metadata": {
        "id": "onQbKQp3J0Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Create simple transform\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize(size=(64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Use ImageFolder to create dataset(s)\n",
        "train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n",
        "                                  transform=data_transform, # transforms to perform on data (images)\n",
        "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
        "\n",
        "test_data = datasets.ImageFolder(root=test_dir,\n",
        "                                 transform=data_transform)\n",
        "\n",
        "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox4vgIlqJ0Ow",
        "outputId": "6eee5f9b-dab3-4736-b8a2-9274136a034b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 210\n",
            "    Root location: data/IFCB_test_train/Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Grayscale(num_output_channels=1)\n",
            "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "           )\n",
            "Test data:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 42\n",
            "    Root location: data/IFCB_test_train/Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Grayscale(num_output_channels=1)\n",
            "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class names as a list\n",
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlFj6w2FJ0Lq",
        "outputId": "c021a1cc-1885-44e7-a322-dbabd557de68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chaetoceros_decipiens_118',\n",
              " 'Guinardia_delicatula_095',\n",
              " 'Tripos_muelleri_008']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Can also get class names as a dict\n",
        "class_dict = train_data.class_to_idx\n",
        "class_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUn5F8DnJ0Ej",
        "outputId": "406a44b4-c6d4-4084-998a-2b3b28f86b3b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Chaetoceros_decipiens_118': 0,\n",
              " 'Guinardia_delicatula_095': 1,\n",
              " 'Tripos_muelleri_008': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the lengths\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUAl3Yy9LQJT",
        "outputId": "b176fc4e-9df3-49f1-fcd0-1fd36de47061"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210, 42)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn train and test Datasets into DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=1, # how many samples per batch?\n",
        "                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n",
        "                              shuffle=True) # shuffle the data?\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=1,\n",
        "                             num_workers=1,\n",
        "                             shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm4gYMxHLQFk",
        "outputId": "8af1bf98-1e5c-43f1-c370-24cdb36f1870"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7c7722216710>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7c7722217670>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out single image size/shape\n",
        "img, label = next(iter(train_dataloader))\n",
        "\n",
        "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
        "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Label shape: {label.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHp2VC9RLQCT",
        "outputId": "85bef8de-4dd7-4bc3-a8cf-6f7646e6e23d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([1, 1, 64, 64]) -> [batch_size, color_channels, height, width]\n",
            "Label shape: torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Create Datasets and DataLoaders (script mode)\n",
        "\n",
        "Rather than rewriting all of the code above everytime we wanted to load data, we can turn it into a script called `data_setup.py`.\n",
        "\n",
        "Let's capture all of the above functionality into a function called `create_dataloaders()`."
      ],
      "metadata": {
        "id": "cBI5pcVV1x9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "\"\"\"\n",
        "Contains functionality for creating PyTorch DataLoaders for\n",
        "image classification data.\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "  \"\"\"Creates training and testing DataLoaders.\n",
        "\n",
        "  Takes in a training directory and testing directory path and turns\n",
        "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
        "\n",
        "  Args:\n",
        "    train_dir: Path to training directory.\n",
        "    test_dir: Path to testing directory.\n",
        "    transform: torchvision transforms to perform on training and testing data.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "    Where class_names is a list of the target classes.\n",
        "    Example usage:\n",
        "      train_dataloader, test_dataloader, class_names = \\\n",
        "        = create_dataloaders(train_dir=path/to/train_dir,\n",
        "                             test_dir=path/to/test_dir,\n",
        "                             transform=some_transform,\n",
        "                             batch_size=32,\n",
        "                             num_workers=4)\n",
        "  \"\"\"\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqE81PtA2QP5",
        "outputId": "3dbffe16-c960-42b2-c86d-732b208ab8fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Making a model (TinyVGG)\n",
        "We're going to use the same model we used in notebook 04: TinyVGG from the CNN Explainer website.\n",
        "\n",
        "The only change here from notebook 04 is that a docstring has been added using Google's Style Guide for Python."
      ],
      "metadata": {
        "id": "4S9cChVcLP-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  \"\"\"Creates the TinyVGG architecture.\n",
        "\n",
        "  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
        "  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "  Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "      super().__init__()\n",
        "      self.conv_block_1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=input_shape,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3, # how big is the square that's going over the image?\n",
        "                    stride=1, # default\n",
        "                    padding=0), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2,\n",
        "                        stride=2) # default stride value is same as kernel_size\n",
        "      )\n",
        "      self.conv_block_2 = nn.Sequential(\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2)\n",
        "      )\n",
        "      self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          # Where did this in_features shape come from?\n",
        "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "          nn.Linear(in_features=hidden_units*13*13,\n",
        "                    out_features=output_shape)\n",
        "      )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "      x = self.conv_block_1(x)\n",
        "      x = self.conv_block_2(x)\n",
        "      x = self.classifier(x)\n",
        "      return x\n",
        "      # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion"
      ],
      "metadata": {
        "id": "fNUTlcdyLP7J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create an instance of `TinyVGG` and put it on the target device.\n",
        "\n",
        "> **Note:** If you're using Google Colab, and you'd like to use a GPU (recommended), you can turn one on via going to Runtime -> Change runtime type -> Hardware accelerator -> GPU."
      ],
      "metadata": {
        "id": "px0VJdU05cAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate an instance of the model\n",
        "torch.manual_seed(42)\n",
        "model_0 = TinyVGG(input_shape=1, # number of color channels (3 for RGB)\n",
        "                  hidden_units=10,\n",
        "                  output_shape=len(train_data.classes)).to(device)\n",
        "model_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fndI0flUL--p",
        "outputId": "5315401b-8aa2-48ca-c4cd-e572af341af8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyVGG(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check out our model by doing a dummy forward pass."
      ],
      "metadata": {
        "id": "yTzuIaECL_1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get a batch of images and labels from the DataLoader\n",
        "img_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
        "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
        "print(f\"Single image shape: {img_single.shape}\\n\")\n",
        "\n",
        "# 3. Perform a forward pass on a single image\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "    pred = model_0(img_single.to(device))\n",
        "\n",
        "# 4. Print out what's happening and convert model logits -> pred probs -> pred label\n",
        "print(f\"Output logits:\\n{pred}\\n\")\n",
        "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
        "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
        "print(f\"Actual label:\\n{label_single}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f88qjEnfL_xU",
        "outputId": "1220cf28-44d1-4893-ca27-18d86273cd93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single image shape: torch.Size([1, 1, 64, 64])\n",
            "\n",
            "Output logits:\n",
            "tensor([[-0.0139,  0.0263,  0.0175]], device='cuda:0')\n",
            "\n",
            "Output prediction probabilities:\n",
            "tensor([[0.3254, 0.3388, 0.3358]], device='cuda:0')\n",
            "\n",
            "Output prediction label:\n",
            "tensor([1], device='cuda:0')\n",
            "\n",
            "Actual label:\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Making a model (TinyVGG) (script mode)\n",
        "\n",
        "Over the past few notebooks (notebook 03 and notebook 04), we've built the TinyVGG model a few times.\n",
        "\n",
        "So it makes sense to put the model into its file so we can reuse it again and again.\n",
        "\n",
        "Let's put our `TinyVGG()` model class into a script called `model_builder.py` with the line `%%writefile going_modular/model_builder.py`."
      ],
      "metadata": {
        "id": "O9qGw_6k6H3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_builder.py\n",
        "\"\"\"\n",
        "Contains PyTorch model code to instantiate a TinyVGG model.\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "    \"\"\"Creates the TinyVGG architecture.\n",
        "\n",
        "    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
        "    See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "    Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=input_shape,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2,\n",
        "                        stride=2)\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          # Where did this in_features shape come from?\n",
        "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "          nn.Linear(in_features=hidden_units*13*13,\n",
        "                    out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "        # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rj9FuK06KGU",
        "outputId": "2b84dba8-02a8-4956-f76e-894464c9e60c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an instance of `TinyVGG` (from the script)."
      ],
      "metadata": {
        "id": "NPx3y1rT8SjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from going_modular import model_builder\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate an instance of the model from the \"model_builder.py\" script\n",
        "torch.manual_seed(42)\n",
        "model_1 = model_builder.TinyVGG(input_shape=1, # number of color channels (3 for RGB)\n",
        "                                hidden_units=10,\n",
        "                                output_shape=len(class_names)).to(device)\n",
        "model_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jf_cuQ98SUL",
        "outputId": "3aad2605-dbc5-43b1-cf13-b73c187e0d17"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyVGG(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do a dummy forward pass on `model_1`."
      ],
      "metadata": {
        "id": "0AHFyGYX8gcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get a batch of images and labels from the DataLoader\n",
        "img_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
        "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
        "print(f\"Single image shape: {img_single.shape}\\n\")\n",
        "\n",
        "# 3. Perform a forward pass on a single image\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "    pred = model_1(img_single.to(device))\n",
        "\n",
        "# 4. Print out what's happening and convert model logits -> pred probs -> pred label\n",
        "print(f\"Output logits:\\n{pred}\\n\")\n",
        "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
        "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
        "print(f\"Actual label:\\n{label_single}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVN4wRGj8jkT",
        "outputId": "ff2b4826-aa02-43df-b5c3-f73f06088297"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single image shape: torch.Size([1, 1, 64, 64])\n",
            "\n",
            "Output logits:\n",
            "tensor([[-0.0139,  0.0263,  0.0175]], device='cuda:0')\n",
            "\n",
            "Output prediction probabilities:\n",
            "tensor([[0.3254, 0.3388, 0.3358]], device='cuda:0')\n",
            "\n",
            "Output prediction label:\n",
            "tensor([1], device='cuda:0')\n",
            "\n",
            "Actual label:\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Creating `train_step()` and `test_step()` functions and `train()` to combine them  \n",
        "Rather than writing them again, we can reuse the train_step() and test_step() functions from notebook 04.\n",
        "\n",
        "The same goes for the train() function we created.\n",
        "\n",
        "The only difference here is that these functions have had docstrings added to them in Google's Python Functions and Methods Style Guide.\n",
        "\n",
        "Let's start by making train_step()."
      ],
      "metadata": {
        "id": "pVKhEcvVL_ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forward\n",
        "  pass, loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "  \"\"\"\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "      # Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # 2. Calculate  and accumulate loss\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backward\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "\n",
        "      # Calculate and accumulate accuracy metric across all batches\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc"
      ],
      "metadata": {
        "id": "EsX7_bIaL_rY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll do test_step().\n",
        "\n"
      ],
      "metadata": {
        "id": "YRiHOCSKL_oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "  \"\"\"\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "      # Loop through DataLoader batches\n",
        "      for batch, (X, y) in enumerate(dataloader):\n",
        "          # Send data to target device\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # 1. Forward pass\n",
        "          test_pred_logits = model(X)\n",
        "\n",
        "          # 2. Calculate and accumulate loss\n",
        "          loss = loss_fn(test_pred_logits, y)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          # Calculate and accumulate accuracy\n",
        "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc"
      ],
      "metadata": {
        "id": "-AKgvfJ0L_lO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we'll combine train_step() and test_step() into train()."
      ],
      "metadata": {
        "id": "UbJG1oe9L_iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List[float]]:\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Passes a target PyTorch models through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "                  train_acc: [...],\n",
        "                  test_loss: [...],\n",
        "                  test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "                 {train_loss: [2.0616, 1.0537],\n",
        "                  train_acc: [0.3945, 0.3945],\n",
        "                  test_loss: [1.2641, 1.5706],\n",
        "                  test_acc: [0.3400, 0.2973]}\n",
        "  \"\"\"\n",
        "  # Create empty results dictionary\n",
        "  results = {\"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "      train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "      test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "      # Print out what's happening\n",
        "      print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "      )\n",
        "\n",
        "      # Update results dictionary\n",
        "      results[\"train_loss\"].append(train_loss)\n",
        "      results[\"train_acc\"].append(train_acc)\n",
        "      results[\"test_loss\"].append(test_loss)\n",
        "      results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return results"
      ],
      "metadata": {
        "id": "me9h6kXvq7sT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Creating `train_step()` and `test_step()` functions and `train()` to combine them (script mode)   \n",
        "\n",
        "To create a script for `train_step()`, `test_step()` and `train()`, we'll combine their code all into a single cell.\n",
        "\n",
        "We'll then write that cell to a file called `engine.py` because these functions will be the \"engine\" of our training pipeline.\n",
        "\n",
        "We can do so with the magic line `%%writefile going_modular/engine.py`.\n",
        "\n",
        "We'll also make sure to put all the imports we need (`torch`, `typing`, and `tqdm`) at the top of the cell."
      ],
      "metadata": {
        "id": "R924swHn85zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "    Turns a target PyTorch model to training mode and then\n",
        "    runs through all of the required training steps (forward\n",
        "    pass, loss calculation, optimizer step).\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "    \"\"\"\n",
        "    # Put model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # Setup train loss and train accuracy values\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    # Loop through data loader data batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Send data to target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate  and accumulate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate and accumulate accuracy metric across all batches\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "    a forward pass on a testing dataset.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "    \"\"\"\n",
        "    # Put model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Setup test loss and test accuracy values\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        # Loop through DataLoader batches\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Send data to target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            # 2. Calculate and accumulate loss\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate and accumulate accuracy\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List[float]]:\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "              train_acc: [...],\n",
        "              test_loss: [...],\n",
        "              test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "             {train_loss: [2.0616, 1.0537],\n",
        "              train_acc: [0.3945, 0.3945],\n",
        "              test_loss: [1.2641, 1.5706],\n",
        "              test_acc: [0.3400, 0.2973]}\n",
        "    \"\"\"\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clSMtKEf85un",
        "outputId": "9f387956-7359-4f2d-9e85-28856180fa97"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Creating a function to save the model\n",
        "\n",
        "Let's setup a function to save our model to a directory."
      ],
      "metadata": {
        "id": "TUCBwMuYrD4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "  \"\"\"Saves a PyTorch model to a target directory.\n",
        "\n",
        "  Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "\n",
        "  Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "  \"\"\"\n",
        "  # Create target directory\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "\n",
        "  # Create model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  # Save the model state_dict()\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "id": "68ivpt0qrHwO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Creating a function to save the model (script mode)\n",
        "\n",
        "How about we add our `save_model()` function to a script called `utils.py` which is short for \"utilities\".\n",
        "\n",
        "We can do so with the magic line `%%writefile going_modular/utils.py`."
      ],
      "metadata": {
        "id": "e2wK2hbC9Sb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/utils.py\n",
        "\"\"\"\n",
        "Contains various utility functions for PyTorch model training and saving.\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "    \"\"\"Saves a PyTorch model to a target directory.\n",
        "\n",
        "    Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "\n",
        "    Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "    \"\"\"\n",
        "    # Create target directory\n",
        "    target_dir_path = Path(target_dir)\n",
        "    target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "\n",
        "    # Create model save path\n",
        "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "    model_save_path = target_dir_path / model_name\n",
        "\n",
        "    # Save the model state_dict()\n",
        "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "    torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnCUesIO9SR4",
        "outputId": "b48c8961-51ea-462e-c85b-505378b1c3d2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train, evaluate and save the model\n",
        "\n",
        "Let's leverage the functions we've got above to train, test and save a model to file.\n"
      ],
      "metadata": {
        "id": "O30Pj6OprObK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Recreate an instance of TinyVGG\n",
        "model_0 = TinyVGG(input_shape=1, # number of color channels (3 for RGB)\n",
        "                  hidden_units=10,\n",
        "                  output_shape=len(train_data.classes)).to(device)\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Train model_0\n",
        "model_0_results = train(model=model_0,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        optimizer=optimizer,\n",
        "                        loss_fn=loss_fn,\n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        device=device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n",
        "\n",
        "# Save the model\n",
        "save_model(model=model_0,\n",
        "           target_dir=\"models\",\n",
        "           model_name=\"05_going_modular_cell_mode_tinyvgg_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "22552054f5cf4a3aba19df1d6d766f16",
            "e82f35ba49e048dd9040c28a24022b19",
            "df0ccf2e74c0453eb77ed4e6bee55295",
            "54e4cd2728df4e2a9824e49130955fc5",
            "b89a0df7bb4b40f4802dee5df78c1501",
            "c271d5f5c5564cb39c998024737feb84",
            "49ea8eae9d424c7d8c0a75ea7986dc90",
            "a8fce04fd4374e47a31470527edb813b",
            "f36037e59923408680c7b061bb0f01bc",
            "41dfcfe4fbb64793b412002d09616b5a",
            "db12b9148dbe45dba62caf289e83fc26"
          ]
        },
        "id": "o7JRbUqHrOUZ",
        "outputId": "1045bdf0-98ab-4f2d-9516-7c6cd47019e6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22552054f5cf4a3aba19df1d6d766f16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 1.1066 | train_acc: 0.3238 | test_loss: 1.0981 | test_acc: 0.3333\n",
            "Epoch: 2 | train_loss: 1.0906 | train_acc: 0.3286 | test_loss: 1.0367 | test_acc: 0.5000\n",
            "Epoch: 3 | train_loss: 0.8091 | train_acc: 0.6619 | test_loss: 0.4904 | test_acc: 0.7857\n",
            "Epoch: 4 | train_loss: 0.4551 | train_acc: 0.8429 | test_loss: 0.4226 | test_acc: 0.8571\n",
            "Epoch: 5 | train_loss: 0.4271 | train_acc: 0.8619 | test_loss: 0.4308 | test_acc: 0.8810\n",
            "[INFO] Total training time: 10.525 seconds\n",
            "[INFO] Saving model to: models/05_going_modular_cell_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Train, evaluate and save the model (script mode)\n",
        "\n",
        "Let's combine all of our modular files into a single script `train.py`.\n",
        "\n",
        "This will allow us to run all of the functions we've written with a single line of code on the command line:\n",
        "\n",
        "`python going_modular/train.py`\n",
        "\n",
        "Or if we're running it in a notebook:\n",
        "\n",
        "`!python going_modular/train.py`\n",
        "\n",
        "We'll go through the following steps:\n",
        "1. Import the various dependencies, namely `torch`, `os`, `torchvision.transforms` and all of the scripts from the `going_modular` directory, `data_setup`, `engine`, `model_builder`, `utils`.\n",
        "  * **Note:** Since `train.py` will be *inside* the `going_modular` directory, we can import the other modules via `import ...` rather than `from going_modular import ...`.\n",
        "2. Setup various hyperparameters such as batch size, number of epochs, learning rate and number of hidden units (these could be set in the future via [Python's `argparse`](https://docs.python.org/3/library/argparse.html)).\n",
        "3. Setup the training and test directories.\n",
        "4. Setup device-agnostic code.\n",
        "5. Create the necessary data transforms.\n",
        "6. Create the DataLoaders using `data_setup.py`.\n",
        "7. Create the model using `model_builder.py`.\n",
        "8. Setup the loss function and optimizer.\n",
        "9. Train the model using `engine.py`.\n",
        "10. Save the model using `utils.py`."
      ],
      "metadata": {
        "id": "G3MO8Z1W-9q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train.py\n",
        "\"\"\"\n",
        "Trains a PyTorch image classification model using device-agnostic code.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import data_setup, engine, model_builder, utils\n",
        "\n",
        "\n",
        "# Setup hyperparameters\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "HIDDEN_UNITS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Setup directories\n",
        "train_dir = \"data/IFCB_test_train/Train\"\n",
        "test_dir = \"data/IFCB_test_train/Test\"\n",
        "\n",
        "# Setup target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create transforms\n",
        "data_transform = transforms.Compose([\n",
        "  transforms.Grayscale(),\n",
        "  transforms.Resize((64, 64)),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create DataLoaders with help from data_setup.py\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Create model with help from model_builder.py\n",
        "model = model_builder.TinyVGG(\n",
        "    input_shape=1,\n",
        "    hidden_units=HIDDEN_UNITS,\n",
        "    output_shape=len(class_names)\n",
        ").to(device)\n",
        "\n",
        "# Set loss and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "# Start training with help from engine.py\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=NUM_EPOCHS,\n",
        "             device=device)\n",
        "\n",
        "# Save the model with help from utils.py\n",
        "utils.save_model(model=model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S23x7b4D-9X9",
        "outputId": "30bf79b2-7d86-4ee9-bbb1-bf5cb798640c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our final directory structure looks like:\n",
        "```\n",
        "data/\n",
        "  pizza_steak_sushi/\n",
        "    train/\n",
        "      pizza/\n",
        "        train_image_01.jpeg\n",
        "        train_image_02.jpeg\n",
        "        ...\n",
        "      steak/\n",
        "      sushi/\n",
        "    test/\n",
        "      pizza/\n",
        "        test_image_01.jpeg\n",
        "        test_image_02.jpeg\n",
        "        ...\n",
        "      steak/\n",
        "      sushi/\n",
        "going_modular/\n",
        "  data_setup.py\n",
        "  engine.py\n",
        "  model_builder.py\n",
        "  train.py\n",
        "  utils.py\n",
        "models/\n",
        "  saved_model.pth\n",
        "```\n",
        "\n",
        "Now to put it all together!\n",
        "\n",
        "Let's run our `train.py` file from the command line with:\n",
        "\n",
        "```\n",
        "!python going_modular/train.py\n",
        "```\n"
      ],
      "metadata": {
        "id": "seCylvS8_zjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/train.py"
      ],
      "metadata": {
        "id": "cSOsQnN-rOFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ab7fdc-e3c2-4351-af48-a62512787a31"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/5 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.1028 | train_acc: 0.3001 | test_loss: 1.0932 | test_acc: 0.2188\n",
            " 20% 1/5 [00:01<00:05,  1.26s/it]Epoch: 2 | train_loss: 1.0985 | train_acc: 0.3403 | test_loss: 1.0985 | test_acc: 0.2188\n",
            " 40% 2/5 [00:02<00:02,  1.04it/s]Epoch: 3 | train_loss: 1.0976 | train_acc: 0.3447 | test_loss: 1.0977 | test_acc: 0.3156\n",
            " 60% 3/5 [00:02<00:01,  1.14it/s]Epoch: 4 | train_loss: 1.0949 | train_acc: 0.4058 | test_loss: 1.0960 | test_acc: 0.2188\n",
            " 80% 4/5 [00:04<00:01,  1.01s/it]Epoch: 5 | train_loss: 1.0855 | train_acc: 0.5933 | test_loss: 1.0947 | test_acc: 0.2188\n",
            "100% 5/5 [00:05<00:00,  1.03s/it]\n",
            "[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Edit the train.py to accept arguments"
      ],
      "metadata": {
        "id": "aX37Vi_UEGrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_setup.py\n",
        "\"\"\"\n",
        "Contains functionality for creating PyTorch DataLoaders for\n",
        "image classification data.\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "  \"\"\"Creates training and testing DataLoaders.\n",
        "\n",
        "  Takes in a training directory and testing directory path and turns\n",
        "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
        "\n",
        "  Args:\n",
        "    train_dir: Path to training directory.\n",
        "    test_dir: Path to testing directory.\n",
        "    transform: torchvision transforms to perform on training and testing data.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "    Where class_names is a list of the target classes.\n",
        "    Example usage:\n",
        "      train_dataloader, test_dataloader, class_names = \\\n",
        "        = create_dataloaders(train_dir=path/to/train_dir,\n",
        "                             test_dir=path/to/test_dir,\n",
        "                             transform=some_transform,\n",
        "                             batch_size=32,\n",
        "                             num_workers=4)\n",
        "  \"\"\"\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy1aeeDZE1qY",
        "outputId": "a87f5bf2-c7c9-4a60-ff7f-96caea3649f7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile engine.py\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "    Turns a target PyTorch model to training mode and then\n",
        "    runs through all of the required training steps (forward\n",
        "    pass, loss calculation, optimizer step).\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "    \"\"\"\n",
        "    # Put model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # Setup train loss and train accuracy values\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    # Loop through data loader data batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Send data to target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate  and accumulate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate and accumulate accuracy metric across all batches\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "    a forward pass on a testing dataset.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "    \"\"\"\n",
        "    # Put model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Setup test loss and test accuracy values\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        # Loop through DataLoader batches\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Send data to target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            # 2. Calculate and accumulate loss\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate and accumulate accuracy\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List[float]]:\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "              train_acc: [...],\n",
        "              test_loss: [...],\n",
        "              test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "             {train_loss: [2.0616, 1.0537],\n",
        "              train_acc: [0.3945, 0.3945],\n",
        "              test_loss: [1.2641, 1.5706],\n",
        "              test_acc: [0.3400, 0.2973]}\n",
        "    \"\"\"\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnMv_RgmE1fc",
        "outputId": "151e9354-f7f5-4163-ee3d-b7429ee60bc6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_builder.py\n",
        "\"\"\"\n",
        "Contains PyTorch model code to instantiate a TinyVGG model.\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "    \"\"\"Creates the TinyVGG architecture.\n",
        "\n",
        "    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
        "    See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "    Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "      super().__init__()\n",
        "      self.conv_block_1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=input_shape,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2,\n",
        "                       stride=2)\n",
        "      )\n",
        "      self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "      )\n",
        "      self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          # Where did this in_features shape come from?\n",
        "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "          nn.Linear(in_features=hidden_units*13*13,\n",
        "                    out_features=output_shape)\n",
        "      )\n",
        "    def forward(self, x: torch.Tensor):\n",
        "      x = self.conv_block_1(x)\n",
        "      print(x.shape)\n",
        "      x = self.conv_block_2(x)\n",
        "      print(x.shape)\n",
        "      x = self.classifier(x)\n",
        "      print(x.shape)\n",
        "      return x        # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfMiUCBCE1Sa",
        "outputId": "64ea0fe8-eb42-45a3-fe11-fcac33034833"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "\"\"\"\n",
        "Contains various utility functions for PyTorch model training and saving.\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "    \"\"\"Saves a PyTorch model to a target directory.\n",
        "\n",
        "    Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "\n",
        "    Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "    \"\"\"\n",
        "    # Create target directory\n",
        "    target_dir_path = Path(target_dir)\n",
        "    target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "\n",
        "    # Create model save path\n",
        "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "    model_save_path = target_dir_path / model_name\n",
        "\n",
        "    # Save the model state_dict()\n",
        "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "    torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MI-IUglE1Ii",
        "outputId": "083cb5bc-416f-40e1-e727-4f94aa192183"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\"\"\"\n",
        "Trains a PyTorch image classification model using device-agnostic code.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "import data_setup, engine, model_builder, utils\n",
        "# create a parser\n",
        "parser = argparse.ArgumentParser(description=\"Get some hyperparameters.\")\n",
        "\n",
        "# Get an arg for hyperparameters:\n",
        "parser.add_argument(\"--num_epochs\", default=10, type=int, help=\"the number of epochs to train for\")\n",
        "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"the number of samples per batch\")\n",
        "parser.add_argument(\"--hidden_units\", default=10, type=int, help=\"the number of hidden units in hidden layers\")\n",
        "parser.add_argument(\"--learning_rate\", default=0.001, type=float, help=\"learning rate to use for model\")\n",
        "\n",
        "# Get an arg for directories:\n",
        "parser.add_argument(\"--train_dir\", default=\"data/IFCB_test_train/Train\", type=str, help=\"training directory i.e data/IFCB_test_train/Train\")\n",
        "parser.add_argument(\"--test_dir\", default=\"data/IFCB_test_train/Test\", type=str, help=\"training directory i.e data/IFCB_test_train/Test\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Setup hyperparameters\n",
        "NUM_EPOCHS = args.num_epochs\n",
        "BATCH_SIZE = args.batch_size\n",
        "HIDDEN_UNITS = args.hidden_units\n",
        "LEARNING_RATE = args.learning_rate\n",
        "print(f\"[INFO] Training a model for {NUM_EPOCHS} epchs with batch size {BATCH_SIZE} using {HIDDEN_UNITS} hidden units and a learning rate of {LEARNING_RATE}\")\n",
        "\n",
        "\n",
        "# Setup directories\n",
        "#train_dir = \"data/IFCB_test_train/Train\"\n",
        "#test_dir = \"data/IFCB_test_train/Test\"\n",
        "train_dir = args.train_dir\n",
        "test_dir = args.test_dir\n",
        "\n",
        "print(f\"[INFO] Training data file: {train_dir}\")\n",
        "print(f\"[INFO] Testing data file: {test_dir}\")\n",
        "\n",
        "# Setup target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create transforms\n",
        "data_transform = transforms.Compose([\n",
        "  transforms.Grayscale(),\n",
        "  transforms.Resize((64, 64)),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create DataLoaders with help from data_setup.py\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Create model with help from model_builder.py\n",
        "model = model_builder.TinyVGG(\n",
        "    input_shape=1,\n",
        "    hidden_units=HIDDEN_UNITS,\n",
        "    output_shape=len(class_names)\n",
        ").to(device)\n",
        "\n",
        "# Set loss and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "# Start training with help from engine.py\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=NUM_EPOCHS,\n",
        "             device=device)\n",
        "\n",
        "# Save the model with help from utils.py\n",
        "utils.save_model(model=model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBMzdidAEGiu",
        "outputId": "c283fd66-cb15-400f-8feb-610e0bf85f19"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67FnWIAFK-d7",
        "outputId": "0ddb36d3-b91f-491b-ac06-06f47c332b55"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training a model for 10 epchs with batch size 32 using 10 hidden units and a learning rate of 0.001\n",
            "[INFO] Training data file: data/IFCB_test_train/Train\n",
            "[INFO] Testing data file: data/IFCB_test_train/Test\n",
            "\r  0% 0/10 [00:00<?, ?it/s]torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([18, 10, 30, 30])\n",
            "torch.Size([18, 10, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([10, 10, 30, 30])\n",
            "torch.Size([10, 10, 13, 13])\n",
            "torch.Size([10, 3])\n",
            "Epoch: 1 | train_loss: 1.1017 | train_acc: 0.3269 | test_loss: 1.0982 | test_acc: 0.2188\n",
            " 10% 1/10 [00:01<00:09,  1.10s/it]torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([18, 10, 30, 30])\n",
            "torch.Size([18, 10, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([10, 10, 30, 30])\n",
            "torch.Size([10, 10, 13, 13])\n",
            "torch.Size([10, 3])\n",
            "Epoch: 2 | train_loss: 1.0995 | train_acc: 0.3175 | test_loss: 1.0981 | test_acc: 0.2188\n",
            " 20% 2/10 [00:01<00:07,  1.10it/s]torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([18, 10, 30, 30])\n",
            "torch.Size([18, 10, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([10, 10, 30, 30])\n",
            "torch.Size([10, 10, 13, 13])\n",
            "torch.Size([10, 3])\n",
            "Epoch: 3 | train_loss: 1.0991 | train_acc: 0.3170 | test_loss: 1.0912 | test_acc: 0.5625\n",
            " 30% 3/10 [00:02<00:05,  1.17it/s]torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([18, 10, 30, 30])\n",
            "torch.Size([18, 10, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([10, 10, 30, 30])\n",
            "torch.Size([10, 10, 13, 13])\n",
            "torch.Size([10, 3])\n",
            "Epoch: 4 | train_loss: 1.0982 | train_acc: 0.3333 | test_loss: 1.0920 | test_acc: 0.5625\n",
            " 40% 4/10 [00:03<00:04,  1.23it/s]torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([18, 10, 30, 30])\n",
            "torch.Size([18, 10, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([10, 10, 30, 30])\n",
            "torch.Size([10, 10, 13, 13])\n",
            "torch.Size([10, 3])\n",
            "Epoch: 5 | train_loss: 1.0953 | train_acc: 0.3909 | test_loss: 1.0929 | test_acc: 0.5500\n",
            " 50% 5/10 [00:04<00:04,  1.25it/s]torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([18, 10, 30, 30])\n",
            "torch.Size([18, 10, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([10, 10, 30, 30])\n",
            "torch.Size([10, 10, 13, 13])\n",
            "torch.Size([10, 3])\n",
            "Epoch: 6 | train_loss: 1.0896 | train_acc: 0.3904 | test_loss: 1.0878 | test_acc: 0.2188\n",
            " 60% 6/10 [00:04<00:03,  1.26it/s]torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([18, 10, 30, 30])\n",
            "torch.Size([18, 10, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([10, 10, 30, 30])\n",
            "torch.Size([10, 10, 13, 13])\n",
            "torch.Size([10, 3])\n",
            "Epoch: 7 | train_loss: 1.0764 | train_acc: 0.3433 | test_loss: 1.0728 | test_acc: 0.2188\n",
            " 70% 7/10 [00:05<00:02,  1.16it/s]torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([18, 10, 30, 30])\n",
            "torch.Size([18, 10, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([10, 10, 30, 30])\n",
            "torch.Size([10, 10, 13, 13])\n",
            "torch.Size([10, 3])\n",
            "Epoch: 8 | train_loss: 1.0553 | train_acc: 0.3924 | test_loss: 1.0468 | test_acc: 0.3187\n",
            " 80% 8/10 [00:07<00:01,  1.01it/s]torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([18, 10, 30, 30])\n",
            "torch.Size([18, 10, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([10, 10, 30, 30])\n",
            "torch.Size([10, 10, 13, 13])\n",
            "torch.Size([10, 3])\n",
            "Epoch: 9 | train_loss: 1.0094 | train_acc: 0.5580 | test_loss: 1.0176 | test_acc: 0.2812\n",
            " 90% 9/10 [00:08<00:00,  1.02it/s]torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([18, 10, 30, 30])\n",
            "torch.Size([18, 10, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([32, 10, 30, 30])\n",
            "torch.Size([32, 10, 13, 13])\n",
            "torch.Size([32, 3])\n",
            "torch.Size([10, 10, 30, 30])\n",
            "torch.Size([10, 10, 13, 13])\n",
            "torch.Size([10, 3])\n",
            "Epoch: 10 | train_loss: 0.9517 | train_acc: 0.6066 | test_loss: 0.9731 | test_acc: 0.5375\n",
            "100% 10/10 [00:08<00:00,  1.11it/s]\n",
            "[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --num_epochs 3 --batch_size 64 --hidden_units 64 --learning_rate 0.002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEWy62UDNicz",
        "outputId": "f691c55f-1d73-4931-a918-f18382e1332c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training a model for 3 epchs with batch size 64 using 64 hidden units and a learning rate of 0.002\n",
            "[INFO] Training data file: data/IFCB_test_train/Train\n",
            "[INFO] Testing data file: data/IFCB_test_train/Test\n",
            "\r  0% 0/3 [00:00<?, ?it/s]torch.Size([64, 64, 30, 30])\n",
            "torch.Size([64, 64, 13, 13])\n",
            "torch.Size([64, 3])\n",
            "torch.Size([64, 64, 30, 30])\n",
            "torch.Size([64, 64, 13, 13])\n",
            "torch.Size([64, 3])\n",
            "torch.Size([64, 64, 30, 30])\n",
            "torch.Size([64, 64, 13, 13])\n",
            "torch.Size([64, 3])\n",
            "torch.Size([18, 64, 30, 30])\n",
            "torch.Size([18, 64, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([42, 64, 30, 30])\n",
            "torch.Size([42, 64, 13, 13])\n",
            "torch.Size([42, 3])\n",
            "Epoch: 1 | train_loss: 1.1200 | train_acc: 0.3194 | test_loss: 1.1028 | test_acc: 0.3333\n",
            " 33% 1/3 [00:01<00:02,  1.19s/it]torch.Size([64, 64, 30, 30])\n",
            "torch.Size([64, 64, 13, 13])\n",
            "torch.Size([64, 3])\n",
            "torch.Size([64, 64, 30, 30])\n",
            "torch.Size([64, 64, 13, 13])\n",
            "torch.Size([64, 3])\n",
            "torch.Size([64, 64, 30, 30])\n",
            "torch.Size([64, 64, 13, 13])\n",
            "torch.Size([64, 3])\n",
            "torch.Size([18, 64, 30, 30])\n",
            "torch.Size([18, 64, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([42, 64, 30, 30])\n",
            "torch.Size([42, 64, 13, 13])\n",
            "torch.Size([42, 3])\n",
            "Epoch: 2 | train_loss: 1.1000 | train_acc: 0.3689 | test_loss: 1.0918 | test_acc: 0.3333\n",
            " 67% 2/3 [00:02<00:00,  1.01it/s]torch.Size([64, 64, 30, 30])\n",
            "torch.Size([64, 64, 13, 13])\n",
            "torch.Size([64, 3])\n",
            "torch.Size([64, 64, 30, 30])\n",
            "torch.Size([64, 64, 13, 13])\n",
            "torch.Size([64, 3])\n",
            "torch.Size([64, 64, 30, 30])\n",
            "torch.Size([64, 64, 13, 13])\n",
            "torch.Size([64, 3])\n",
            "torch.Size([18, 64, 30, 30])\n",
            "torch.Size([18, 64, 13, 13])\n",
            "torch.Size([18, 3])\n",
            "torch.Size([42, 64, 30, 30])\n",
            "torch.Size([42, 64, 13, 13])\n",
            "torch.Size([42, 3])\n",
            "Epoch: 3 | train_loss: 1.0939 | train_acc: 0.3116 | test_loss: 1.0709 | test_acc: 0.3571\n",
            "100% 3/3 [00:02<00:00,  1.03it/s]\n",
            "[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### now google how to publish a python package :D"
      ],
      "metadata": {
        "id": "Vf09nCdzPJo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.py\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import argparse\n",
        "import model_builder\n",
        "\n",
        "# Creating a parser\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Get image path\n",
        "parser.add_argument(\"--image\",\n",
        "                    help=\"target image to predict on\")\n",
        "\n",
        "#Get a model path\n",
        "parser.add_argument(\"--model_path\",\n",
        "                    default=\"models/05_going_modular_script_mode_tinyvgg_model.pth\",\n",
        "                    type=str,\n",
        "                    help=\"target model to use for prediction filepath\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Setup class names\n",
        "class_names = [\"Chaetoceros_decipiens_118\", \"Guinardia_delicatula_095\", \"Tripos_muelleri_008\"]\n",
        "\n",
        "# Setup device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# get the image path\n",
        "IMG_PATH = args.image\n",
        "print(f\"[INFO] Predicting on {IMG_PATH}\")\n",
        "\n",
        "# function to load in model\n",
        "def load_model(filepath=args.model_path):\n",
        "  # Need to use same hyperparameters as saved model\n",
        "  model = model_builder.TinyVGG(input_shape=1, #color channels\n",
        "                                hidden_units=10,\n",
        "                                output_shape=3).to(device) #length of class names\n",
        "  print(f\"Loading in model from: {filepath}\")\n",
        "  # Load in the saved model state dictionary from file\n",
        "  model.load_state_dict(torch.load(filepath))\n",
        "\n",
        "  return model\n",
        "\n",
        "# function to load in model + predict on selected image\n",
        "def predict_on_image(image_path=IMG_PATH, filepath=args.model_path):\n",
        "  # load the model\n",
        "  model = load_model(filepath)\n",
        "\n",
        "  #Load in the image and turn it into torch.float32 (same type as model)\n",
        "  image = torchvision.io.read_image(str(IMG_PATH)).type(torch.float32)\n",
        "\n",
        "  # preprocess the image to get it between zero and 1\n",
        "  image = image / 255.\n",
        "\n",
        "  # Resize the image to be same size as model\n",
        "  # Create transforms\n",
        "  transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((64, 64)),\n",
        "  ])\n",
        "  image = transform(image)\n",
        "\n",
        "  # Predict on image\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    # put image on target device\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Get pred logits\n",
        "    pred_logits = model(image.unsqueeze(0)) #adds a batch dimension\n",
        "\n",
        "    # Get pred probs\n",
        "    pred_prob = torch.softmax(pred_logits, dim=1)\n",
        "\n",
        "    # Get pred labels\n",
        "    pred_label = torch.argmax(pred_prob, dim=1)\n",
        "    pred_label_class = class_names[pred_label]\n",
        "\n",
        "  print(f\"[INFO] Pred class: {pred_label_class}, Pred prob: {pred_prob.max():.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  predict_on_image()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV2ZxAHAAPby",
        "outputId": "85ff571f-245b-419a-fb9c-d95aefe062e2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --image data/IFCB_test_train/Test/Tripos_muelleri_008/D20220331T000144_IFCB139_00001.png\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jwji5kyAPOU",
        "outputId": "3f88b744-c317-40ba-a346-b9da00b9b95c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Predicting on data/IFCB_test_train/Test/Tripos_muelleri_008/D20220331T000144_IFCB139_00001.png\n",
            "Loading in model from: models/05_going_modular_script_mode_tinyvgg_model.pth\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/predict.py\", line 82, in <module>\n",
            "    predict_on_image()\n",
            "  File \"/content/predict.py\", line 47, in predict_on_image\n",
            "    model = load_model(filepath)\n",
            "  File \"/content/predict.py\", line 40, in load_model\n",
            "    model.load_state_dict(torch.load(filepath))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2153, in load_state_dict\n",
            "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
            "RuntimeError: Error(s) in loading state_dict for TinyVGG:\n",
            "\tsize mismatch for conv_block_1.0.weight: copying a param with shape torch.Size([64, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([10, 1, 3, 3]).\n",
            "\tsize mismatch for conv_block_1.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([10]).\n",
            "\tsize mismatch for conv_block_1.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([10, 10, 3, 3]).\n",
            "\tsize mismatch for conv_block_1.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([10]).\n",
            "\tsize mismatch for conv_block_2.0.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([10, 10, 3, 3]).\n",
            "\tsize mismatch for conv_block_2.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([10]).\n",
            "\tsize mismatch for conv_block_2.2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([10, 10, 3, 3]).\n",
            "\tsize mismatch for conv_block_2.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([10]).\n",
            "\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([3, 10816]) from checkpoint, the shape in current model is torch.Size([3, 1690]).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFER LEARNING VIDEO NOTEBOOK 06\n",
        "putting this notebook in here to have all the models and python scripts already made"
      ],
      "metadata": {
        "id": "ZD43ou0sevSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06. PyTorch Transfer Learning\n",
        "\n",
        "> **Note:** This notebook uses `torchvision`'s new [multi-weight support API (available in `torchvision` v0.13+)](https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/).\n",
        "\n",
        "We've built a few models by hand so far.\n",
        "\n",
        "But their performance has been poor.\n",
        "\n",
        "You might be thinking, **is there a well-performing model that already exists for our problem?**\n",
        "\n",
        "And in the world of deep learning, the answer is often *yes*.\n",
        "\n",
        "We'll see how by using a powerful technique called [**transfer learning**](https://developers.google.com/machine-learning/glossary#transfer-learning)."
      ],
      "metadata": {
        "id": "p8B4sw44E42n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is transfer learning?\n",
        "\n",
        "**Transfer learning** allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\n",
        "\n",
        "For example, we can take the patterns a computer vision model has learned from datasets such as [ImageNet](https://www.image-net.org/) (millions of images of different objects) and use them to power our FoodVision Mini model.\n",
        "\n",
        "Or we could take the patterns from a [language model](https://developers.google.com/machine-learning/glossary#masked-language-model) (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.\n",
        "\n",
        "The premise remains: find a well-performing existing model and apply it to your own problem.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-example-overview.png\" alt=\"transfer learning overview on different problems\" width=900/>\n",
        "\n",
        "*Example of transfer learning being applied to computer vision and natural language processing (NLP). In the case of computer vision, a computer vision model might learn patterns on millions of images in ImageNet and then use those patterns to infer on another problem. And for NLP, a language model may learn the structure of language by reading all of Wikipedia (and perhaps more) and then apply that knowledge to a different problem.*"
      ],
      "metadata": {
        "id": "TmV96BotE1Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why use transfer learning?\n",
        "\n",
        "There are two main benefits to using transfer learning:\n",
        "\n",
        "1. Can leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.\n",
        "2. Can leverage a working model which has **already learned** patterns on similar data to our own. This often results in achieving **great results with less custom data**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-for-foodvision-mini%20.png\" alt=\"transfer learning applied to FoodVision Mini\" width=900/>\n",
        "\n",
        "*We'll be putting these to the test for our FoodVision Mini problem, we'll take a computer vision model pretrained on ImageNet and try to leverage its underlying learned representations for classifying images of pizza, steak and sushi.*\n",
        "\n",
        "Both research and practice support the use of transfer learning too.\n",
        "\n",
        "A finding from a recent machine learning research paper recommended practioner's use transfer learning wherever possible.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-how-to-train-your-vit-section-6-transfer-learning-highlight.png\" width=900 alt=\"how to train your vision transformer paper section 6, advising to use transfer learning if you can\"/>\n",
        "\n",
        "*A study into the effects of whether training from scratch or using transfer learning was better from a practioner's point of view, found transfer learning to be far more beneficial in terms of cost and time. **Source:** [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270) paper section 6 (conclusion).*\n",
        "\n",
        "And Jeremy Howard (founder of [fastai](https://www.fast.ai/)) is a big proponent of transfer learning.\n",
        "\n",
        "> The things that really make a difference (transfer learning), if we can do better at transfer learning, it’s this world changing thing. Suddenly lots more people can do world-class work with less resources and less data. — [Jeremy Howard on the Lex Fridman Podcast](https://youtu.be/Bi7f1JSSlh8?t=72)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vEDUJ_0yBjyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where to find pretrained models\n",
        "\n",
        "The world of deep learning is an amazing place.\n",
        "\n",
        "So amazing that many people around the world share their work.\n",
        "\n",
        "Often, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.\n",
        "\n",
        "And there are several places you can find pretrained models to use for your own problems.\n",
        "\n",
        "| **Location** | **What's there?** | **Link(s)** |\n",
        "| ----- | ----- | ----- |\n",
        "| **PyTorch domain libraries** | Each of the PyTorch domain libraries (`torchvision`, `torchtext`) come with pretrained models of some form. The models there work right within PyTorch. | [`torchvision.models`](https://pytorch.org/vision/stable/models.html), [`torchtext.models`](https://pytorch.org/text/main/models.html), [`torchaudio.models`](https://pytorch.org/audio/stable/models.html), [`torchrec.models`](https://pytorch.org/torchrec/torchrec.models.html) |\n",
        "| **HuggingFace Hub** | A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. | https://huggingface.co/models, https://huggingface.co/datasets |\n",
        "| **`timm` (PyTorch Image Models) library** | Almost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. | https://github.com/rwightman/pytorch-image-models|\n",
        "| **Paperswithcode** | A collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. | https://paperswithcode.com/ |\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-where-to-find-pretrained-models.png\" alt=\"different locations to find pretrained neural network models\" width=900/>\n",
        "\n",
        "*With access to such high-quality resources as above, it should be common practice at the start of every deep learning problem you take on to ask, \"Does a pretrained model exist for my problem?\"*\n",
        "\n",
        "> **Exercise:** Spend 5-minutes going through [`torchvision.models`](https://pytorch.org/vision/stable/models.html) as well as the [HuggingFace Hub Models page](https://huggingface.co/models), what do you find? (there's no right answers here, it's just to practice exploring)"
      ],
      "metadata": {
        "id": "tSmhakJ9FKAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What we're going to cover\n",
        "\n",
        "We're going to take a pretrained model from `torchvision.models` and customise it to work on (and hopefully improve) our FoodVision Mini problem.\n",
        "\n",
        "| **Topic** | **Contents** |\n",
        "| ----- | ----- |\n",
        "| **0. Getting setup** | We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. |\n",
        "| **1. Get data** | Let's get the pizza, steak and sushi image classification dataset we've been using to try and improve our model's results. |\n",
        "| **2. Create Datasets and DataLoaders** | We'll use the `data_setup.py` script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. |\n",
        "| **3. Get and customise a pretrained model** | Here we'll download a pretrained model from `torchvision.models` and customise it to our own problem. |\n",
        "| **4. Train model** | Let's see how the new pretrained model goes on our pizza, steak, sushi dataset. We'll use the training functions we created in the previous chapter. |\n",
        "| **5. Evaluate the model by plotting loss curves** | How did our first transfer learning model go? Did it overfit or underfit?  |\n",
        "| **6. Make predictions on images from the test set** | It's one thing to check out a model's evaluation metrics but it's another thing to view its predictions on test samples, let's *visualize, visualize, visualize*! |"
      ],
      "metadata": {
        "id": "CfhD9JF1FJ7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where can you get help?\n",
        "All of the materials for this course are available on GitHub.\n",
        "\n",
        "If you run into trouble, you can ask a question on the course GitHub Discussions page.\n",
        "\n",
        "And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.\n"
      ],
      "metadata": {
        "id": "eVGT2su2FJ3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Getting setup\n",
        "\n",
        "Let's get started by importing/downloading the required modules for this section.\n",
        "\n",
        "To save us writing extra code, we're going to be leveraging some of the Python scripts (such as `data_setup.py` and `engine.py`) we created in the previous section, [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n",
        "\n",
        "We'll get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.\n",
        "\n",
        "`torchinfo` will help later on to give us a visual representation of our model.\n",
        "\n",
        "> **Note:** As of June 2022, this notebook uses the nightly versions of `torch` and `torchvision` as `torchvision` v0.13+ is required for using the updated multi-weights API. You can install these using the command below."
      ],
      "metadata": {
        "id": "5WLYAOYJFJzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU\n",
        "  **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`."
      ],
      "metadata": {
        "id": "djXfHNnRJbZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIcini4KFJu9",
        "outputId": "cb4e57e6-7650-4998-fab2-fccbd692db02"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "torch version: 2.2.1+cu121\n",
            "torchvision version: 0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "import data_setup, engine\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6PulCd-FJnC",
        "outputId": "3a919507-82c9-4f29-b225-678e688cb857"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's setup device agnostic code.\n",
        "\n",
        "> **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`."
      ],
      "metadata": {
        "id": "vwWyEj91JKz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uDV9uyRpM9Zc",
        "outputId": "149b16ec-fbe6-46c7-f087-170d8ca38cc8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Get data\n",
        "Before we can start to use transfer learning, we'll need a dataset.\n",
        "\n",
        "To see how transfer learning compares to our previous attempts at model building, we'll download the same dataset we've been using for FoodVision Mini.\n",
        "\n",
        "Let's download  the IFCB dataset, if we've already got the data, it doesn't redownload."
      ],
      "metadata": {
        "id": "C4bGj4hLM9V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python get_data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6JuB3IfM9RN",
        "outputId": "4c88f7f3-14d2-42e4-f199-e2afd2c5b0e6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/IFCB_test_train directory exists.\n",
            "Downloading IFCB data...\n",
            "Unzipping IFCB_test_train data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "\n",
        "Since we've downloaded the `going_modular` directory, we can use the [`data_setup.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py) script we created in section [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy) to prepare and setup our DataLoaders.\n",
        "\n",
        "***But since we'll be using a pretrained model from [`torchvision.models`](https://pytorch.org/vision/stable/models.html), there's a specific transform we need to prepare our images first.***"
      ],
      "metadata": {
        "id": "45L3-9-hM9Mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Creating a transform for `torchvision.models` (manual creation)\n",
        "\n",
        "> **Note:** As of `torchvision` v0.13+, there's an update to how data transforms can be created using `torchvision.models`. I've called the previous method \"manual creation\" and the new method \"auto creation\". This notebook showcases both.\n",
        "\n",
        "When using a pretrained model, it's important that **your custom data going into the model is prepared in the same way as the original training data that went into the model**.\n",
        "\n",
        "Prior to `torchvision` v0.13+, to create a transform for a pretrained model in `torchvision.models`, the documentation stated:\n",
        "\n",
        "> All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.\n",
        ">\n",
        "> The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`.\n",
        ">\n",
        "> You can use the following transform to normalize:\n",
        ">\n",
        "> ```\n",
        "> normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        ">                                  std=[0.229, 0.224, 0.225])\n",
        "> ```\n",
        "\n",
        "The good news is, we can achieve the above transformations with a combination of:\n",
        "\n",
        "| **Transform number** | **Transform required** | **Code to perform transform** |\n",
        "| ----- | ----- | ----- |\n",
        "| 1 | Mini-batches of size `[batch_size, 3, height, width]` where height and width are at least 224x224^. | `torchvision.transforms.Resize()` to resize images into `[3, 224, 224]`^ and `torch.utils.data.DataLoader()` to create batches of images. |\n",
        "| 2 | Values between 0 & 1. | `torchvision.transforms.ToTensor()` |\n",
        "| 3 | A mean of `[0.485, 0.456, 0.406]` (values across each colour channel). | `torchvision.transforms.Normalize(mean=...)` to adjust the mean of our images.  |\n",
        "| 4 | A standard deviation of `[0.229, 0.224, 0.225]` (values across each colour channel). | `torchvision.transforms.Normalize(std=...)` to adjust the standard deviation of our images.  |\n",
        "\n",
        "> **Note:** ^some pretrained models from `torchvision.models` in different sizes to `[3, 224, 224]`, for example, some might take them in `[3, 240, 240]`. For specific input image sizes, see the documentation.\n",
        "\n",
        "> **Question:** *Where did the mean and standard deviation values come from? Why do we need to do this?*\n",
        ">\n",
        "> These were calculated from the data. Specifically, the ImageNet dataset by taking the means and standard deviations across a subset of images.\n",
        ">\n",
        "> We also don't *need* to do this. Neural networks are usually quite capable of figuring out appropriate data distributions (they'll calculate where the mean and standard deviations need to be on their own) but setting them at the start can help our networks achieve better performance quicker.\n",
        "\n",
        "Let's compose a series of `torchvision.transforms` to perform the above steps."
      ],
      "metadata": {
        "id": "hKuX-g4HM9IB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transforms pipeline manually (required for torchvision < 0.13)\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "])"
      ],
      "metadata": {
        "id": "9AnZtjtBM8-v"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wonderful!\n",
        "\n",
        "Now we've got a **manually created series of transforms** ready to prepare our images, let's create training and testing DataLoaders.\n",
        "\n",
        "We can create these using the `create_dataloaders` function from the [`data_setup.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py) script we created in [05. PyTorch Going Modular Part 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).\n",
        "\n",
        "We'll set `batch_size=32` so our model see's mini-batches of 32 samples at a time.\n",
        "\n",
        "And we can transform our images using the transform pipeline we created above by setting `transform=manual_transforms`.\n",
        "\n",
        "> **Note:** I've included this manual creation of transforms in this notebook because you may come across resources that use this style. It's also important to note that because these transforms are manually created, they're also infinitely customizable. So if you wanted to included data augmentation techniques in your transforms pipeline, you could."
      ],
      "metadata": {
        "id": "62QqGLQcUoHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n",
        "                                                                               batch_size=32) # set mini-batch size to 32\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "kUepWw9uUtRY",
        "outputId": "9fd7220c-b7e9-4f8e-bd75-32eaee7e638c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7c77222173a0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7c76290b9210>,\n",
              " ['Chaetoceros_decipiens_118',\n",
              "  'Guinardia_delicatula_095',\n",
              "  'Tripos_muelleri_008'])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Creating a transform for `torchvision.models` (auto creation)\n",
        "\n",
        "As previously stated, when using a pretrained model, it's important that **your custom data going into the model is prepared in the same way as the original training data that went into the model**.\n",
        "\n",
        "Above we saw how to manually create a transform for a pretrained model.\n",
        "\n",
        "But as of `torchvision` v0.13+, an automatic transform creation feature has been added.\n",
        "\n",
        "When you setup a model from `torchvision.models` and select the pretrained model weights you'd like to use, for example, say we'd like to use:\n",
        "    \n",
        "```python\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "```\n",
        "\n",
        "Where,\n",
        "* `EfficientNet_B0_Weights` is the model architecture weights we'd like to use (there are many differnt model architecture options in `torchvision.models`).\n",
        "* `DEFAULT` means the *best available* weights (the best performance in ImageNet).\n",
        "    * **Note:** Depending on the model architecture you choose, you may also see other options such as `IMAGENET_V1` and `IMAGENET_V2` where generally the higher version number the better. Though if you want the best available, `DEFAULT` is the easiest option. See the [`torchvision.models` documentation](https://pytorch.org/vision/main/models.html) for more.\n",
        "    \n",
        "Let's try it out."
      ],
      "metadata": {
        "id": "X0LsnOW2UtMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of pretrained model weights\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\n",
        "weights"
      ],
      "metadata": {
        "id": "7e73hi3pUtHH",
        "outputId": "6393c7e1-e388-4499-bccf-d07abca22780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EfficientNet_B0_Weights.IMAGENET1K_V1"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now to access the transforms assosciated with our `weights`, we can use the `transforms()` method.\n",
        "\n",
        "This is essentially saying \"get the data transforms that were used to train the `EfficientNet_B0_Weights` on ImageNet\"."
      ],
      "metadata": {
        "id": "yyPmzMR6ACXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the transforms used to create our pretrained weights\n",
        "auto_transforms = weights.transforms()\n",
        "auto_transforms"
      ],
      "metadata": {
        "id": "iH-G7B8rAEfp",
        "outputId": "75c890c7-1385-46db-ede5-782989449d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[256]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BICUBIC\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how auto_transforms is very similar to manual_transforms, the only difference is that auto_transforms came with the model architecture we chose, where as we had to create manual_transforms by hand.\n",
        "\n",
        "The benefit of automatically creating a transform through weights.transforms() is that you ensure you're using the same data transformation as the pretrained model used when it was trained.\n",
        "\n",
        "However, the tradeoff of using automatically created transforms is a lack of customization.\n",
        "\n",
        "We can use auto_transforms to create DataLoaders with create_dataloaders() just as before."
      ],
      "metadata": {
        "id": "D4fIoiL_AFB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n",
        "                                                                               batch_size=32) # set mini-batch size to 32\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "Yfjiul4TAE8m",
        "outputId": "5e2d3e3e-cd4b-4aa9-d2ba-8352d802601f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7c76290ba6b0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7c76290b9ae0>,\n",
              " ['Chaetoceros_decipiens_118',\n",
              "  'Guinardia_delicatula_095',\n",
              "  'Tripos_muelleri_008'])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Getting a pretrained model\n",
        "\n",
        "Alright, here comes the fun part!\n",
        "\n",
        "Over the past few notebooks we've been building PyTorch neural networks from scratch.\n",
        "\n",
        "And while that's a good skill to have, our models haven't been performing as well as we'd like.\n",
        "\n",
        "That's where **transfer learning** comes in.\n",
        "\n",
        "The whole idea of transfer learning is to **take an already well-performing model on a problem-space similar to yours and then customising it to your use case**.\n",
        "\n",
        "Since we're working on a computer vision problem (image classification with FoodVision Mini), we can find pretrained classification models in [`torchvision.models`](https://pytorch.org/vision/stable/models.html#classification).\n",
        "\n",
        "Exploring the documentation, you'll find plenty of common computer vision architecture backbones such as:\n",
        "\n",
        "| **Architecuture backbone** | **Code** |\n",
        "| ----- | ----- |\n",
        "| [ResNet](https://arxiv.org/abs/1512.03385)'s | `torchvision.models.resnet18()`, `torchvision.models.resnet50()`... |\n",
        "| [VGG](https://arxiv.org/abs/1409.1556) (similar to what we used for TinyVGG) | `torchvision.models.vgg16()` |\n",
        "| [EfficientNet](https://arxiv.org/abs/1905.11946)'s | `torchvision.models.efficientnet_b0()`, `torchvision.models.efficientnet_b1()`... |\n",
        "| [VisionTransformer](https://arxiv.org/abs/2010.11929) (ViT's)| `torchvision.models.vit_b_16()`, `torchvision.models.vit_b_32()`... |\n",
        "| [ConvNeXt](https://arxiv.org/abs/2201.03545) | `torchvision.models.convnext_tiny()`,  `torchvision.models.convnext_small()`... |\n",
        "| More available in `torchvision.models` | `torchvision.models...` |"
      ],
      "metadata": {
        "id": "yEEhSrIoAE4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Which pretrained model should you use?\n",
        "\n",
        "It depends on your problem/the device you're working with.\n",
        "\n",
        "Generally, the higher number in the model name (e.g. `efficientnet_b0()` -> `efficientnet_b1()` -> `efficientnet_b7()`) means *better performance* but a *larger* model.\n",
        "\n",
        "You might think better performance is *always better*, right?\n",
        "\n",
        "That's true but **some better performing models are too big for some devices**.\n",
        "\n",
        "For example, say you'd like to run your model on a mobile-device, you'll have to take into account the limited compute resources on the device, thus you'd be looking for a smaller model.\n",
        "\n",
        "But if you've got unlimited compute power, as [*The Bitter Lesson*](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) states, you'd likely take the biggest, most compute hungry model you can.\n",
        "\n",
        "Understanding this **performance vs. speed vs. size tradeoff** will come with time and practice.\n",
        "\n",
        "For me, I've found a nice balance in the `efficientnet_bX` models.\n",
        "\n",
        "As of May 2022, [Nutrify](https://nutrify.app) (the machine learning powered app I'm working on) is powered by an `efficientnet_b0`.\n",
        "\n",
        "[Comma.ai](https://comma.ai/) (a company that makes open source self-driving car software) [uses an `efficientnet_b2`](https://geohot.github.io/blog/jekyll/update/2021/10/29/an-architecture-for-life.html) to learn a representation of the road.\n",
        "\n",
        "> **Note:** Even though we're using `efficientnet_bX`, it's important not to get too attached to any one architecture, as they are always changing as new research gets released. Best to experiment, experiment, experiment and see what works for your problem."
      ],
      "metadata": {
        "id": "y8t4LdazAEy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Setting up a pretrained model\n",
        "\n",
        "The pretrained model we're going to be using is [`torchvision.models.efficientnet_b0()`](https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html).\n",
        "\n",
        "The architecture is from the paper *[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)*.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-effnet-b0-feature-extractor.png\" alt=\"efficienet_b0 from PyTorch torchvision feature extraction model\" width=900/>\n",
        "\n",
        "*Example of what we're going to create, a pretrained [`EfficientNet_B0` model](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html) from `torchvision.models` with the output layer adjusted for our use case of classifying pizza, steak and sushi images.*\n",
        "\n",
        "We can setup the `EfficientNet_B0` pretrained ImageNet weights using the same code as we used to create the transforms.\n",
        "\n",
        "\n",
        "```python\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights for ImageNet\n",
        "```\n",
        "\n",
        "This means the model has already been trained on millions of images and has a good base representation of image data.\n",
        "\n",
        "The PyTorch version of this pretrained model is capable of achieving ~77.7% accuracy across ImageNet's 1000 classes.\n",
        "\n",
        "We'll also send it to the target device."
      ],
      "metadata": {
        "id": "hbY5iA4oAEvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n",
        "# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n",
        "\n",
        "# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
        "\n",
        "#model # uncomment to output (it's very long)"
      ],
      "metadata": {
        "id": "7L74epEOAEmY",
        "outputId": "0fe22fc4-b1ac-453f-e0b3-07bad0c4bdd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 154MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: In previous versions of torchvision, you'd create a pretrained model with code like:\n",
        "\n",
        "model = torchvision.models.efficientnet_b0(pretrained=True).to(device)\n",
        "\n",
        "However, running this using torchvision v0.13+ will result in errors such as the following:\n",
        "\n",
        "UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
        "\n",
        "And...\n",
        "\n",
        "UserWarning: Arguments other than a weight enum or None for weights are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing weights=EfficientNet_B0_Weights.IMAGENET1K_V1. You can also use weights=EfficientNet_B0_Weights.DEFAULT to get the most up-to-date weights."
      ],
      "metadata": {
        "id": "jc8FNjkjAvCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we print the model, we get something similar to the following:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-v2-effnetb0-model-print-out.png\" alt=\"output of printing the efficientnet_b0 model from torchvision.models\" width=900/>\n",
        "\n",
        "Lots and lots and lots of layers.\n",
        "\n",
        "This is one of the benefits of transfer learning, taking an existing model, that's been crafted by some of the best engineers in the world and applying to your own problem.\n",
        "\n",
        "Our `efficientnet_b0` comes in three main parts:\n",
        "1. `features` - A collection of convolutional layers and other various activation layers to learn a base representation of vision data (this base representation/collection of layers is often referred to as **features** or **feature extractor**, \"the base layers of the model learn the different **features** of images\").\n",
        "2. `avgpool` - Takes the average of the output of the `features` layer(s) and turns it into a **feature vector**.\n",
        "3. `classifier` - Turns the **feature vector** into a vector with the same dimensionality as the number of required output classes (since `efficientnet_b0` is pretrained on ImageNet and because ImageNet has 1000 classes, `out_features=1000` is the default)."
      ],
      "metadata": {
        "id": "kNK0ZrxRA3IN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Getting a summary of our model with `torchinfo.summary()`\n",
        "\n",
        "To learn more about our model, let's use `torchinfo`'s [`summary()` method](https://github.com/TylerYep/torchinfo#documentation).\n",
        "\n",
        "To do so, we'll pass in:\n",
        " * `model` - the model we'd like to get a summary of.\n",
        " * `input_size` - the shape of the data we'd like to pass to our model, for the case of `efficientnet_b0`, the input size is `(batch_size, 3, 224, 224)`, though [other variants of `efficientnet_bX` have different input sizes](https://github.com/pytorch/vision/blob/d2bfd639e46e1c5dc3c177f889dc7750c8d137c7/references/classification/train.py#L92-L93).\n",
        "    * **Note:** Many modern models can handle input images of varying sizes thanks to [`torch.nn.AdaptiveAvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html), this layer adaptively adjusts the `output_size` of a given input as required. You can try this out by passing different size input images to `summary()` or your models.\n",
        " * `col_names` - the various information columns we'd like to see about our model.\n",
        " * `col_width` - how wide the columns should be for the summary.\n",
        " * `row_settings` - what features to show in a row."
      ],
      "metadata": {
        "id": "Ylc4S8eHA3Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary using torchinfo (uncomment for actual output)\n",
        "summary(model=model,\n",
        "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "mgWDkZq2A2_W",
        "outputId": "ebb70ffc-1b6e-4548-cee4-169c0d187589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   True\n",
              "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   True\n",
              "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   True\n",
              "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   864                  True\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   64                   True\n",
              "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
              "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\n",
              "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     6,004                True\n",
              "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n",
              "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     15,350               True\n",
              "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     31,290               True\n",
              "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     37,130               True\n",
              "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n",
              "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n",
              "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    126,004              True\n",
              "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n",
              "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n",
              "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      262,492              True\n",
              "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n",
              "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n",
              "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n",
              "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      717,232              True\n",
              "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   True\n",
              "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     409,600              True\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     2,560                True\n",
              "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
              "├─Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   True\n",
              "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
              "│    └─Linear (1)                                            [32, 1280]           [32, 1000]           1,281,000            True\n",
              "============================================================================================================================================\n",
              "Total params: 5,288,548\n",
              "Trainable params: 5,288,548\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 12.35\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3452.35\n",
              "Params size (MB): 21.15\n",
              "Estimated Total Size (MB): 3492.77\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-torchinfo-summary-unfrozen-layers.png\" alt=\"output of torchinfo.summary() when passed our model with all layers as trainable\" width=900/>\n",
        "\n",
        "Woah!\n",
        "\n",
        "Now that's a big model!\n",
        "\n",
        "From the output of the summary, we can see all of the various input and output shape changes as our image data goes through the model.\n",
        "\n",
        "And there are a whole bunch more total parameters (pretrained weights) to recognize different patterns in our data.\n",
        "\n",
        "For reference, our model from previous sections, **TinyVGG had 8,083 parameters vs. 5,288,548 parameters for `efficientnet_b0`, an increase of ~654x**!\n",
        "\n",
        "What do you think, will this mean better performance?"
      ],
      "metadata": {
        "id": "U-cUd5PSA274"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Freezing the base model and changing the output layer to suit our needs\n",
        "\n",
        "The process of transfer learning usually goes: freeze some base layers of a pretrained model (typically the `features` section) and then adjust the output layers (also called head/classifier layers) to suit your needs.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-v2-effnet-changing-the-classifier-head.png\" alt=\"changing the efficientnet classifier head to a custom number of outputs\" width=900/>\n",
        "\n",
        "*You can customise the outputs of a pretrained model by changing the output layer(s) to suit your problem. The original `torchvision.models.efficientnet_b0()` comes with `out_features=1000` because there are 1000 classes in ImageNet, the dataset it was trained on. However, for our problem, classifying images of pizza, steak and sushi we only need `out_features=3`.*\n",
        "\n",
        "Let's freeze all of the layers/parameters in the `features` section of our `efficientnet_b0` model.\n",
        "\n",
        "> **Note:** To *freeze* layers means to keep them how they are during training. For example, if your model has pretrained layers, to *freeze* them would be to say, \"don't change any of the patterns in these layers during training, keep them how they are.\" In essence, we'd like to keep the pretrained weights/patterns our model has learned from ImageNet as a backbone and then only change the output layers.\n",
        "\n",
        "We can freeze all of the layers/parameters in the `features` section by setting the attribute `requires_grad=False`.\n",
        "\n",
        "For parameters with `requires_grad=False`, PyTorch doesn't track gradient updates and in turn, these parameters won't be changed by our optimizer during training.\n",
        "\n",
        "In essence, a parameter with `requires_grad=False` is \"untrainable\" or \"frozen\" in place."
      ],
      "metadata": {
        "id": "TMNQz0_nA24L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "7QeCPlOIBuXE"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extractor layers frozen!\n",
        "\n",
        "Let's now adjust the output layer or the `classifier` portion of our pretrained model to our needs.\n",
        "\n",
        "Right now our pretrained model has `out_features=1000` because there are 1000 classes in ImageNet.\n",
        "\n",
        "However, we don't have 1000 classes, we only have three, pizza, steak and sushi.\n",
        "\n",
        "We can change the `classifier` portion of our model by creating a new series of layers.\n",
        "\n",
        "The current `classifier` consists of:\n",
        "\n",
        "```\n",
        "(classifier): Sequential(\n",
        "    (0): Dropout(p=0.2, inplace=True)\n",
        "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
        "```\n",
        "\n",
        "We'll keep the `Dropout` layer the same using [`torch.nn.Dropout(p=0.2, inplace=True)`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html).\n",
        "\n",
        "> **Note:** [Dropout layers](https://developers.google.com/machine-learning/glossary#dropout_regularization) randomly remove connections between two neural network layers with a probability of `p`. For example, if `p=0.2`, 20% of connections between neural network layers will be removed at random each pass. This practice is meant to help regularize (prevent overfitting) a model by making sure the connections that remain learn features to compensate for the removal of the other connections (hopefully these remaining features are *more general*).\n",
        "\n",
        "And we'll keep `in_features=1280` for our `Linear` output layer but we'll change the `out_features` value to the length of our `class_names` (`len(['pizza', 'steak', 'sushi']) = 3`).\n",
        "\n",
        "Our new `classifier` layer should be on the same device as our `model`."
      ],
      "metadata": {
        "id": "5tEDCT9FByX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Get the length of class_names (one output unit for each class)\n",
        "output_shape = len(class_names)\n",
        "\n",
        "# Recreate the classifier layer and seed it to the target device\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(in_features=1280,\n",
        "                    out_features=output_shape, # same number of output units as our number of classes\n",
        "                    bias=True)).to(device)"
      ],
      "metadata": {
        "id": "RfDFRSLzB6Y8"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice!\n",
        "\n",
        "Output layer updated, let's get another summary of our model and see what's changed."
      ],
      "metadata": {
        "id": "ufK8yvHaB6U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\n",
        "summary(model,\n",
        "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
        "        verbose=0,\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "7HccmtkXB6Oc",
        "outputId": "1734c37b-223f-46d3-cd53-c99eeade636c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
              "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
              "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
              "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
              "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
              "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
              "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
              "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
              "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
              "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
              "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
              "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
              "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
              "├─Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n",
              "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
              "│    └─Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n",
              "============================================================================================================================================\n",
              "Total params: 4,011,391\n",
              "Trainable params: 3,843\n",
              "Non-trainable params: 4,007,548\n",
              "Total mult-adds (G): 12.31\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3452.09\n",
              "Params size (MB): 16.05\n",
              "Estimated Total Size (MB): 3487.41\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-torchinfo-summary-frozen-layers.png\" alt=\"output of torchinfo.summary() after freezing multiple layers in our model and changing the classifier head\" width=900/>\n",
        "\n",
        "Ho, ho! There's a fair few changes here!\n",
        "\n",
        "Let's go through them:\n",
        "* **Trainable column** - You'll see that many of the base layers (the ones in the `features` portion) have their Trainable value as `False`. This is because we set their attribute `requires_grad=False`. Unless we change this, these layers won't be updated during furture training.\n",
        "* **Output shape of `classifier`** - The `classifier` portion of the model now has an Output Shape value of `[32, 3]` instead of `[32, 1000]`. It's Trainable value is also `True`. This means its parameters will be updated during training. In essence, we're using the `features` portion to feed our `classifier` portion a base representation of an image and then our `classifier` layer is going to learn how to base representation aligns with our problem.\n",
        "* **Less trainable parameters** - Previously there was 5,288,548 trainable parameters. But since we froze many of the layers of the model and only left the `classifier` as trainable, there's now only 3,843 trainable parameters (even less than our TinyVGG model). Though there's also 4,007,548 non-trainable parameters, these will create a base representation of our input images to feed into our `classifier` layer.\n",
        "\n",
        "> **Note:** The more trainable parameters a model has, the more compute power/longer it takes to train. Freezing the base layers of our model and leaving it with less trainable parameters means our model should train quite quickly. This is one huge benefit of transfer learning, taking the already learned parameters of a model trained on a problem similar to yours and only tweaking the outputs slightly to suit your problem."
      ],
      "metadata": {
        "id": "mPate7kwB6KV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wonderful!\n",
        "\n",
        "To train our model, we can use `train()` function we defined in the [05. PyTorch Going Modular section 04](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them).\n",
        "\n",
        "The `train()` function is in the [`engine.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py) script inside the [`going_modular` directory](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular/going_modular).\n",
        "\n",
        "Let's see how long it takes to train our model for 5 epochs.\n",
        "\n",
        "> **Note:** We're only going to be training the parameters `classifier` here as all of the other parameters in our model have been frozen."
      ],
      "metadata": {
        "id": "QsixQti1B6GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "results = engine.train(model=model,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=5,\n",
        "                       device=device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "sTMc6PkmCfTk",
        "outputId": "a90e8444-13f8-493b-b902-43fceb272069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "69e53e04a7c54b99985af754aa55be14",
            "9f47a7a3a2b947b4ac3383582eff92ef",
            "47ee4c9442184207adacefdf5d5da32e",
            "f5ac750f49f24036a12bfa5da2f72f27",
            "774830d23c0a4b24ae5d4b005dc0b0c5",
            "b6b44949c4444c8a9d87453aa81e661e",
            "8884a505b2a74d8e98b37c943b1849d3",
            "be86f25a13cd4e9fb9c9528f450bc291",
            "d01dca085db541a9b6a4dcee35b64d99",
            "fb95e8fd5f414e1d91f1f7b698b26688",
            "20913245b6ae425fb230569f44cce306"
          ]
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69e53e04a7c54b99985af754aa55be14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 1.1056 | train_acc: 0.3323 | test_loss: 0.9996 | test_acc: 0.5469\n",
            "Epoch: 2 | train_loss: 1.1095 | train_acc: 0.3542 | test_loss: 1.0909 | test_acc: 0.3156\n",
            "Epoch: 3 | train_loss: 1.0837 | train_acc: 0.3467 | test_loss: 1.0910 | test_acc: 0.2969\n",
            "Epoch: 4 | train_loss: 1.1002 | train_acc: 0.3031 | test_loss: 1.0747 | test_acc: 0.4969\n",
            "Epoch: 5 | train_loss: 1.1101 | train_acc: 0.2817 | test_loss: 1.0548 | test_acc: 0.4656\n",
            "[INFO] Total training time: 11.731 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow!\n",
        "\n",
        "Our model trained quite fast (~5 seconds on my local machine with a [NVIDIA TITAN RTX GPU](https://www.nvidia.com/en-au/deep-learning-ai/products/titan-rtx/)/about 15 seconds on Google Colab with a [NVIDIA P100 GPU](https://www.nvidia.com/en-au/data-center/tesla-p100/)).\n",
        "\n",
        "And it looks like it smashed our (NOOPE) previous model results out of the park!\n",
        "\n",
        "With an `efficientnet_b0` backbone, our model achieves almost 85%+ accuracy on the test dataset, almost *double* what we were able to achieve with TinyVGG.\n",
        "\n",
        "Not bad for a model we downloaded with a few lines of code."
      ],
      "metadata": {
        "id": "mqHra7owCe8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it\n",
        "try:\n",
        "    from helper_functions import plot_loss_curves\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n",
        "    with open(\"helper_functions.py\", \"wb\") as f:\n",
        "        import requests\n",
        "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "        f.write(request.content)\n",
        "    from helper_functions import plot_loss_curves\n",
        "\n",
        "# Plot the loss curves of our model\n",
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "9w--CCcIDJ3j",
        "outputId": "f95f5758-0cd4-49d6-d9b9-3d3413f0f544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find helper_functions.py, downloading...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x700 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABMcAAAJwCAYAAACNjAagAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD0I0lEQVR4nOzdd3gU5frG8e9ueiEBEkgogQRCld4jXZCiIMUCNkQBBeUcFbEdyw/FcsRyUARROoqiKCiKIBCkSeiEXkNCDxBKOqn7+2PIYqQGkswmuT/XtddOZmc39yaBTJ553+e12Gw2GyIiIiIiIiIiIiWQ1ewAIiIiIiIiIiIiZlFxTERERERERERESiwVx0REREREREREpMRScUxEREREREREREosFcdERERERERERKTEUnFMRERERERERERKLBXHRERERERERESkxFJxTERERERERERESiwVx0REREREREREpMRScUxEREREREREREosFcdExOFNnz4di8XCxo0bzY4iIiIiIhdNmDABi8VCy5YtzY4iInJLVBwTERERERGRPJs1axbBwcGsX7+eAwcOmB1HROSmqTgmIiIiIiIieRIdHc2aNWv45JNPKFeuHLNmzTI70hUlJyebHUFEigAVx0SkWNiyZQvdu3fHx8cHb29vOnXqxNq1a3Mdk5GRwVtvvUWNGjVwd3fHz8+PNm3asGTJEvsxsbGxPP7441SuXBk3NzcqVKhAr169iImJKeR3JCIiIuK4Zs2aRZkyZbj77ru57777rlgcO3/+PM8//zzBwcG4ublRuXJlBgwYQFxcnP2YCxcuMGrUKGrWrIm7uzsVKlSgb9++REVFAbB8+XIsFgvLly/P9doxMTFYLBamT59u3zdw4EC8vb2JiorirrvuolSpUjz88MMArFq1ivvvv58qVarg5uZGUFAQzz//PKmpqZfl3rNnDw888ADlypXDw8ODWrVq8dprrwHw559/YrFYmDdv3mXP+/bbb7FYLEREROT56yki5nI2O4CIyK3auXMnbdu2xcfHh5deegkXFxe+/PJLOnTowIoVK+x9MEaNGsX777/P4MGDadGiBQkJCWzcuJHNmzdz5513AnDvvfeyc+dO/vWvfxEcHMypU6dYsmQJhw8fJjg42MR3KSIiIuI4Zs2aRd++fXF1deXBBx/kiy++YMOGDTRv3hyApKQk2rZty+7du3niiSdo0qQJcXFxzJ8/n6NHj+Lv709WVhY9evQgPDyc/v378+yzz5KYmMiSJUvYsWMH1atXz3OuzMxMunbtSps2bfjoo4/w9PQEYM6cOaSkpDBs2DD8/PxYv34948aN4+jRo8yZM8f+/G3bttG2bVtcXFx48sknCQ4OJioqil9//ZV3332XDh06EBQUxKxZs+jTp89lX5Pq1asTFhZ2C19ZETGFTUTEwU2bNs0G2DZs2HDFx3v37m1zdXW1RUVF2fcdP37cVqpUKVu7du3s+xo2bGi7++67r/p5zp07ZwNsH374Yf6FFxERESlmNm7caANsS5YssdlsNlt2dratcuXKtmeffdZ+zJtvvmkDbHPnzr3s+dnZ2TabzWabOnWqDbB98sknVz3mzz//tAG2P//8M9fj0dHRNsA2bdo0+77HHnvMBtheeeWVy14vJSXlsn3vv/++zWKx2A4dOmTf165dO1upUqVy7ft7HpvNZnv11Vdtbm5utvPnz9v3nTp1yubs7Gz7v//7v8s+j4g4Pk2rFJEiLSsri8WLF9O7d2+qVatm31+hQgUeeughVq9eTUJCAgClS5dm586d7N+//4qv5eHhgaurK8uXL+fcuXOFkl9ERESkqJk1axYBAQF07NgRAIvFQr9+/Zg9ezZZWVkA/PTTTzRs2PCy0VU5x+cc4+/vz7/+9a+rHnMzhg0bdtk+Dw8P+3ZycjJxcXHcfvvt2Gw2tmzZAsDp06dZuXIlTzzxBFWqVLlqngEDBpCWlsaPP/5o3/f999+TmZnJI488ctO5RcQ8Ko6JSJF2+vRpUlJSqFWr1mWP1alTh+zsbI4cOQLA22+/zfnz56lZsyb169fnxRdfZNu2bfbj3dzc+OCDD1i4cCEBAQG0a9eOMWPGEBsbW2jvR0RERMSRZWVlMXv2bDp27Eh0dDQHDhzgwIEDtGzZkpMnTxIeHg5AVFQU9erVu+ZrRUVFUatWLZyd86/bj7OzM5UrV75s/+HDhxk4cCBly5bF29ubcuXK0b59ewDi4+MBOHjwIMB1c9euXZvmzZvn6rM2a9YsWrVqRWhoaH69FREpRCqOiUiJ0a5dO6Kiopg6dSr16tVj8uTJNGnShMmTJ9uPee6559i3bx/vv/8+7u7uvPHGG9SpU8d+RVFERESkJFu2bBknTpxg9uzZ1KhRw3574IEHAPJ91cqrjSDLGaH2T25ublit1suOvfPOO1mwYAEvv/wyP//8M0uWLLE388/Ozs5zrgEDBrBixQqOHj1KVFQUa9eu1agxkSJMDflFpEgrV64cnp6e7N2797LH9uzZg9VqJSgoyL6vbNmyPP744zz++OMkJSXRrl07Ro0axeDBg+3HVK9enRdeeIEXXniB/fv306hRIz7++GO++eabQnlPIiIiIo5q1qxZlC9fnvHjx1/22Ny5c5k3bx4TJ06kevXq7Nix45qvVb16ddatW0dGRgYuLi5XPKZMmTKAsfLl3x06dOiGM2/fvp19+/YxY8YMBgwYYN//9xXLAXuLjuvlBujfvz8jRozgu+++IzU1FRcXF/r163fDmUTEsWjkmIgUaU5OTnTp0oVffvmFmJgY+/6TJ0/y7bff0qZNG3x8fAA4c+ZMrud6e3sTGhpKWloaACkpKVy4cCHXMdWrV6dUqVL2Y0RERERKqtTUVObOnUuPHj247777LrsNHz6cxMRE5s+fz7333svWrVuZN2/eZa9js9kAY5XwuLg4Pv/886seU7VqVZycnFi5cmWuxydMmHDDuZ2cnHK9Zs72p59+muu4cuXK0a5dO6ZOncrhw4evmCeHv78/3bt355tvvmHWrFl069YNf3//G84kIo5FI8dEpMiYOnUqixYtumz/qFGjWLJkCW3atOHpp5/G2dmZL7/8krS0NMaMGWM/rm7dunTo0IGmTZtStmxZNm7cyI8//sjw4cMB2LdvH506deKBBx6gbt26ODs7M2/ePE6ePEn//v0L7X2KiIiIOKL58+eTmJjIPffcc8XHW7VqRbly5Zg1axbffvstP/74I/fffz9PPPEETZs25ezZs8yfP5+JEyfSsGFDBgwYwMyZMxkxYgTr16+nbdu2JCcns3TpUp5++ml69eqFr68v999/P+PGjcNisVC9enV+++03Tp06dcO5a9euTfXq1Rk5ciTHjh3Dx8eHn3766YoLMH322We0adOGJk2a8OSTTxISEkJMTAwLFiwgMjIy17EDBgzgvvvuA2D06NE3/oUUEcdj5lKZIiI3Ytq0aTbgqrcjR47YNm/ebOvatavN29vb5unpaevYsaNtzZo1uV7nnXfesbVo0cJWunRpm4eHh6127dq2d99915aenm6z2Wy2uLg42zPPPGOrXbu2zcvLy+br62tr2bKl7YcffjDjbYuIiIg4lJ49e9rc3d1tycnJVz1m4MCBNhcXF1tcXJztzJkztuHDh9sqVapkc3V1tVWuXNn22GOP2eLi4uzHp6Sk2F577TVbSEiIzcXFxRYYGGi77777bFFRUfZjTp8+bbv33nttnp6etjJlytieeuop244dO2yAbdq0afbjHnvsMZuXl9cVc+3atcvWuXNnm7e3t83f3982ZMgQ29atWy97DZvNZtuxY4etT58+ttKlS9vc3d1ttWrVsr3xxhuXvWZaWpqtTJkyNl9fX1tqauoNfhVFxBFZbLZ/jA8VERERERERkWvKzMykYsWK9OzZkylTppgdR0RugXqOiYiIiIiIiOTRzz//zOnTp3M1+ReRokkjx0RERERERERu0Lp169i2bRujR4/G39+fzZs3mx1JRG6RRo6JiIiIiIiI3KAvvviCYcOGUb58eWbOnGl2HBHJBxo5JiIiIiIiIiIiJZZGjomIiIiIiIiISIml4piIiIiIiIiIiJRYzmYHyC/Z2dkcP36cUqVKYbFYzI4jIiIiRYDNZiMxMZGKFStiteqaoaPSeZ6IiIjkVV7O84pNcez48eMEBQWZHUNERESKoCNHjlC5cmWzY8hV6DxPREREbtaNnOcVm+JYqVKlAONN+/j4mJxGREREioKEhASCgoLs5xHimHSeJyIiInmVl/O8YlMcyxli7+Pjo5MmERERyRNN1XNsOs8TERGRm3Uj53lqriEiIiIiIiIiIiWWimMiIiIiIiIiIlJiqTgmIiIiIiIiIiIlVrHpOSYiIiIiIiIijslms5GZmUlWVpbZUaSYcHJywtnZOV96x6o4JiIiIiIiIiIFJj09nRMnTpCSkmJ2FClmPD09qVChAq6urrf0OiqOiYiIiIiIiEiByM7OJjo6GicnJypWrIirq6tWiZZbZrPZSE9P5/Tp00RHR1OjRg2s1pvvHKbimIiIiIiIiIgUiPT0dLKzswkKCsLT09PsOFKMeHh44OLiwqFDh0hPT8fd3f2mX0sN+UVERERERESkQN3KqB6Rq8mvnyv9dIqIiIiIiIiISIml4piIiIiIiIiIiJRYKo6JiIiIiIiIiBSg4OBgxo4da3YMuQo15BcRERERERER+YcOHTrQqFGjfClqbdiwAS8vr1sPJQVCI8dERERExG78+PEEBwfj7u5Oy5YtWb9+/VWPnT59OhaLJdftnytFDRw48LJjunXrVtBvQ0REpMDZbDYyMzNv6Nhy5coV69U609PTzY5wS1QcExEREREAvv/+e0aMGMH//d//sXnzZho2bEjXrl05derUVZ/j4+PDiRMn7LdDhw5ddky3bt1yHfPdd98V5NsQEREHZrPZSEnPNOVms9luOOfAgQNZsWIFn376qf3iTs5FoYULF9K0aVPc3NxYvXo1UVFR9OrVi4CAALy9vWnevDlLly7N9Xr/nFZpsViYPHkyffr0wdPTkxo1ajB//vwbypaVlcWgQYMICQnBw8ODWrVq8emnn1523NSpU7nttttwc3OjQoUKDB8+3P7Y+fPneeqppwgICMDd3Z169erx22+/ATBq1CgaNWqU67XGjh1LcHBwrq9P7969effdd6lYsSK1atUC4Ouvv6ZZs2aUKlWKwMBAHnroocvOI3bu3EmPHj3w8fGhVKlStG3blqioKFauXImLiwuxsbG5jn/uuedo27btDX1tbpamVYqIiIgIAJ988glDhgzh8ccfB2DixIksWLCAqVOn8sorr1zxORaLhcDAwGu+rpub23WP+bu0tDTS0tLsHyckJNzwc0VExLGlZmRR980/TPncu97uiqfrjZVBPv30U/bt20e9evV4++23AaOoA/DKK6/w0UcfUa1aNcqUKcORI0e46667ePfdd3Fzc2PmzJn07NmTvXv3UqVKlat+jrfeeosxY8bw4YcfMm7cOB5++GEOHTpE2bJlr5ktOzubypUrM2fOHPz8/FizZg1PPvkkFSpU4IEHHgDgiy++YMSIEfz3v/+le/fuxMfH89dff9mf3717dxITE/nmm2+oXr06u3btwsnJ6Ya+NjnCw8Px8fFhyZIl9n0ZGRmMHj2aWrVqcerUKUaMGMHAgQP5/fffATh27Bjt2rWjQ4cOLFu2DB8fH/766y8yMzNp164d1apV4+uvv+bFF1+0v96sWbMYM2ZMnrLllYpjIiIiIkJ6ejqbNm3i1Vdfte+zWq107tyZiIiIqz4vKSmJqlWrkp2dTZMmTXjvvfe47bbbch2zfPlyypcvT5kyZbjjjjt455138PPzu+prvv/++7z11lu3/qZERERukq+vL66urnh6etov8OzZsweAt99+mzvvvNN+bNmyZWnYsKH949GjRzNv3jzmz5+fa7TWPw0cOJAHH3wQgPfee4/PPvuM9evXX7f9gIuLS67fkyEhIURERPDDDz/Yi2PvvPMOL7zwAs8++6z9uObNmwOwdOlS1q9fz+7du6lZsyYA1apVu/4X5R+8vLyYPHkyrq6u9n1PPPGEfbtatWp89tlnNG/enKSkJLy9vRk/fjy+vr7Mnj0bFxcXAHsGgEGDBjFt2jR7cezXX3/lwoUL9vdVUFQcExERERHi4uLIysoiICAg1/6AgAD7HwP/VKtWLaZOnUqDBg2Ij4/no48+4vbbb2fnzp1UrlwZMKZU9u3bl5CQEKKiovjPf/5D9+7diYiIuOoV6ldffZURI0bYP05ISCAoKCif3qmIiJjJw8WJXW93Ne1z54dmzZrl+jgpKYlRo0axYMECTpw4QWZmJqmpqRw+fPiar9OgQQP7tpeXFz4+PtdsZfB348ePZ+rUqRw+fJjU1FTS09PtUyFPnTrF8ePH6dSp0xWfGxkZSeXKlXMVpW5G/fr1cxXGADZt2sSoUaPYunUr586dIzs7G4DDhw9Tt25dIiMjadu2rb0w9k8DBw7k9ddfZ+3atbRq1Yrp06fzwAMPFPhiBiqOiYiIiMhNCQsLIywszP7x7bffTp06dfjyyy8ZPXo0AP3797c/Xr9+fRo0aED16tVZvnz5VU/a3dzccHNzK9jwIiJiCovFcsNTGx3VPws1I0eOZMmSJXz00UeEhobi4eHBfffdd90m9f8sEFksFnsx6Vpmz57NyJEj+fjjjwkLC6NUqVJ8+OGHrFu3DgAPD49rPv96j1ut1sv6s2VkZFx23D+/DsnJyXTt2pWuXbsya9YsypUrx+HDh+natav9a3G9z12+fHl69uzJtGnTCAkJYeHChSxfvvyaz8kPRfsnUkRERETyhb+/P05OTpw8eTLX/pMnT95wvzAXFxcaN27MgQMHrnpMtWrV8Pf358CBA1ctjomIiDgCV1dXsrKyrnvcX3/9xcCBA+nTpw9gjCSLiYkpsFx//fUXt99+O08//bR9X1RUlH27VKlSBAcHEx4eTseOHS97foMGDTh69Cj79u274uixcuXKERsbi81mw2KxAMZos+vZs2cPZ86c4b///a99xPfGjRsv+9wzZswgIyPjqqPHBg8ezIMPPkjlypWpXr06rVu3vu7nvlVarVJEREQcWnrm9a+gyq1zdXWladOmhIeH2/dlZ2cTHh6ea3TYtWRlZbF9+3YqVKhw1WOOHj3KmTNnrnmMKW7gSr2IiJQswcHBrFu3jpiYGOLi4q46qqtGjRrMnTuXyMhItm7dykMPPXRDI8BuVo0aNdi4cSN//PEH+/bt44033mDDhg25jhk1ahQff/wxn332Gfv372fz5s2MGzcOgPbt29OuXTvuvfdelixZQnR0NAsXLmTRokUAdOjQgdOnTzNmzBiioqIYP348CxcuvG6uKlWq4Orqyrhx4zh48CDz58+3jyTPMXz4cBISEujfvz8bN25k//79fP311+zdu9d+TNeuXfHx8eGdd96xLxJU0FQcExEREYdzISOL+VuPM3Daeu75fHWell6XmzdixAgmTZrEjBkz2L17N8OGDSM5Odl+YjpgwIBcDfvffvttFi9ezMGDB9m8eTOPPPIIhw4dYvDgwYBx5fzFF19k7dq1xMTEEB4eTq9evQgNDaVrV3P6zVxmz+8wsS0sfNHsJCIi4mBGjhyJk5MTdevWtU8RvJJPPvmEMmXKcPvtt9OzZ0+6du1KkyZNCizXU089Rd++fenXrx8tW7bkzJkzuUaRATz22GOMHTuWCRMmcNttt9GjRw/2799vf/ynn36iefPmPPjgg9StW5eXXnrJPkquTp06TJgwgfHjx9OwYUPWr1/PyJEjr5urXLlyTJ8+nTlz5lC3bl3++9//8tFHH+U6xs/Pj2XLlpGUlET79u1p2rQpkyZNyjWKzGq1MnDgQLKyshgwYMCtfKlumMVWTM42ExIS8PX1JT4+Hh8fH7PjiIiISB5lZ9tYF32WeVuO8vv2WJLSMu2PLXm+HTUCSuX759T5w+U+//xzPvzwQ2JjY2nUqBGfffYZLVu2BIwrycHBwUyfPh2A559/nrlz5xIbG0uZMmVo2rQp77zzDo0bNwYgNTWV3r17s2XLFs6fP0/FihXp0qULo0ePvqzx/7UU6Pdp70L4rj+UrgLPboOL00dERCR/XLhwgejoaEJCQnB3dzc7jhQRgwYN4vTp08yfP/+ax13r5ysv5w8qjokAaZlZuDnnz8olIiKSNwdOJTFvy1F+3nKcY+dT7fsrl/Ggb+NK9G5ciWrlvAvkc+v8oWgo0O9TWhJ8EAzZGTB8E/iH5u/ri4iUcCqOSV7Ex8ezfft27rzzTubPn8+dd955zePzqzimhvxSYsSnZBB9JplDZ5KJjkvm0JmUi/fJnEvJ4K76gXzyQCPc82l5XxERubqzyen8uvU4czcfZevRePv+Um7O3N2gAn2bVKZZ1TJYrRrFIwXMzRuqhkH0SjiwVMUxEREx3dChQ/nmm2+u+NgjjzzCxIkTCzlR4enVqxfr169n6NCh1y2M5ScVx6RYOZ+SflnhK+ZMCjFnkjmfcvnSs3/3+/ZYziVvYPJjzfBy0z8NEZH8lpaZxbLdp/hp8zGW7z1FZrYxeN3JaqF9zXL0bVKJznUCdJFCCl9o50vFsVZDzU4jIiIl3Ntvv33VHl/FfaT78uXLTfm8qgBIkWKz2Tj/txFgMXFG4SvmTAoxccnEp167ABbg40ZVPy9C/Lyo6u9p3Pt5cSY5jWHfbCbi4BkenbKOaY+3wNfjysvKiojIjbPZbGw6dI65W47x29bjJFy41EesfiVf+jSuxD2NKuLv7WZiSinxQjvDkjchZjVkpIKLh9mJRESkBCtfvjzly5c3O0aJouKYOBybzca5lAyj6BV3qfCVMx3y739YXUmgjztV/TwJ8TcKXyH+nlT186Kqnyeerlf/kZ81uCUDpq5n8+HzPPjVWr4e1AI//bEmInJTDp1JZu7mY/wceYxDZ1Ls+yv4utO7cSX6Nq5UIA32RW5K+bpQqgIknoBDayC0k9mJREREpBCpOCamsNlsnE1Oz134OpNycTTY9QtgFXxzF8CC/TwJ9veiStlrF8CupWFQaWY/2YpHp6xj14kE+n21lm8GtSTQV00jRURuRHxKBr9tP868zcfYeOicfb+nqxPd61Wgb5NKtKrmh5P6iImjsViMgtiWb+BAuIpjIiIiJYyKY1JgbDYbZ5LTL474SrnU/ysumZgzySTeQAEs2M+LYH9Pgv1yRoEZBTAP14LpR1Ongg8/PBXGw5PXceBUEg98GcGswS0JKutZIJ9PRKSoS8/MZsW+08zdfJTw3adIz8oGwGqB1qH+3NukMl1uC7jpCxcihSa0s1Eciwo3O4mIiIgUMp2pyi2x2WzEJaVfVviKOZPMobgUEtOuXQCr6OtO8D+mP+YUwMxqyFytnDc/PBXGI1PWcehMCvdPjGDWkJZUL+dtSh4REUdjs9nYejSeeZuPMn/rcc79bcGT2oGl6NukEr0aVSLARyNvpQip1gEsVji9B84fgdJBZicSERGRQqLimFxXTgHsUg+wv/cBSyHpGgUwiwUq+noQfLHwFezneXE0mLkFsOsJKutpFMgmr2P/qST6fRnBzCdaUrdi8V4ZRETkWo6eS+GXyOP8tPkoB08n2/eXK+VG70YV6dO4sv6flKLLowxUbg5H1hmjx5oONDuRiIiIFBIVxwQwCmCnk9Iurf54sfAVcyZvBbBgPy978SvYz5MgBy6AXU+AjzvfPxXGo1PWsfN4Av2/imDGEy1oXKWM2dFERApN4oUMFm6PZe6Wo6w9eNa+393FStfbAunbpDKtq/vh7GQ1MaVIPgntbBTHDixVcUxERKQEUXGsBLHZbJxOTCP6b4UvoxBm9ANLTs+66nMtFqhU2iNXD7Cc7cplim4B7HrKerny7ZBWPDF9A5sOneORyeuY/Fhzwqr7mR1NRKTAZGZls+pAHHM3H2PxzljSMo0+YhYLtArxo2+TSnSrF0gpdxeTk4rks9BO8Oe7cHAFZGWAk37GRURKsg4dOtCoUSPGjh2bL683cOBAzp8/z88//5wvryf5R8WxYsZms3EqMe2y6Y8xF1eCTLlGAcxqgUplPC42v889CiyorAduzsWzAHY9vh4uzHyiBUNmbmRN1BkGTlvPxEeb0rFWebOjiYjkG5vNxs7jCczbcoxfIo8Tl5Rmf6x6OS/6NqlM78aVqFTaw8SUIgWsQmPw9IOUM3B0A1S93exEIiIiDiU9PR1XV1ezY+Q7zYEogmw2G7HxF1h78Ayz1x/mvwv3MPTrTXQbu5K6b/5By/fC6ffVWl7+aTtfLI9i4Y5Ydp9IICU9C6sFgsp60LaGP4+2qsobPeoy5bFmhL/Qnt2ju7HqpTv4elBL3uldn8Ftq9G5bgCh5b1LbGEsh5ebM1MHNqdznfKkZWbz5MyNLNx+wuxYIiK3LDb+Al+uiKLb2FX0GLeaKaujiUtKo6yXKwNvD2b+8NYsHdGeZzqGqjAmxZ/VCtXvMLYPLDU3i4hIcWWzQXqyOTeb7YZjDhw4kBUrVvDpp59isViwWCzExMSwY8cOunfvjre3NwEBATz66KPExcXZn/fjjz9Sv359PDw88PPzo3PnziQnJzNq1ChmzJjBL7/8Yn+95cuXXzfHyy+/TM2aNfH09KRatWq88cYbZGRk5Drm119/pXnz5ri7u+Pv70+fPn3sj6WlpfHyyy8TFBSEm5sboaGhTJkyBYDp06dTunTpXK/1888/Y7FY7B+PGjWKRo0aMXnyZEJCQnB3NxZcWrRoEW3atKF06dL4+fnRo0cPoqKicr3W0aNHefDBBylbtixeXl40a9aMdevWERMTg9VqZePGjbmOHzt2LFWrViU7O/u6X5f8ppFjDio728bJxAuXeoBdXP0xZ/tCxtV/WKwWqFzG09736+9TISuX8cTVWTXRm+Hu4sQXjzTl+e8j+W3bCZ75djMf3d+Qvk0qmx1NRCRPktMy+WNnLPO2HGP1gTj7eaKrs5U76wTQt0kl2tUsh4v6iElJVL0TbJ9jFMc6vWl2GhGR4icjBd6raM7n/s9xcPW6oUM//fRT9u3bR7169Xj77bcBcHFxoUWLFgwePJj//e9/pKam8vLLL/PAAw+wbNkyTpw4wYMPPsiYMWPo06cPiYmJrFq1CpvNxsiRI9m9ezcJCQlMmzYNgLJly143R6lSpZg+fToVK1Zk+/btDBkyhFKlSvHSSy8BsGDBAvr06cNrr73GzJkzSU9P5/fff7c/f8CAAURERPDZZ5/RsGFDoqOjcxXzbsSBAwf46aefmDt3Lk5OxsCZ5ORkRowYQYMGDUhKSuLNN9+kT58+REZGYrVaSUpKon379lSqVIn58+cTGBjI5s2byc7OJjg4mM6dOzNt2jSaNWtm/zzTpk1j4MCBWK2Ffw6q4piJcgpg9h5gcX/rAXb22gUwJ6uFyhenQAb75RTCjOmQKoAVHBcnK5/2b4ynqxM/bDzKiB+2kpKexSOtqpodTUTkmrKybUREnWHu5qMs2hmba5p98+Ay9G1SmbvqV8DXQz2WpITLGTl2YisknQJvtVEQESmJfH19cXV1xdPTk8DAQADeeecdGjduzHvvvWc/burUqQQFBbFv3z6SkpLIzMykb9++VK1q/I1Yv359+7EeHh6kpaXZX+9GvP766/bt4OBgRo4cyezZs+3FsXfffZf+/fvz1ltv2Y9r2LAhAPv27eOHH35gyZIldO7cGYBq1arl9UtBeno6M2fOpFy5cvZ99957b65jpk6dSrly5di1axf16tXj22+/5fTp02zYsMFeBAwNDbUfP3jwYIYOHconn3yCm5sbmzdvZvv27fzyyy95zpcfVBwrYNnZNmITLtj7fuWsBJmzCmROk+MrcbJaCCrjQVU/L0L8L/YBu1gEq1zGQ1f0TeJktfDfvg3wdHVm+poYXv95BynpmTzZrrrZ0URELrM3NpG5W47yy5bjxCZcsO8P9vOkT+PK9GlciSp+niYmFHEwpQIgsAHEboOoZdCwv9mJRESKFxdPYwSXWZ/7FmzdupU///wTb2/vyx6LioqiS5cudOrUifr169O1a1e6dOnCfffdR5kyZW76c37//fd89tlnREVF2YtvPj4+9scjIyMZMmTIFZ8bGRmJk5MT7du3v+nPD1C1atVchTGA/fv38+abb7Ju3Tri4uLsUyEPHz5MvXr1iIyMpHHjxlcdHde7d2+eeeYZ5s2bR//+/Zk+fTodO3YkODj4lrLeLBXH8kF2to0T9gKYUfQyRoNdvwDmbLUQVNbzbw3wLxXAKqkA5rCsVgv/17MuXm5OjP8zivd+30NyWhbPda6Ra362iIgZTiemMX/rceZuPsrO4wn2/b4eLvRoUIG+TSrTpEpp/X8lcjWhnY3i2IGlKo6JiOQ3i+WGpzY6mqSkJHr27MkHH3xw2WMVKlTAycmJJUuWsGbNGhYvXsy4ceN47bXXWLduHSEhIXn+fBERETz88MO89dZbdO3aFV9fX2bPns3HH39sP8bD4+o9Ya/1GIDVasX2jz5s/+xnBuDldfn3q2fPnlStWpVJkyZRsWJFsrOzqVevHunp6Tf0uV1dXRkwYADTpk2jb9++fPvtt3z66afXfE5BUnHsBmVn2zgen5qr8BUdZ6wAeehsCunXKYBVuVgA+/sosBB/LyqWVgGsqLJYLLzYtTaers58+MdePg3fT3JaJq/dXUd/cIpIobuQkcXiXSeZt/koK/fHkZVtnOi4OFnoWKs8fZtUomPt8iV+gRWRGxLaGVZ/AgfCITsLrPp3IyJSErm6upKVdakVRZMmTfjpp58IDg7G2fnK5RSLxULr1q1p3bo1b775JlWrVmXevHmMGDHiste7njVr1lC1alVee+01+75Dhw7lOqZBgwaEh4fz+OOPX/b8+vXrk52dzYoVK+zTKv+uXLlyJCYmkpycbC+ARUZGXjfXmTNn2Lt3L5MmTaJt27YArF69+rJckydP5uzZs1cdPTZ48GDq1avHhAkT7NNRzaLi2A2YvOogY/7Ye80CmIuThaCLTfBzCl9V/bwI8fOiYml3nFUAK7ae6RiKl6sTo37dxeTV0SSnZ/FO73o4WVUgE5GClZ1tY33MWeZtPsbv20+QmJZpf6xRUGnubVKJHg0qUsar+C23LVKgglqAaylIPQsnIqFSU7MTiYiICYKDg+2rK3p7e/PMM88wadIkHnzwQV566SXKli3LgQMHmD17NpMnT2bjxo2Eh4fTpUsXypcvz7p16zh9+jR16tSxv94ff/zB3r178fPzw9fXFxeXq/d7rVGjBocPH2b27Nk0b96cBQsWMG/evFzH/N///R+dOnWievXq9O/fn8zMTH7//XdefvllgoODeeyxx3jiiSfsDfkPHTrEqVOneOCBB2jZsiWenp785z//4d///jfr1q1j+vTp1/26lClTBj8/P7766isqVKjA4cOHeeWVV3Id8+CDD/Lee+/Ru3dv3n//fSpUqMCWLVuoWLEiYWFhANSpU4dWrVrx8ssv88QTT1x3tFlBUnHsBvh4uJCemW0UwMp6EuJ3sfDlf2kkWAVfFcBKsoGtQ/B0deaVudv4bv1hUtMz+ej+hvqZEJECEXU6iXmbjzFvyzGOnU+1769U2oO+TSrRp3ElqpW7vBeGiNwgJxeo1h72/GaMHlNxTESkRBo5ciSPPfYYdevWJTU1lejoaP766y9efvllunTpQlpaGlWrVqVbt25YrVZ8fHxYuXIlY8eOJSEhgapVq/Lxxx/TvXt3AIYMGcLy5ctp1qwZSUlJ/Pnnn3To0OGqn/+ee+7h+eefZ/jw4aSlpXH33XfzxhtvMGrUKPsxHTp0YM6cOYwePZr//ve/+Pj40K5dO/vjX3zxBf/5z394+umnOXPmDFWqVOE///kPYKyW+c033/Diiy8yadIkOnXqxKhRo3jyySev+XWxWq3Mnj2bf//739SrV49atWrx2Wef5Xovrq6uLF68mBdeeIG77rqLzMxM6taty/jx43O91qBBg1izZg1PPPHEDX5XCobF9s8JpkVUQkICvr6+xMfH52pOlx/iUzJIuJBBxdIeGg0k1/Tr1uM8/30kmdk2utQNYNxDjTWFSUTyxdnkdH7bdpyfNh9j65Hz9v2l3Jy5q34F+japRPPgslj1eypPCvL8QfKPKd+njdPgt+cgqCUMWlw4n1NEpBi6cOEC0dHRhISE4O7ubnYccTCjR49mzpw5bNu27aaef62fr7ycP2jk2A3w9XTB11NL28v19WxYEQ8XJ57+djOLd51k8IyNfPVoMzxcVSATkbxLy8xi2e5TzN1yjD/3nCLzYh8xJ6uF9jXL0adxJe6sG4C7i/6PEcl3oZ2M+6MbIPUceNz8SmMiIiKSW1JSEjExMXz++ee88847ZsdBc75E8lnnugFMG9gcDxcnVu2P47Gp60m8cPmKHyIiV2Kz2dh06CyvzdtOi3fDGTZrM0t2nSQz20a9Sj680aMua1/txNSBzenZsKIKYyIFpXQV8K8Ftmw4uMLsNCIiUgy99957eHt7X/GWMxWzuBo+fDhNmzalQ4cOpk+pBI0cEykQrUP9+WZwCwZO28D6mLM8MnkdM55oQWlPNcUWkSs7fCaFuVuOMm/LMQ6dSbHvD/Rxp3fjSvRtUomaAaVMTChSAoV2hri9cGAp3Nbb7DQiIlLMDB06lAceeOCKj5nZnL4wTJ8+/Yaa/xcWFcdECkjTqmX5bkgrHp2yjq1H4+n/1Vq+HtSScqXczI4mIg4iPiWDBdtPMHfzUTYeOmff7+nqRLd6gdzbpDKtqvmp36WIWUI7wdrxRlN+mw0s+rcoIiL5p2zZspQtW9bsGIKKYyIFql4lX354KoyHJ69jT2wiD3wZwazBLalYunhfBRCRq8vIymbF3tPM3XKUpbtPkZ6ZDYDVYow67dukEl1vC8TTVb+iRUxXtTU4e0DicTi1GwLqmp1IRKTIKiZrAYqDya+fK515ixSwGgGlmDM0jIcmrSM6Lpn7JxoFsmB/L7OjiUghsdlsbDsaz7wtx5i/9Thnk9Ptj9UKKEXfJpXo1agSgb5awUnEobi4Q3AbOLDEmFqp4piISJ65uBiL26WkpBT7qYJS+FJSjHYkOT9nN0vFMZFCUNXPizlDw3hk8joOxiXbR5DVUP8gkWLt2PlUft5yjLmbjxJ1Otm+39/bjd6NKtKnSSXqVvDBoqlaIo4rtPOl4ljrf5udRkSkyHFycqJ06dKcOnUKAE9PT537yC2z2WykpKRw6tQpSpcujZPTrS1SpeKYSCGpWNqD758K49Epl6ZYfj2oJfUq+ZodTUTyUeKFDBbuiGXe5mNEHDxj3+/uYqVL3UD6NqlEm1B/nJ20YLRIkRDaybg/HAFpSeDmbW4eEZEiKDAwEMBeIBPJL6VLl7b/fN0KFcdEClG5Um7MfrIVj01dz9aj8Tz41VqmPd6cZsFqwihSlGVmZbP6QBxzNx9j8a5YLmRk2x9rVa0sfZtUpnu9QEq539pwbxExgV8olK4C5w9DzGqo1c3sRCIiRY7FYqFChQqUL1+ejIwMs+NIMeHi4nLLI8ZyqDgmUshKe7ryzeCWDJqxkfXRZ3l0ynomDWhGmxr+ZkcTkTzadTyBuZuP8nPkceKS0uz7q5fzom+TyvRqVJHKZTxNTCgit8xiMaZWbpxqTK1UcUxE5KY5OTnlWzFDJD+pOCZiglLuLsx4vAVPfbOJlftO88T0DUx4uAmd6waYHU1EruNkwgV+3nKMeVuOsSc20b6/rJcr9zSsSJ/GlWhQ2Ve9NESKk78Xx0RERKTYUXFMxCQerk5MGtCUf3+3hT92nmToN5v4X79G9GxY0exoIvIPKemZ/LEzlrmbj/HXgTiyL64Y7epkpXPd8vRpXJn2Ncvh6qw+YiLFUkg7sDrDuWg4EwV+1c1OJCIiIvlIxTERE7k5OzH+oSaMnLOVnyOP8+/ZW0hNz+KB5kFmRxMp8bKybaw9eIafNh9l0Y5YUtKz7I81q1qGvk0qc3f9Cvh6qo+YSLHnVgqqhEHMKjgQruKYiIhIMaPimIjJnJ2sfPJAIzzdnPl23WFe+mkbyemZPN46xOxoIiXSvpOJzN18jJ+3HCM24YJ9f1U/T/o0rkSfxpWo6udlYkIRMUVop4vFsaXQ8kmz04iIiEg+UnFMxAFYrRbe7V0PL1cnJq2K5q1fd5GSnsUzHUPNjiZSIsQlpTE/8jhztxxlx7EE+34fd2d6NqxI3yaVaFKljPqIiZRkoZ1h6SijQJaZBs5uZicSERGRfKLimIiDsFgs/OeuOni5OTN26X4+/GMvyWmZvNi1lv4gFykAFzKyWLLrJPO2HGPFvtNkXWwk5my10LF2efo2rsQddcrj5qwVlUQECKgH3gGQdBIOR0C1DmYnEhERkXyi4piIA7FYLDzXuSZers68+/tuJiyPIiU9izd71MVqVYFM5FZlZ9vYEHOWuZuP8fv2EySmZdofaxhUmnubVKJHg4qU9XI1MaWIOCSLxRg9FjnLmFqp4piIiEixoeKYiAMa0q4aHq5OvPHLDqaviSE5LZP/3tsAJxXIRG7KwdNJzNtyjLmbj3HsfKp9f6XSHkYfsSaVqF7O28SEIlIkhHa6WBwLhy7vmJ1GRERE8omKYyIO6pFWVfF0dWLknK3M2XSUlIws/vdAI1ydrWZHEykSziWn89u24/y0+RiRR87b93u7OXNX/UD6NqlMi+CyGpUpIjeuWkewWOHULog/Br6VzE4kIiIi+SDPf2WvXLmSnj17UrFiRSwWCz///PM1jz9x4gQPPfQQNWvWxGq18txzz13xuDlz5lC7dm3c3d2pX78+v//+e16jiRQ7fZtUZsLDTXBxsrBg2wmGfbOJCxlZZscScVhpmVks2hHLkzM30uK9pbzxy04ij5zHyWqhY61yfPZgYza81pkx9zWkVTU/FcZEJG88y0KlpsZ2VLi5WURERCTf5Lk4lpycTMOGDRk/fvwNHZ+Wlka5cuV4/fXXadiw4RWPWbNmDQ8++CCDBg1iy5Yt9O7dm969e7Njx468xhMpdrrVq8CkAc1wc7YSvucUT0zfQPLf+iSJlHQ2m41Nh87x+s/bafFuOEO/2cTiXSfJyLJxW0Uf3uhRl7WvdmLa4y24p2FFPFzVYF9EbkFoZ+P+wFJzc4iIiEi+sdhsNttNP9liYd68efTu3fuGju/QoQONGjVi7Nixufb369eP5ORkfvvtN/u+Vq1a0ahRIyZOnHhDr52QkICvry/x8fH4+Pjc6FsQKTLWHjzDoOkbSE7PokmV0kx7vAW+Hi5mxxIx1er9cbz92072nUyy7wvwcaN340r0bVyZWoGlTEwnRYHOH4oGh/o+Hd0IkzuBmy+8dBCc1KVERETEEeXl/MEhfptHREQwYsSIXPu6du16zSmbaWlppKWl2T9OSEgoqHgiDqFVNT++GdySx6auZ/Ph8zw0aS0zn2iBn7eb2dFECt3JhAu8s2A3v249DoCnqxPdbjP6iIVV99PiFSJScCo2Bo8ykHoOjm2EKq3MTiQiIiK3yCE6e8fGxhIQEJBrX0BAALGxsVd9zvvvv4+vr6/9FhQUVNAxRUzXuEoZZj8Zhp+XKzuPJ9Dvq7WcTLhgdiyRQpOZlc2U1dF0+ngFv249jtUCA28PJuLVTnzSrxFtavirMCYiBcvqZDTmB02tFBERKSYcojh2M1599VXi4+PttyNHjpgdSaRQ1K3oww9Dw6jg686BU0ncPzGCI2dTzI4lUuA2HTpLj3GrGf3bLpLSMmkUVJr5w9sw6p7bNMVYRAqX+o6JiIgUKw4xrTIwMJCTJ0/m2nfy5EkCAwOv+hw3Nzfc3DSdTEqm6uW8+eGpMB6evI7DZ1N44MsIvhnckurlvM2OJpLvzian88HCPXy/0bgI4uvhwivda9OvWZBWmxQRc4R2Mu6Pb4Gk0+Bdztw8IiIickscYuRYWFgY4eG5l8NesmQJYWFhJiUScXxBZT2ZMzSM0PLenIi/QL8vI9h9Qr33pPjIzrYxe/1h7vh4ub0w9kCzyix7oT0PtqiiwpiImKdUIATUN7YP/mluFhEREblleS6OJSUlERkZSWRkJADR0dFERkZy+PBhwJjuOGDAgFzPyTk+KSmJ06dPExkZya5du+yPP/vssyxatIiPP/6YPXv2MGrUKDZu3Mjw4cNv4a2JFH8BPu58/2QrbqvoQ1xSOv2/WkvkkfNmxxK5ZTuOxXPvxDW8Mnc751MyqB1Yip+GhTHmvoZahEJEHEPO6DFNrRQRESnyLDabzZaXJyxfvpyOHTtetv+xxx5j+vTpDBw4kJiYGJYvX37pk1guv7pftWpVYmJi7B/PmTOH119/nZiYGGrUqMGYMWO46667bjiXQy3xLVLI4lMzeHyasYqll6sTUwc2p2U1P7NjieRZwoUMPlm8j5kRMWTbwMvViRFdavFYWFWcnRxisLMUMzp/KBoc8vsUvQpm9ABPfxi5H6z6P0pERMSR5OX8Ic/FMUflkCdNIoUoOS2TITM3sibqDO4uViY+0pQOtcqbHUvkhthsNuZvPc47C3ZzOjENgB4NKvD63XUJ9HU3OZ0UZzp/KBoc8vuUmQ5jQiA9CZ5cARUbmZ1IRERE/iYv5w+6xCVSTHi5OTN1YHPuqF2eCxnZDJm5kUU7TpgdS+S6DpxK5KFJ63h2diSnE9Oo5u/FN4Na8vlDTVQYExHH5ewKIe2NbU2tFBERKdJUHBMpRtxdnJj4SFPurl+BjCwbz3y7hXlbjpodS+SKUtOzGLNoD90/XUXEwTO4OVsZ2aUmC59rS5sa/mbHExG5PnvfsfBrHyciIiIOzdnsACKSv1ydrXz2YGM8XJ34cdNRRvywlZT0LB5uWdXsaCJ2S3adZNT8nRw7nwrAHbXL89Y9txFU1tPkZCIieZBTHDuyDi7Eg7uvuXlERETkpqg4JlIMOVktjLm3AV6uTsyIOMRr83aQkpbFkHbVzI4mJdyRsym89etOlu4+BUCl0h78X8+63Fk34IqLt4iIOLQyweBXA87sh4MroO49ZicSERGRm6DimEgxZbVaGHXPbXi6OfPF8ije/X03yemZPNuphooQUujSMrOYtPIg45YdIC0zGxcnC0PaVmP4HaF4uupXkYgUYaGdjeLYgaUqjomIiBRR+otEpBizWCy83K023m7OfPjHXsYu3U9yWib/uauOCmRSaFbvj+PNX3ZwMC4ZgLBqfozufRuh5UuZnExEJB+EdoZ1Xxh9x2w20O9XERGRIkfFMZES4JmOoXi6OvHWr7uYtCqalPQsRveqh9WqE3gpOCcTLjD6t138ts1YNdXf2403etThnoYVVZwVkeIjuDU4u0PCUTi9F8rXNjuRiIiI5JGKYyIlxOOtQ/BydebluduYte6wsVLgfQ1wdtKitZK/MrOymRFxiP8t2UdSWiZWCwwIC2ZEl5r4uLuYHU9EJH+5eEDV1hAVbkytVHFMRESkyFFxTKQEeaB5EO6uToz4PpK5W46Rkp7Fpw82ws3ZyexoUkxsOnSW1+btYE9sIgCNgkrzTu961KukFdxEpBgL7XSpOHb7cLPTiIiISB5pyIhICXNPw4p88UhTXJ2sLNoZy5MzN5GanmV2LCnizian8/KP27j3iwj2xCbi6+HC+33rM3fY7SqMiUjxF9rZuD/0F6Qnm5tFRERE8kzFMZES6M66AUwd2BwPFydW7DvNY9PWk3ghw+xYUgRlZ9uYvf4wd3y8nO83HgHggWaVWfZCex5sUUV97USkZPCvCb5BkJUOMX+ZnUZERETySMUxkRKqTQ1/vh7UglJuzqyPPssjk9dxPiXd7FhShOw4Fs+9E9fwytztnE/JoHZgKX4aFsaY+xri5+1mdjwRkcJjsRhTK8GYWikiIiJFiopjIiVYs+CyfDukFWU8Xdh6NJ7+X63ldGKa2bHEwSVcyGDU/J3c8/lqthw+j5erE2/0qMtv/2pD06plzY4nImKOnKmVKo6JiIgUOSqOiZRw9Sv78v1TYZQr5cae2ET6fRnB8fOpZscSB2Sz2fgl8hidPl7B9DUxZNugR4MKhL/QgUFtQrTyqYiUbCHtwOoMZ6PgbLTZaURERCQP9JeMiFAzoBRzngqjUmkPDsYlc//ECA6dUUNhueTAqUQemrSOZ2dHcjoxjWr+XnwzqCWfP9SEQF93s+OJiJjP3ReCWhrbUeHmZhEREZE8UXFMRAAI9vfih6FhhPh7cex8KvdPjGD/yUSzY4nJUtOzGLNoD90/XUXEwTO4OVsZ2aUmC59rS5sa/mbHExFxLPa+YyqOiYiIFCUqjomIXaXSHnz/VCtqBZTiVGIa/b5ay45j8WbHEpMs2XWSzp+sYMLyKDKybNxRuzxLR7Rn+B01cHN2MjueiIjjyek7dnAFZGqRGxERkaJCxTERyaV8KXdmP9mKBpV9OZuczoOT1rLp0FmzY0khOnI2hcEzNjBk5kaOnU+lUmkPvnq0KVMea0ZQWU+z44mIOK6A+uBVHjKS4chas9OIiIjIDVJxTEQuU8bLlVmDW9IiuCyJFzJ5dMp61hyIMzuWFLC0zCw+X7afzp+sYOnuU7g4WXi6Q3WWjGhHl9sCsVgsZkcUEXFsVuvfplZq1UoREZGiQsUxEbmiUu4uzHiiBW1r+JOSnsXA6RsI333S7FhSQFbvj6P72FV8tHgfaZnZhFXzY+GzbXmpW208XZ3NjiciUnTkTK1U3zEREZEiQ8UxEbkqD1cnJj/WjC51A0jPzOaprzfx69bjZseSfHQy4QLDv93MI1PWcTAuGX9vNz7t34hvh7QktHwps+OJiBQ91ToCFji5AxJOmJ1GREREboCKYyJyTW7OTox/uAm9GlUkM9vGs7O38MPGI2bHkluUmZXNlNXRdPp4Bb9tO4HVAgNvD2bZyPb0alRJUyhFRG6Wlx9UamJsR2n0mIiISFGguTIicl0uTlY+eaARnq5OfLf+CC/9uI2UtEwGtg4xO5rchE2HzvLavB3siU0EoFFQad7pXY96lXxNTiYiUkyEdoZjm4y+Y40fMTuNiIiIXIeKYyJyQ5ysFt7rUx9PV2emrI5m1K+7SE7P4pmOoWZHkxt0NjmdDxbu4fuLI/98PVx4pXtt+jULwmrVSDERkXxTvROs+ACi/oSsTHDSKbeIiIgj029qEblhFouF1++ug5ebM5+F7+fDP/aSkp7JyC61NA3PgWVn2/hh4xH+u2gP51MyAHigWWVe7lYbP283k9OJiBRDlZqCuy9cOA/HN0NQC7MTiYiIyDWoOCYieWKxWBhxZ028XJ14f+Eexv8ZRXJaFm/2qKvRRw5ox7F43vhlB1sOnwegdmAp3u1Tj6ZVy5obTESkOHNyNhrz7/rZmFqp4piIiIhDU3FMRG7KU+2r4+nqxBu/7GT6mhhS0jN5v28DnFQgcwgJFzL4ZPE+ZkbEkG0DL1cnRnSpxWNhVXF20losIiIFLrTzpeJYx/+YnUZERESuQcUxEblpj4YF4+nqzIs/buWHjUdJSc/if/0a4aLii2lsNhvztx7nnQW7OZ2YBkCPBhV4/e66BPq6m5xORKQECe1k3B/bDMlnjFUsRURExCGpOCYit+TeppXxdHXi37O38Nu2E1zIyOLzh5rg7uJkdrQS58CpRN74eScRB88AUM3fi7d71aNNDX+Tk4mIlEA+FaH8bXBqJxz8E+rfZ3YiERERuQoN7xCRW9a9fgW+GtAMN2crS3efYtCMDaSkZ5odq8RITc9izKI9dP90FREHz+DmbGVkl5osfK6tCmMiImbKGT12INzcHCIiInJNKo6JSL7oWKs80x9vgZerE38dOMOjU9YTn5phdqxib8muk3T+ZAUTlkeRkWXjjtrlWTqiPcPvqIGbs0bviYiYKrSzcX9gKWRnm5tFRERErkrFMRHJN2HV/fhmcEt83J3ZdOgcD30VwdmkC5CdBVmZkJUBmemQmQYZFyAjFdJTID0Z0hLhQgJciIfU85B6DlLOGrfkM5AcB0mnIekUJJ6ExFhIOAEJxyH+GMQfhfNH4PxhOHcIzkbD2YNwJsq4zypeI9mOnE1h8IwNDJm5kWPnU6lU2oOvHm3KlMeaEVTW0+x4IlKEjR8/nuDgYNzd3WnZsiXr16+/6rHTp0/HYrHkurm75+5vaLPZePPNN6lQoQIeHh507tyZ/fv3F/TbcAxVWoGLFySfgpM7zE4jIiIiV6GeYyLpKRD+NiSdBGxgs13hnqvsv3hvy775517xPvtv2+Txuf+85zr5rvPcPL7nxjYb27CBO3AO+Ci/v2E3qUwwdB8DNbuaneSWpGVmMWnlQcYtO0BaZjYuThaGtK3G8DtC8XTVf+kicmu+//57RowYwcSJE2nZsiVjx46la9eu7N27l/Lly1/xOT4+Puzdu9f+scWSe9XiMWPG8NlnnzFjxgxCQkJ444036Nq1K7t27bqskFbsOLtBSDvYt9AYPVahgdmJRERE5Ar0l5TI1m9h3Rdmp5A8s4DFChbLxe1r3Gelw7kY+PYBqHUXdHvfKJYVMav3x/HmLzs4GJcMQFg1P0b3vo3Q8qVMTiYixcUnn3zCkCFDePzxxwGYOHEiCxYsYOrUqbzyyitXfI7FYiEwMPCKj9lsNsaOHcvrr79Or169AJg5cyYBAQH8/PPP9O/fv2DeiCMJ7XSxOBYObUeYnUZERESuQMUxkX1/GPd1e0OVsH8UV7hCseUGCzLXfI2/3Vus13kN8vC5rpYvr6/xz9e6znu4yv2x8xcYOmsTR89doLyPB18+2oxgf++beq1LX6/cIxJuSFoSrPgA1k6Avb9D1DJo+wLc/m9wcfxRCycTLjD6t138tu0EAP7ebrzRow73NKx42QgNEZGblZ6ezqZNm3j11Vft+6xWK507dyYiIuKqz0tKSqJq1apkZ2fTpEkT3nvvPW677TYAoqOjiY2NpXPnzvbjfX19admyJREREVctjqWlpZGWlmb/OCEh4Vbfnnly+o4dWWu0D3D3MTePiIiIXEbFMSnZ0pPh4Apju/3LEFDX3DzFTCVvmDS0PA9PXsve08ncN2MP3wxuQe3AQv7DwM0buoyGRg/D7yMhZhX8+S5Efgt3fQg17izcPDcoMyubGRGH+N+SfSSlZWK1wICwYEZ0qYmPu4vZ8USkmImLiyMrK4uAgIBc+wMCAtizZ88Vn1OrVi2mTp1KgwYNiI+P56OPPuL2229n586dVK5cmdjYWPtr/PM1cx67kvfff5+33nrrFt+RgygbAmWrw9koiF4JdXqYnUhERET+QQ35pWSLXglZaeBbBcrXMTtNsRTo6873T4VRt4IPcUlp9PtyLVuPnDcnTPna8NivcO8U8A6Ec9Ew6z6Y/bDRyN+BbDp0lh7jVjP6t10kpWXSKKg084e3YdQ9t6kwJiIOIywsjAEDBtCoUSPat2/P3LlzKVeuHF9++eUtve6rr75KfHy8/XbkyJF8SmySv69aKSIiIg5HxTEp2fYtMu5rdr256XpyQ/y93fhuSCsaVylNfGoGD09ex7qDZ8wJY7FA/fvgXxshbDhYnWHPb/B5C1j5obGSponOJqfz8o/buPeLCPbEJuLr4cL7feszd9jt1Kvka2o2ESne/P39cXJy4uTJk7n2nzx58qo9xf7JxcWFxo0bc+DAAQD78/L6mm5ubvj4+OS6FWn24lj4pcV6RERExGGoOCYll812qd9YzW7mZikBfD1d+HpQS8Kq+ZGUlslj09azYt9p8wK5lYKu78LQ1RDcFjJTYdk7MCHMlCv72dk2vlt/mDs+Xs73G40REg80q8yyF9rzYIsqWK0q3opIwXJ1daVp06aEh4fb92VnZxMeHk5YWNgNvUZWVhbbt2+nQoUKAISEhBAYGJjrNRMSEli3bt0Nv2axENwanFwh/jDE7Tc7jYiIiPyDimNScsVug8QT4OIJwW3MTlMieLs5M+3x5txRuzwXMrIZMmMjf+y8es+ZQlG+Tu6plmej4Jt74ftH4XzhTOPZcSyevl+s4dW52zmfkkHtwFL8NCyMMfc1xM/brVAyiIgAjBgxgkmTJjFjxgx2797NsGHDSE5Otq9eOWDAgFwN+99++20WL17MwYMH2bx5M4888giHDh1i8ODBgLGS5XPPPcc777zD/Pnz2b59OwMGDKBixYr07t3bjLdoDlcvqHq7sa2plSIiN+7ENji0RqNupcCpIb+UXDmjxqp1LBIrFhYX7i5OTHykKc9/H8mC7Sd4etZmPr6/Ib0bVzIvVM5UyxpdLq5q+QXsnm/8AdNupDH90jn/i1QJFzL4ZPE+ZkbEkG0DL1cnRnSpxWNhVXF20rULESl8/fr14/Tp07z55pvExsbSqFEjFi1aZG+of/jwYazWS/8/nTt3jiFDhhAbG0uZMmVo2rQpa9asoW7dSwvcvPTSSyQnJ/Pkk09y/vx52rRpw6JFi3B3L2G/e0M7w8Hlxu+WsKfNTiMi4viS42BKF2OGR4VGxmrztXuAVefJkv8sNlvxKMEmJCTg6+tLfHx80e9LIYVj0h1wbBP0/AyaPmZ2mhInMyubl3/azk+bj2KxwLu96/NQyypmxzKc3GWsannoL+Njv1BjVcvqd+TLy9tsNuZvPc47C3ZzOtHocdajQQVev7sugb4l7I9FEZPp/KFoKBbfp1O7YUIrcHaHl2PAxcPsRCIijm3TdPj12dz7ytU2imS39QUnjfWRa8vL+YNKrlIyJZ0yCmNgjBaSQufsZOXD+xowIKwqNhv8Z952Jq86aHYsQ0BdGLgA+nwFXuXhzAH4ug/8MADij97SSx84lchDk9bx7OxITiemUc3fi28GteTzh5qoMCYiUpyVqw0+lSDzAsT8ZXYaERHHt2u+cX/7v6DtSHDzgdN7YO4Q+LypUTwzeTEtKT5UHJOSaf9i475CI/CpYGqUksxqtfDWPbcxtH11AN5ZsJtPl+7HIQa0WizQsJ+xqmXLYWCxwq5f4PPmsPp/kJmep5dLTc9izKI9dP90FREHz+DmbGVkl5osfK4tbWr4F9CbEBERh2GxQGgnY1t9x0REri31HESvMLabDIROb8DzO+CON8DTD87FGKPKPm1ktERJTzExrBQHKo5JyaRVKh2GxWLh5W61GNmlJgD/W7qP9xfucYwCGYC7L3T/Lzy1CqqEQUYKLB0FE1sbvWNuwJJdJ+n8yQomLI8iI8vGHbXLs3REe4bfUQM3Z6cCjS8iIg4ktLNxHxV+7eNEREq6vQshOxPK1wX/UGOfu6/RD/i57dD1fShVARKPw6JXYGx9WPUxXEgwN7cUWSqOScmTmQ5Ry4ztml3NzSKAUSAbfkcN3uhhNHD+auVBXv95B9nZDlIgAwisB48vhD5fglc5iNsHM3vBnIGQcPyKTzlyNoVB0zcwZOZGjp1PpVJpD756tClTHmtGUFnPws0vIiLmC2kPFifjd8i5Q2anERFxXDlTKuvcc/ljrl7GwibPboUeY6F0VUiJg/C34X/1YNk7kHymUONK0afimJQ8h/6C9CTwDjCmVYrDGNQmhP/2rY/FArPWHWbknK1kZmWbHesSiwUa9ofhG6HlUGOq5c55MK4Z/PWpfaplWmYWny/bT+dPVhC+5xQuThaGdajOkhHt6HJbIBaLxeQ3IiIipvAoDUEtjG2NHhMRubK0xEuDGepeoTiWw9kNmj0O/9psXMD2rwVp8bDyQ2Mk2R+vQWJs4WSWIk/FMSl5cqZU1uiiZYAdUP8WVRjbrxFOVgtztxzjX99tIT3TgQpkYPxx0/0DeHIFBLWEjGRY8iZMbMP2VfPpPnYVHy3eR1pmNmHV/Fj4bFte7lYbT1etqCMiUuLZ+46pOCYickX7/oCsNChb3ZhWeT1OzsYF7KfXwgMzIbCBcX4e8TmMbQC/jdBoXbkuVQakZLHZYN9CY1v9xhxWr0aV+OLhJrg6WVm4I5Ynv97IhYwss2NdrkIDeHwR9P6CbA8/iNtL/fBHeS7+v9TxSuLT/o34dkhLQsuXMjupiIg4ipy+YwdX5HlxFxGREmH3xSmVdXsZMzdulNVqPOeplfDwjxDUyiiybZwC45rAvGEQt79gMkuRp+KYlCxx+42VTZxcoVoHs9PINXS5LZApA5vh4eLE8r2neWzqepLSMs2OdZlMG0xJCqNNyodMz+xCls3CPU4R/O70PL1S5mLJdrzMIiJiosCG4OkP6YlwdL3ZaUREHEt6CuxfYmxfa0rltVgsUONOeGIRDFxg/N2XnQlbvzVWnv/hMTixLd8iS/Gg4piULPsWGffBbcHN29wscl1ta5Rj5qAWlHJzZl30WR6ZvI74lAyzY9ltOnSWHuNWM/q3XRxPc+fnCs8R3XcBVG6OJT0ZFr8OE9tA9Cqzo4qIiKOwWv82tXKpuVlERBzNgaXG6vClq9x6f2iLBYLbwIBfYPAyqHUXYINdP8OXbeHbfnBkQz6EluJAxTEpWXL6jWlKZZHRPLgs3w5pRWlPFyKPnKffVxGcTkwzNdPZ5HRe/nEb934RwZ7YRHw9XHi/b33mDrud0Iat4YnFcM/n4OkHp/fAjB7w02A1BBUREUPO1EoVx0REctv9t1Uq83MRq8pN4cHvYNgaqHevsbDWvkUwpTPM6GlMdbfZ8u/zSZGj4piUHKnn4HCEsV2zi7lZJE/qV/bl+yfDKFfKjT2xifT7MoIT8amFniM728Z36w9zx8fL+X7jEQAeaFaZZS+058EWVbBaL/4Ct1qhyaPGqpbNBwMW2D7HWNUyYjxkaaqliEiJVv0OwAKx23XhREQkR2bapcEMdW5ySuX1BNwG9001ztMbPwJWZ4heCTPvgSl3wt5FKpKVUCqOSclxIBxsWVCuNpQJNjuN5FGtwFL88FQYFX3dORiXzP0TIzh0JrnQPv+OY/H0/WINr87dzvmUDGoHluLHoWGMua8hft5uV36SZ1m4+2N48k+o1MzoL/PHf4xh3DF/FVp2ERFxMF7+ULGRsR21zNQoIiIO4+BySEuAUhWgcvOC/Vx+1aHXePh3JLR4Epzd4egG+K4fTGwLO+ZCtgMuCCYFRsUxKTnsUyq7mptDblqIvxdzht1OsJ8nR8+l8sCXERw4lVignzPhQgaj5u/kns9XE3nkPF6uTrzRoy6//asNzYLL3tiLVGwMg5bAPePAoyyc2gXT74K5T0LiyQLNLyIiDqq6+o6JiOSyK2dKZU9jJkZhKB0Ed30Iz26D2/8Nrt5wcjv8+DiMbwmR30KW4/Q8loKj4piUDFmZcODiqifqN1akVSrtwQ9PhVEzwJuTCWk88OVadhyLz/fPY7PZ+CXyGJ0+XsH0NTFk26BHgwqEv9CBQW1CcHbK43+fVis0GQD/2gTNngAssO17+LwZrP1CUy1FREqanL5jUcs0OkFEJCsD9i4wtgtqSuW1lAqALqPhue3Q/hVwLw1n9sPPw2BcE9gwGTIuFH4uKTQqjknJcHSD0XPMvTRUbmF2GrlF5X3c+f7JMOpX8uVscjoPTlrLpkPn8u31D5xK5KFJ63h2diSnE9Oo5u/FN4Na8vlDTQj0db+1F/csCz3+B0OWQcUmxtDxRa/AV+3hUET+vAEREXF8lZuDm69xfnJ8i9lpRETMFbPK+P/Q0x+q3m5eDs+y0PFVeH4HdH4LvMrB+cOw4AX4tCGsGQdpSeblkwKj4piUDPsWGfc17gQnZ3OzSL4o4+XKrCEtaR5chsQLmTw6ZR1rDsTd0mumpmcxZtEeun+6ioiDZ3BztjKyS00WPteWNjX88yn5RZWawOBw6PkpeJSBkztgWjeYNxSSTuXv5xIREcfj5AzV2hvbmlopIiVdzpTK2neD1cncLABupaDNc8ZIsu4fgk9lSIqFxa/D2PqwYgyknjc7peQjFcekZLD3G9OUyuLEx92FGU+0oG0Nf1LSsxg4fQPL9txcD68lu07S+ZMVTFgeRUaWjTtql2fpiPYMv6MGbs4F9AvaaoWmA+Ffm417LLD1O2NVy3VfaqqliEhxlzO18kC4uTlERMyUnQV7fjO265owpfJaXDyg5ZPw7y1G/+Cy1SD1LPz5LvyvHiwdBUmnzU4p+UDFMSn+zsXA6d1gcbq4dLoUJ56uzkwa0IzOdQJIz8zmyZmbWLDtxA0//8jZFAZN38CQmRs5dj6VSqU9+OrRpkx5rBlBZT0LMPnfeJY1RpANDocKjSAtHha+BF91gMPrCieDiIgUvtCLTfmPbYSUs+ZmERExy+G1kHzaaIET0t7sNFfm7Gr0D35mA9w7BcrXNVaiX/0/YyTZwlcg/pjZKeUWqDgmxd++xcZ9lVZGEUKKHXcXJ754pAn3NKxIZraNf323mTkbj1zzOWmZWXy+bD+dP1lB+J5TuDhZGNahOktGtKPLbYFYLJZCSv83lZsavcju/sQ4OTi5HaZ2gZ+f1hUpEZHiyLcylKsDtmw4uNzsNCIi5th9cUplrbvAycXcLNfj5Az174Ohf0H/b40ewpmpsO4LoyfZ/H/D2YNmp5SboOKYFH85/cZqdjU3hxQoFycr/+vXiP7Ng8i2wYs/bmNmRMwVj129P47uY1fx0eJ9pGVmE1bNj4XPtuXlbrXxdDW5J53VCZoPMqZaNhlg7IucBZ83hfWTtKKZiEhxkzN6TFMrRaQkys6G3b8a2442pfJarFajP9qQZfDoPKjaBrIzYPMMGNcUfhoCp3abnVLyQMUxKd7SkoyVT0D9xkoAJ6uF9/vW54nWIQC8+ctOJiw/YH/8ZMIFhn+7mUemrONgXDL+3m582r8R3w5pSWj5UmbFvjIvP6OvwaClUKEhXIiH30caUy2PrDc7nYiI5Bd737GlYLOZm0VEpLAd2wQJx8DVG6p1NDtN3lksRuuexxfA44sg9E5jNPD2H2BCK5j9sFYkLiK0bJ8Ub9ErICsdygSDf02z00ghsFgsvNGjDt5uTny27ABjFu0lOS2Tsl5u/G/JPpLSMrFaYEBYMCO61MTH3cGHbgc1hyF/wqZpEP42xG6DKXdC40cuLi+dz6toiohI4aoSBi6exipoJ3dCYD2zE4mIFJ7dvxj3NbuCi7u5WW5V1TCo+qNRDFv1sTEibs9vxi20M7QdaRwjDkkjx6R4s0+p7GZU9aVEsFgsjOhSi1e61wZg/J9RjP5tF0lpmTQKKs384W0Ydc9tjl8Yy2F1guaDjamWjR8x9m35BsY1gQ2TNdVSRKQoc3GH4LbG9oGl5mYRESlMNhvsuthvrE4RmlJ5PRUbQ79v4Ol10KCfsTDcgaUwrRtMu8uYRq+Rwg5HxTEpvrKzLzXjV7+xEmlo++qM7nUbAL4eLrzftz5zh91OvUq+Jie7SV7+0Gs8DFoCgfWNqZYLXoBJHeHoRrPTiYjIzfr71EoRkZIidhucPwTOHlDjTrPT5L/ytaHvV/CvTdB0IDi5wqG/4Ju+MOkO2P2b8TerOAQVx6T4it1qTFFw9Yaqrc1OIyZ5NCyYlS92ZNXLHXmwRRWs1mIwgjCoBTy5Au76CNx84cRWmNwJ5v8Lks+YnU5ERPIqpyn/4bWQlmhuFhGRwpIzaqxGZ3D1MjdLQSobAj0/hWe3QqunjWLg8c3w/cMwsTVs/1EzQRyAimNSfO37w7iv3hGc3czNIqaq4udZdKZQ3iirE7QYAv/aCI0eNvZtnmmsarlxqn7BiogUJX7VoUyIsdJZ9Cqz04iIFDybDXZd7DdWp5e5WQqLT0Xo9j48tx3ajAA3Hzi1C34aBJ83M87lM9PNTlliqTgmxdff+42JFFfe5aH3BHjiDwioD6nn4LfnjZFkxzaZnU5ERG6UplaKSElyeg+c2W9MNSxpLXC8y0Hn/zOKZB1fB4+ycPagMQvks8aw7kvISDU7ZYmj4pgUT4mxl5bMrdHF3CwihaFKK3hyOXQfY1yFOr4FJnWCX5+FlLNmpxMRkevJmVp5YIkaNYtI8ZczpbJaR3D3MTeLWTxKQ/sXjSJZl3fBOxASjsLCl2BsfVj9P7iQYHbKEkPFMSme9l9sxF+pqTGyRqQkcHKGlk/B8I3Q8EHABpumG6tabpquhp8iIo4suC1YXeD8YTgTZXYaEZGCtfticaxuMVql8ma5ecPtw42eZHd/Ar5VIPk0LB0FY+vBn+/rYnchUHFMiqecfmM1StgQXRGAUgHQZyI8vhDK32ZMtfz1WZjSGY5tNjudiIhciZs3VA0ztjW1UkSKszNRcHIHWJ2h1l1mp3EcLu7QfBD8ezP0/gL8ahir06/4rzGSbPEbkHjS7JTFlopjUvxkXICoP43tkjZ/XeTvqt4OT62Ebv8F11JGD7JJdxg9yXT1SUTE8ajvmIiUBDmjxoLbgmdZc7M4IicXaPQQPLMO7p9u9BVOT4I1nxlFsgUj4fwRs1MWOyqOSfFzaDVkJEOpClChodlpRMzl5AythhmrWjboB9iM1SzHNTVWxNFUSxERx5FTHItZbVzsExEpjnJWqdSUymuzOsFtfWDoKnjoB6jcArLSYMMk+KwR/PwMxB0wO2WxoeKYFD/2KZVdwGIxN4uIoygVCH2/goELoFwdSD1rrIgz5U44Hml2OhERAShf17i4l5kKh9eYnUZEJP+dP3xx4TQL1O5hdpqiwWIxZkQNWgyP/Qoh7SE7EyK/gfHNYc7jELvD7JRFnopjUrzYbLBvkbFds5u5WUQcUXAb4+pT1/cuTrXcCF91gAUvGL3JRETEPBbL31atDDc3i4hIQdj9q3Ff9XYtnJZXFguEtIPH5sOgpVCzO9iyYedcmNgavu0PRzeanbLIUnFMipfTe4yrEU5uUK292WlEHJOTC4Q9A8M3QP37ARtsmGxMtdzyjaZaioiYSX3HRKQ425WzSmUvc3MUdUHN4aHZMHS1MfUSC+xbCJM7wcxeEL3KGDgiN0zFMSleckaNhbQDVy9zs4g4Op8KcO9keOw3KFcbUs7AL8/A1K5wYqvZ6URESqZqHcBivXjBTw2XRaQYSYyFI+uM7To9zc1SXATWN5r2D98AjR42VgA9uBxm9DDO6fctVpHsBuW5OLZy5Up69uxJxYoVsVgs/Pzzz9d9zvLly2nSpAlubm6EhoYyffr0XI9nZWXxxhtvEBISgoeHB9WrV2f06NHY9E2UvMrpN6ZVKkVuXEhb46pTl3fA1RuOrjemWv7+IqSeNzudiEjJ4lEGKjc3tqM0tVJEipHdvwI24/84n4pmpyle/GtA7wnw7y3QfLAxk+rIOvj2fviyHez8WbNDriPPxbHk5GQaNmzI+PHjb+j46Oho7r77bjp27EhkZCTPPfccgwcP5o8//rAf88EHH/DFF1/w+eefs3v3bj744APGjBnDuHHj8hpPSrKUs5euRKg4JpI3Ti5w+7+Mq0717jX6F6z/Cj5vBpHf6pepiEhh0tRKESmOclaprKNVKgtM6Spw98fw3Dbj3N7FC2K3wZzHYEIr2DobsjLNTumQLLZbGJ5lsViYN28evXv3vuoxL7/8MgsWLGDHjkurJ/Tv35/z58+zaJExBa5Hjx4EBAQwZcoU+zH33nsvHh4efPPNNzeUJSEhAV9fX+Lj4/Hx8bm5NyRF27Y5MHcwlL8NntYKTyK35OAK+H0kxO0zPg5qBXd/ZAzdFilGdP5QNJS479OxTTDpDnDzgZcOGhcwRESKsuQ4+KiGcQH22a1QJtjsRCVDyllY+wWs/xIuxBv7SleFNs8Z0zCd3UyNV9Dycv5Q4D3HIiIi6Ny5c659Xbt2JSIiwv7x7bffTnh4OPv2GX+Ebd26ldWrV9O9e/ervm5aWhoJCQm5blLC2Vep1KgxkVtWrT0M/QvufNu44nRkrTEke+HLl36xiohIwajQGDz9IC0Bjm4wO42IyK3bs8AojAU2UGGsMHmWhTteg+d2QKf/A09/OH8IfnsePm0IEeMhPdnslA6hwItjsbGxBAQE5NoXEBBAQkICqampALzyyiv079+f2rVr4+LiQuPGjXnuued4+OGHr/q677//Pr6+vvZbUFBQgb4PcXBZmXBgibFds5u5WUSKC2dXaP2sMdXytj7GCc26iTCumTEkW30hRUQKhtUK1e8wtjW1UkSKg91apdJU7j7QdgQ8tx26fQA+lSDxBPzxHxhbH1Z+WOIvgDvEapU//PADs2bN4ttvv2Xz5s3MmDGDjz76iBkzZlz1Oa+++irx8fH225EjWs2nRDuyzvjH7FEWKjczO41I8eJbyVgF59Gfwa8GJJ+CeU/BtO5wcqfZ6UREiif1HROR4iL1vNGyA1QcM5urJ7QaajTu7/kZlAkxVqxf9g78rz6Ej4bkM2anNEWBF8cCAwM5efJkrn0nT57Ex8cHDw8PAF588UX76LH69evz6KOP8vzzz/P+++9f9XXd3Nzw8fHJdZMSLGdKZY0uYHUyN4tIcVW9IwxbA51HgYsnHI6AiW1h0asl/kqTiEi+yxk5dmIrJJ0yN4uIyK3YtwiyM6BcHWNVRTGfsxs0fQyGb4S+k6FcbUiLh1Ufwdh6sOg/kHDC7JSFqsCLY2FhYYSH516GesmSJYSFhdk/TklJwWrNHcXJyYlsrY4mN2rfxdVP1W9MpGA5u0Kb542plnV7gS0L1k6Az5vDth801VJEJL94lzd68wBELTM3i4jIrchZpbKuVql0OE7O0OB+GBYB/b6BCo0gIwXWjodPG8Cvz8G5GJNDFo48F8eSkpKIjIwkMjISgOjoaCIjIzl8+DBgTHccMGCA/fihQ4dy8OBBXnrpJfbs2cOECRP44YcfeP755+3H9OzZk3fffZcFCxYQExPDvHnz+OSTT+jTp88tvj0pEc4ehLi9YHW+dJVVRAqWb2V4YCY8Mhf8QiHpJMwdAtPvhpO7zE4nIlI8aGqliBR1aYlw4OJgmToqjjksqxXq9IQnl8MjP0GV2yErHTZNg8+awNyn4PRes1MWqDwXxzZu3Ejjxo1p3LgxACNGjKBx48a8+eabAJw4ccJeKAMICQlhwYIFLFmyhIYNG/Lxxx8zefJkuna9NMJn3Lhx3HfffTz99NPUqVOHkSNH8tRTTzF69OhbfX9SEuxbbNxXCQOP0qZGESlxQjsZUy07vQnOHnDoL5jYBv54DS5oFWERkVuSUxyLWgaaUSEiRdH+xZCVBmWrQcBtZqeR67FYjN89TyyExxdC9U7GTJFts2F8S/hhgDHdvxiy2GzFYw5MQkICvr6+xMfHq/9YSTOzNxz8E7q8C7cPNzuNSMl1/rCx4s3uX42PvQOh67tQ717jF62IA9L5Q9FQYr9PWRnwQQikJ8KQP6FSE7MTiYjkzQ+Pwa6fjbYcnUeZnUZuxrHNsOpj2PPbpX01ukDbkVClpXm5bkBezh8cYrVKkZuWlggxq41t9RsTMVfpKkavgod/Mq4OJsXCT4NgRk84tcfsdCIiRY+TC1Rrb2wfCL/2sSIijiY9BfYvMbY1pbLoqtQE+s8y+pLVvx8sVmNE4NQuML0HRP1ZLPoOqzgmRVvUn8bKJ2WrGX2PRMR8NTobvzzveN2YahmzCia2hsWvGwVtERG5ceo7JiJFVVQ4ZCSDbxWo2NjsNHKrAurCvZONFS6bDACri3Ge/3VvmNwJ9i4s0kUyFcekaLOvUtlN07ZEHImLO7R7EZ5ZB7V7QHYmrBlnrGq546ci/YtTRKRQhXYy7o+uh9Rz5mYREcmLXfON+zo99bdaceJXHe4ZB89GQsuh4OwOxzbBd/2N3sM7foLsLLNT5pmKY1J0ZWfD/pzimKZUijikMlWNYdgPzYEyIZB4An58AmbeU+xXvBERyRelq4B/LbBlw8EVZqcREbkxmWmwb5GxXVdTKosl38rQ/QN4bofRU861FJzcYZzrj28BW74xemcWESqOSdF1fAsknzb+EVa53ew0InItNbvA02uh42vG1aXolfDF7bDkTUhLMjudiIhj09RKESlqDq6AtARjgabKLcxOIwXJu5yx2MLz26HDf8CjDJw5AL88A581hvWTICPV7JTXpeKYFF05VyJC7wBnV3OziMj1ubhD+5eMqZa17jKmWv71qTHVcuc8TbUUEbmanKmVB8L1f6WIFA27fzHu6/QEq8oOJYJHGejwsjGS7M7R4B0A8Ufg95EwtoFx3u/A/Yf1UypFV05xrGY3c3OISN6UCYYHv4MHv4fSVSHxOMwZaDTzPL3P5HAiIg6oamtjgZPE43Bqt9lpRESuLSsD9iwwtjWlsuRx84bW/4Znt8FdH4FvECSfMmaM/K8eLP/AIXtoqjgmRVPCcYjdBlgg9E6z04jIzajVzRhF1uFVcHKDg8uNqZZLR0F6stnpREQch4s7BLcxtjW1UkQcXcxqo/jh6af2NyWZizu0GAL/3gK9JoBfKFw4D8vfg//VhyX/B0mnzE5pp+KYFE37Fxv3lZsZc5xFpGhy8YAOr8Aza6FGV8jOgNX/g89bwK5fNH1IRCSH+o6JSFGx++IqlbXvBidnc7OI+ZxcoPHD8Mx6uG8qBNSD9ET4ayyMrQ+/vwQJJ8xOqeKYFFH7tEqlSLFStho8/AM8ONtYmS3hKPwwAL7pC3EHzE4nImK+nOLY4QgtZCIijis7C3b/ZmzX6WVuFnEsVieody8MXW2c81dqBpkXYP2XkBJndjoVx6QIykg1pl+B+o2JFDe1uhtXldq/bEy1jFoGE1pB+NuaaikiJZtfdePiQVa6MWVJRMQRHVln9Jdy94WQdmanEUdksRjn/IOXwoBfoN1LEFjf7FQqjkkRFLMaMlLAp5IxJFNEihcXD+j4H3g6wugpmJ0Bqz6G8S1h96+aaikiJZPFoqmVIuL4dl2cUlnrLnB2NTeLODaLBap1gDteMzsJoOKYFEX2VSq7Gv+gRKR48qsOD8+B/t+CbxVjKejvH4FZ98GZKLPTiYgUPhXHRMSRZWdf6jdWR6tUStGi4pgULTbb3/qNaUqlSLFnsRjNXJ9ZB+1eBCdX44/CCa1g2TuQnmJ2QhGRwhPSDqzOcC5aFwlExPEc3wwJx8DVG6rfYXYakTxRcUyKllO7jNEjzh6awy5Skrh6wh2vw9NroXono+fOyg+NqZZ7FmiqpYiUDG6loEqYsR21zNwsIiL/tOsX475GF3BxNzeLSB6pOCZFS86Uymrtjb5EIlKy+FWHR36Cft+AbxDEH4bZD8HshyH5jNnpREQKXmgn415TK0XEkdhsl6ZU1tWUSil6VByTosU+pbKruTlExDwWC9TpaUy1bPsCWF1g7wKY2AZi/jI7nYhIwcrpOxa9EjLTzM0iIpIjdjucizFm+ITeaXYakTxTcUyKjuQzcGS9sV1DxTGREs/VCzq9CU/+CX41IPE4zOgByz+A7Cyz04mIFIyAeuAdYKzcfTjC7DQiIoacUWOhncDN29wsIjdBxTEpOg4sAWwQWB98K5mdRkQcRWB9eGoFNHoYbNmw/D2Y2QsSTpidTEQk/1ksWrVSRBxPTr+xur3MzSFyk1Qck6Ijp9+YRo2JyD+5ekHvCdDnK3DxgphVMLE17F9idjIRkfxn7zsWbm4OERGAU3sgbp/R6kLtb6SIUnFMioasjEsngDW7mZtFRBxXw37w1EoIbAApZ2DWfbD4dchMNzuZiEj+qdYRLNaLq3gfMzuNiJR0OVMqq3cEd19zs4jcJBXHpGg4HAFpCeDpD5WamJ1GRByZfygMXgotnjI+XjMOpnWDs9Hm5hIRyS+eZaFSU2M7SqPHRMRkuy4Wx+polUopulQck6IhZ5XKGl3A6mRuFhFxfM5ucNcY6P8tuJeGY5vgy3awc57ZyURE8of6jomIIzh7EE5uB4sT1L7b7DQiN03FMSkacvqNaQ67iORF7bth6GoIamWMPp0zEH59FjJSzU4mInJrcopjUcshK9PUKCJSguWMGgtpa4xqFSmiVBwTxxd3AM4cAKszVL/D7DQiUtSUDoKBC6DtSMACm6bDpDuM5rEiIkVVxcbgUQbS4uHYRrPTiEhJlbNKpaZUShGn4pg4vv0Xp1RWbQ3uPuZmEZGiyckZOr0Bj84Dr/JGE+uvOsDmmWCzmZ1ORCTvrE6XLhpqaqWImOH8ETi+GbBA7R5mpxG5JSqOiePL6TemVSpF5FZV7wjD/jL+oMxMhfn/gp8Gw4UEs5OJiORd9U7GvYpjImKG3b8a91XCoFSAuVlEbpGKY+LYLiTAob+MbfUbE5H84F0eHv4JOo8ymsfu+NFo1n9ss9nJRETyJvRicez4Fkg6bW4WESl5dl/sN1a3l7k5RPKBimPi2KKWQXYm+NUAv+pmpxGR4sJqhTbPwxOLwLcKnIuGKV0gYoKmWYpI0VEqEALqG9sH/zQ3i4iULIkn4fBaY7tOT3OziOQDFcfEsdmnVGrUmIgUgKAWMHSlcVKXnQF/vArf9YfkM2YnExG5MTmjxw6Em5tDREqWPb8CNqjUDHwrmZ1G5JapOCaOKzsL9i82ttVvTEQKikcZeOBruPtjcHKDfYtgYhuI+cvsZCIi1xfa2biPCofsbHOziEjJkbNKZV2tUinFg4pj4riObYaUOHDzhSqtzE4jIsWZxQLNB8OQcGMad+JxmNEDln9gFOpFRBxVUEtw9Ybk0xC7zew0IlISJJ+5dBGxjopjUjyoOCaOa98i4z60Ezi5mJtFREqGwPrw1Apo9DDYsmH5ezCzFyScMDuZiMiVObtCSHtjW6tWikhh2LsAbFnGeVPZELPTiOQLFcfEcdn7jWlKpYgUIlcv6D0B+nwFLl4Qswomtob9S8xOJiJyZeo7JiKFaZdWqZTiR8UxcUzxR+HkdrBYL/XSEBEpTA37wVMrjauiKWdg1n2w+HXITDc7mUiBGj9+PMHBwbi7u9OyZUvWr19/Q8+bPXs2FouF3r1759o/cOBALBZLrlu3brrwla9yimNH1sGFeHOziEjxlnoeDi43tuuoOCbFh4pj4phyRo1VbgFefuZmEZGSyz8UBodDi6eMj9eMg2nd4Gy0ublECsj333/PiBEj+L//+z82b95Mw4YN6dq1K6dOnbrm82JiYhg5ciRt27a94uPdunXjxIkT9tt3331XEPFLrjLBRr9EWxYcXGF2GhEpzvb9YazwXa42lKtpdhqRfKPimDgm+5TKrubmEBFxdoO7xkD/b8G9NBzbBF+2g53zzE4mku8++eQThgwZwuOPP07dunWZOHEinp6eTJ069arPycrK4uGHH+att96iWrVqVzzGzc2NwMBA+61MmTIF9RZKrpyR9uo7JiIFKWeVSjXil2JGxTFxPOkpEH3xqqeKYyLiKGrfDUNXQ1ArSEuAOQPh12chI9XsZCL5Ij09nU2bNtG586V2Blarlc6dOxMREXHV57399tuUL1+eQYMGXfWY5cuXU758eWrVqsWwYcM4c+bMNbOkpaWRkJCQ6ybXYS+OhYPNZm4WESme0pIg6mJvw7oqjknxouKYOJ7olZB5AXyDoHxds9OIiFxSOggGLoC2LwAW2DQdJt0Bp/aYnUzklsXFxZGVlUVAQECu/QEBAcTGxl7xOatXr2bKlClMmjTpqq/brVs3Zs6cSXh4OB988AErVqyge/fuZGVlXfU577//Pr6+vvZbUFDQzb2pkiS4NTi7Q8JROL3X7DQiUhztX2z8nVYmBALqmZ1GJF+pOCaOZ98i475mV7BYzM0iIvJPTs7Q6U14dB54lYdTu+CrDrB5pkZrSImSmJjIo48+yqRJk/D397/qcf379+eee+6hfv369O7dm99++40NGzawfPnyqz7n1VdfJT4+3n47cuRIAbyDYsbFA6q2NrY1tVJECsLuv61Sqb/TpJhRcUwci832t35jWslKRBxY9Y4w7C+ofgdkpsL8f8FPg+GCpn9J0eTv74+TkxMnT57Mtf/kyZMEBgZednxUVBQxMTH07NkTZ2dnnJ2dmTlzJvPnz8fZ2ZmoqKgrfp5q1arh7+/PgQMHrprFzc0NHx+fXDe5Aeo7JiIFJSMV9i02tjWlUoohFcfEscRuh8Tj4OIJwVde8UpExGF4l4eHf4LOo8DiBDt+NJr1H9tsdjKRPHN1daVp06aEh4fb92VnZxMeHk5YWNhlx9euXZvt27cTGRlpv91zzz107NiRyMjIq06FPHr0KGfOnKFChQoF9l5KrNBOxv2hvyA92dwsIlK8HAiHjGSj9U3FJmanEcl3Ko6JY8kZNVatA7i4mxpFROSGWK3Q5nl4YhH4VoFz0TClC0RM0DRLKXJGjBjBpEmTmDFjBrt372bYsGEkJyfz+OOPAzBgwABeffVVANzd3alXr16uW+nSpSlVqhT16tXD1dWVpKQkXnzxRdauXUtMTAzh4eH06tWL0NBQunbVojv5zr+m8YdrVjrE/GV2GhEpTnKmVNbpqSmVUiypOCaOZX/OlEqdMItIERPUAoauNE4aszPgj1fhu/6QfO1V+UQcSb9+/fjoo4948803adSoEZGRkSxatMjepP/w4cOcOHHihl/PycmJbdu2cc8991CzZk0GDRpE06ZNWbVqFW5ubgX1Nkoui+XS6LGo8GsfKyJyozLTYe/FvtB1NKVSiieLzVY8LmsnJCTg6+tLfHy8+lIUVUmn4aMagA1G7AafimYnEhHJO5sNNk6BRf+BrDQoVRHunWysJCcOR+cPRYO+T3mw+1f4/hHwC4V/bTI7jYgUB/uXwKz7wDvQ+DvNqjE2UjTk5fxBP9XiOA4sAWxQoaEKYyJSdFks0HwwDAkHvxpGH8UZPWD5B5CdZXY6ESnuQtqB1RnOHICz0WanEZHiYNcvxn2dHiqMSbGln2xxHPsuDtXVKpUiUhwE1ocnl0Ojh8GWDcvfg5m9IOHGp6SJiOSZuy8EtTS2NbVSRG5VVibsWWBsa0qlFGMqjoljyEyHA8uMbfUbE5Hiws0bek+APl+BixfErIKJrY3pCSIiBSWn79gBFcdE5BYdWg2pZ8GjLFRViwgpvlQcE8dweA2kJ4JXeajQ2Ow0IiL5q2E/eGqlMZos5YzRt2Px68aFARGR/Bba2bg/uEL/z4jIrdl1cZXK2neDk7O5WUQKkIpj4hj25axS2UXz2EWkePIPhcHh0OIp4+M142BaN/UEEpH8F1DfuOCYkQxH1pqdRkSKquxs2PObsV23l7lZRAqYqhBiPpsN9i40ttVvTESKM2c3uGsM9JsF7qXh2Cb4sh3snGd2MhEpTqzWv02tXGpuFhEpuo6sg6ST4OYLIe3NTiNSoFQcE/OdOQDnosHJFap1MDuNiEjBq9MDhq6GoFaQlgBzBsKvz0JGqtnJRKS4yJlaqb5jInKzdl+cUlmrOzi7mptFpICpOCbmy1mlMrgNuJUyN4uISGEpHQQDF0DbFwALbJoOk+6AU3vMTiYixUG1joAFTu7QKrkiknc226V+Y3W1SqUUfyqOifns/cY0pVJEShgnZ+j0Jjw6z+gPdGoXfNUBNs80TkpFRG6Wlx9UamJsR2n0mIjk0bHNkHDUWG27+h1mpxEpcCqOiblSz8OhNcZ2jS6mRhERMU31jjDsL+PkMzMV5v8LfhoMFxLMTiYiRZl9aqX6jolIHu3+xbiv2QVcPMzNIlIIVBwTc0WFgy0L/GtB2RCz04iImMe7PDz8E3QeBRYn2PGj0az/2Gazk4lIUZVTHIv6E7Iyzc0iIkXH36dU1tGUSikZVBwTc9mnVHY1N4eIiCOwWqHN8/DEIvCtYixWMqULREzQNEsRybuKTcDdFy6ch+MqtIvIDTq5wzgHcXbX7B4pMVQcE/NkZ8H+xca2+o2JiFwS1AKGroQ6PSE7A/54Fb7rD8lnzE4mIkWJk/PFxvxoaqWI3LicUWOhncHN29wsIoVExTExz9ENkHrOuKIZ1NLsNCIijsWjDDzwNdz1ETi5GSv7TmwDMX+ZnUxEihJ73zE15ReRG7RbUyql5FFxTMyzb5FxH3qncWVTRERys1igxRAYEg5+NSDxOMzoAcs/MEbfiohcT2gn4/7YJkg5a24WEXF8p/fC6T1gdVHrGylRVBwT89j7jWlKpYjINQXWhyeXQ8OHwJYNy9+Dmb0g4YTZyUTE0flUhPK3ATaIWmZ2GhFxdDlTKqt1AI/SZiYRKVQqjok5zh+GU7vAYr10RVNERK7OzRv6fAF9vgQXL4hZBRNbw/4lZicTEUeXc66lqZUicj27fzHu6/YyN4dIIVNxTMyRM2osqBV4ljU3i4hIUdKwPzy10hhNlnIGZt0Hi1+HzHSzk4mIo7L3HVsK2dnmZhERx3U2GmK3g8UJat9tdhqRQqXimJjDPqVS89hFRPLMPxQGLYUWTxkfrxkH07oZJ7UiIv9UpZUx4jT5FJzcYXYaEXFUOY34g9toAIOUOCqOSeFLT4bolca2+o2JiNwcF3e4awz0mwXupY1m21+2g53zzE4mIo7G2Q1C2hnbB5aam0VEHNeunCmVWqVSSh4Vx6TwHVwBWWlQuiqUq2V2GhGRoq1ODxi62pimnpYAcwbCr89CRqrZyUTEkajvmIhcS/xR40IbFqjd0+w0IoVOxTEpfPsWGfc1u4HFYm4WEZHioHQQDFwAbV8ALLBpOky6A07tMTuZiDiKnL5jR9bChQRzs4iI49n9q3FfpRWUCjA3i4gJVByTwmWzqd+YiEhBcHKGTm/Co/PAq7yxIvBXHWDz18b/vSJSspUNgbLVITvzUnsLEZEcuy72G9MqlVJCqTgmhevEVkiKNZrCBrcxO42ISPFTvSMM+wuq3wGZqTB/OPw0WCNFRCT3qpUiIjkST8LhCGO7jqZUSsmk4pgUrpxRY9U7Gs1hRUQk/3mXh4d/gs6jjOXYd/xoNOs/ttnsZCJiJntxLFwjSkXkkj2/ATao1BR8K5udRsQUKo5J4fp7vzERESk4Viu0eR6eWAS+VeBcNEzpAhET9EexSEkV3Bqc3CD+MMTtNzuNiDiKnFUq62iVSim5VByTwpN4Eo5fHLVQo4u5WURESoqgFjB0pTFNIjsD/ngVvusPyWfMTiYihc3VC6qGGduaWikiAClnIWa1sV1XxTEpuVQck8Kzf7FxX7GJVkARESlMHmXgga/hro+MUSP7FsHENhDzl9nJRKSwqe+YiPzdngVgy4KA+lC2mtlpREyj4pgUHvuUSq1SKSJS6CwWaDEEBi8FvxqQeBxm9IDlH0B2ltnpRKSw5BTHDv0FGanmZhER8+3WKpUioOKYFJbMNIj609hWcUxExDwVGsCTy6HhQ2DLhuXvwcxekHDC7GQiUhjK1QafSpB5wSiQiUjJdSH+0t9omlIpJZyKY1I4YlZDRjJ4B0JgQ7PTiIiUbG7e0OcL6PMluHhBzCqY2Br2LzE7mYgUNIsFQjsZ2wfCzc0iIuba94fRj9S/FpSrZXYaEVOpOCaFY98fxn3NLsYKaiIiYr6G/eGplRBYH1LOwKz7YPHrkJludjIRKUjqOyYicGmVSo0aE1FxTAqBzfa3fmPdzM0iIiK5+YfCoKXQ4knj4zXjYFo3OBttbi4RKTgh7cHiBHH74Nwhs9OIiBnSki4VyOuoOCaS5+LYypUr6dmzJxUrVsRisfDzzz9f9znLly+nSZMmuLm5ERoayvTp0y875tixYzzyyCP4+fnh4eFB/fr12bhxY17jiSM6vRfOHzJWSAtpb3YaERH5Jxd3uOtD6DcL3EvDsU3wZTvYOc/sZCJSEDxKQ1ALYztKUytFSqQDS4zeg2VCjBHkIiVcnotjycnJNGzYkPHjx9/Q8dHR0dx999107NiRyMhInnvuOQYPHswff/xhP+bcuXO0bt0aFxcXFi5cyK5du/j4448pU6ZMXuOJI9p/8Xsd0tbocyMiIo6pTg8YuhqCWkJaAswZCL8+qxXtRIoj9R0TKdl25axSeY/Ri1CkhHPO6xO6d+9O9+7db/j4iRMnEhISwscffwxAnTp1WL16Nf/73//o2tVYtfCDDz4gKCiIadOm2Z8XEhJyzddNS0sjLS3N/nFCQkJe3oYUJnu/MU2pFBFxeKWDYODvxiqWqz6BTdPhyHq4bxqUr212OhHJL6GdYdk7cHCF0WfQ2dXsRCJSWDIuwP7FxnadXuZmEXEQBd5zLCIigs6dO+fa17VrVyIiIuwfz58/n2bNmnH//fdTvnx5GjduzKRJk675uu+//z6+vr72W1BQUIHkl1uUchYOrzW2a3QxN4uIiNwYJ2fo9CY8Og+8ysOpXfBVB9j8tdFHUkSKvsCG4OkP6YlwdL3ZaUSkMEUtg/Qk8KkMlZqYnUbEIRR4cSw2NpaAgIBc+wICAkhISCA11ZimcfDgQb744gtq1KjBH3/8wbBhw/j3v//NjBkzrvq6r776KvHx8fbbkSNHCvR9yE2KWga2LChfF8pUNTuNiIjkRfWOMOwvqNYRMlNh/nD4aTBc0GhtkSLPav3b1EqtWilSouSsUlmnp6ZUilzkEKtVZmdn06RJE9577z0aN27Mk08+yZAhQ5g4ceJVn+Pm5oaPj0+umzgg+yqVXc3NISIiN8e7PDwyFzr9n7G63Y4fjWb9xzabnUxEblXoxdkdKo6JlByZ6bB3obFdV6tUiuQo8OJYYGAgJ0+ezLXv5MmT+Pj44OHhAUCFChWoW7durmPq1KnD4cOHCzqeFKSsTNi/xNhWvzERkaLLaoW2I+CJReAbBOeiYUoXiJigaZYiRVn1OwALxG6HxFiz04hIYYheCWnx4B1gLMAjIkAhFMfCwsIID8+9Cs6SJUsICwuzf9y6dWv27t2b65h9+/ZRtaqm4RVpR9fDhfPgUQYqNzc7jYiI3KqgFjB0FdTuAdkZ8Mer8F1/SD5jdjIRuRle/lCxkbEdtczUKCJSSHZfnFJZuwdYnczNIuJA8lwcS0pKIjIyksjISACio6OJjIy0j/J69dVXGTBggP34oUOHcvDgQV566SX27NnDhAkT+OGHH3j++eftxzz//POsXbuW9957jwMHDvDtt9/y1Vdf8cwzz9zi2xNT5UyprNFF//GKiBQXHmWg3zdw10fg5Gb8Xz+xDcT8ZXYyEbkZ1dV3TKTEyMqEPQuMbU2pFMklz8WxjRs30rhxYxo3bgzAiBEjaNy4MW+++SYAJ06cyDUdMiQkhAULFrBkyRIaNmzIxx9/zOTJk+na9VIPqubNmzNv3jy+++476tWrx+jRoxk7diwPP/zwrb4/MdO+P4x79RsTESleLBZoMQQGLwW/GpB4HGb0gOUfQHaW2elEJC9y+o5FLdO/X5Hi7tBfkHIGPMpC1TZmpxFxKBabrXg0C0lISMDX15f4+Hg153cEZ6Phs0ZG8+aXDoJHabMTiYhIQUhLgt9fhK3fGh8Ht4W+k8Cngrm5bpDOH4oGfZ8KUFYmjKlm9CAavAwqNzU7kYgUlAUvwIbJ0PgR6DXe7DQiBS4v5w8OsVqlFEP7Fxv3VW9XYUxEpDhz84Y+X0CfL8HFC2JWwcTWlxZkERHH5uQM1dob25paKVJ8ZWfD7t+M7Tq9zM0i4oBUHJOCkdNvTFMqRURKhob94amVEFjfmLIx6z5Y/LqxZLyIOLacqZUqjokUX0fXQ1IsuPleKoiLiJ2KY5L/0hIhZrWxXbObuVlERKTw+IfCoKXQ4knj4zXjYFo3Y6q9iDiu/2/vvsOrrO//jz/PyU7IIIQsCBBIgISEPcpQViAMUazW8bMt2mqropVSq9JWnC2tdWAVtd8O0drWUTcbwxJEUCASIKwQdgYziyyS+/fHnZwQIUggyX3OyetxXfd17pxzn5PX7W30zjufz/sTV9OU/8jXcOaktVlEpHns+MR87DEBPH2szSLihFQck6a3bxVUVUDbWGgXZ3UaERFpSV6+MOnPcPO/wTcEjmyCv14N2z+0OpmINCS4I7RPAKPavI8TEfdiGJBZUxxL0CqVIhei4pg0PceUygnmimYiItL6JFwDd6+FmCFQXgjv3Q6fPgCVpVYnE5ELqR09tjfN2hwi0vSOboaCQ2Zv0NqfdRGpR8UxaVrV1bC7phm/+o2JiLRuITFw+0K46leADTbNh7+NgfydVicTkW87t++YeyxmLyK1aqdUxo8DLz9rs4g4KRXHpGnlbIGSfPBuA52HW51GRESs5uEFY2fDjz6AgHDI3wH/Nwo2/0u/gIs4k05DwcvfbNidt93qNCLSVM6dUpmoVSpFGqLimDSt2lFj3caAp7e1WURExHl0GwP3rIOuo+FsKXxyH7x/J5QVWp1MRMDsF9jlKnNfq1aKuI+87XByH3j6Qvx4q9OIOC0Vx6RpndtvTERE5FxtwuGHH8DYx8DmAdv+ZzbrP7LZ6mQiAvWnVoqIe6gdNdZtLPi0sTaLiBNTcUyaTmEO5KQDNnM+u4iIyLfZ7XDVTPjJEgiOgVPZ8I/xsP4VTbMUsVpto+6DX0J5kbVZRKRp1PYbS9QqlSIXo+KYNJ09NVMqOwwwRweIiIg0JGYw3P059LwGqith6Sz47y1QcsLqZCKtV7tu0DbW/JnM/tzqNCJypY7thmOZYPfSzB6R76DimDSd3UvNR/2HV0RELoVfW7j5LZj0LHj4mFPzXxsB+9dZnUyk9dLUShH3kfmx+dh1JPiFWBpFxNmpOCZNo7IM9q0097unWptFRERch80Gg++COz+DdnFQdBTeuAZW/Qmqq6xOJ9L6OIpjyzXVWcTV7dAqlSKXSsUxaRr710LlGQiMhshkq9OIiIirieoNP1sNff4fGNWw6g/w5nVmP0sRaTldRphTsE4fhBNZVqcRkct1Mhtyt5oL4PSYbHUaEaen4pg0DccqlanmKAAREZHG8mkD178K1/8VvAJg/+fw2nDYs9zqZCKth08b6DzU3M9KszaLiFy+zE/Nxy7DIaCdtVlEXICKY3LlDEP9xkREpOn0uQV+vsYciXzmBPz7RjiyyepUIq2H+o6JuL7MmimVCVqlUuRSqDgmVy4/EwoOgqcvxF5tdRoREXEHYXHw089g8M8g+SaI7m91IpHWo7Y4lv252VdWRFxLwRE4/BVgg4QpVqcRcQmeVgcQN1A7pTJ2JHj7W5tFRETch5cvTPqz2ZhfU/ZFWk54IgRGQVEOHPwCuo2xOpGINEbtlMqYIRAYaW0WERehkWNy5RxTKrVKpYiINAO7h9UJRFoXmw3ixpr7e9V3TMTlZGqVSpHGUnFMrkzJCTi80dxXcUxERETEPajvmIhrKs6HA1+Y+5pSKXLJVByTK7P3MzCqISIZgjtanUZEREREmkLXUWCzw7GdcPqQ1WlE5FLtXAAYZq/OkBir04i4DBXH5MrU9hvrPt7aHCIiIiLSdPzaQsdB5n6WplaKuIwdH5uPiVqlUqQxVByTy1dVWdeHovsEa7OIiIiISNPS1EoR13LmpLnKLECCimMijaHimFy+g19CeQH4t4MOA6xOIyIiIiJNqbYp/77V5h9FRcS57VoERhVEJEG7blanEXEpKo7J5audUhk/XiuJiYiIiLibqH7mH0HLC+HwV1anEZHvskOrVIpcLhXH5PLtWWY+apVKEREREfdjt0O3Mea+plaKOLeyQti30tzXlEqRRlNxTC7PiSw4vhvsnnU3TSIiIiLiXtR3TMQ17F4KVRUQ1h3Ce1qdRsTlqDgml6d21FjnYeAbbG0WEREREWketX8EzfkGivOtzSIiDdvxkfmoUWMil0XFMbk8tf3GtEqliIiIiPtqEw6Rvc39rJXWZhGRC6sogb1p5n6iimMil0PFMWm8skLYv87cV3FMRERExL1paqWIc9uzHM6WQtsudcVsEWkUFcek8fathOpKaBenJYJFRERE3F1tcSwrDaqrrc0iIufLrFmlMuFasNmszSLiolQck8bbvdR81KgxEREREfcXMxi8A+HMCchJtzqNiJyrsqzu97PE66zNIuLCVByTxqmuPqc4lmptFhERERFpfh5e0HWkuV/b10hEnMO+lVBRDEEdILq/1WlEXJaKY9I4RzfDmePgEwSdhlqdRkRERERagvqOiTinHR+bjwlTwK5f70Uul356pHFqV6mMG2v+FVFERERE3F/cWPPx8EYoPWVtFhExna2AXYvM/QStUilyJVQck8apLY6p35iIiIhI6xHSCcJ6gFEN+1ZbnUZEAPavgbICCAiHTt+zOo2IS1NxTC5dwRHIzQBsEDfO6jQiIiIi0pI0tVLEueyoXaXyGrB7WJtFxMWpOCaXbk9NI/6YwRDQztosIiIiItKyaqdW7k0Dw7A2i0hrV10FOxea+5pSKXLFVByTS6dVKkVERNzevHnz6NKlC76+vgwZMoSNGzde0vvefvttbDYbU6dOrfe8YRjMnj2bqKgo/Pz8SElJYc+ePc2QXJpd5+Hg6QdFRyE/0+o0Iq3bgS/MhdL82kKXEVanEXF5Ko7Jpak4A/tWmfvxKo6JiIi4o3feeYeZM2fy2GOPsXnzZvr06UNqair5+fkXfd/+/ft58MEHueqqq8577ZlnnuEvf/kLr732Ghs2bCAgIIDU1FTKysqa6zSkuXj51v0SrqmVItaqXaWyx2QtlCbSBFQck0uz/3M4WwZBHSGil9VpREREpBk8//zz3HXXXdxxxx0kJiby2muv4e/vzz//+c8G31NVVcVtt93GE088QdeuXeu9ZhgGc+fO5Xe/+x3XXXcdvXv35s033+To0aN89NFHzXw20izUd0zEetXVkPmpuZ+oKZUiTUHFMbk0jlUqU8FmszaLiIiINLmKigo2bdpESkqK4zm73U5KSgrr169v8H1PPvkk4eHh/PSnPz3vtezsbHJzc+t9ZnBwMEOGDLnoZ5aXl1NYWFhvEydRWxw7uB7Ki63NItJaHf4KinPBJwi6jrI6jYhbUHFMvpthnNNvbIK1WURERKRZHD9+nKqqKiIiIuo9HxERQW5u7gXfs3btWv7xj3/wt7/97YKv176vMZ8JMGfOHIKDgx1bTExMY05FmlO7bhDSGaoqYP9aq9OItE6ZNatUdp8Anj7WZhFxEyqOyXfL2w6FR8wGrLHn9xIRERGR1qeoqIgf/ehH/O1vfyMsLKxJP3vWrFkUFBQ4tkOHDjXp58sVsNnqVq3MSrM2i0hrZBiwo6Y4pimVIk3G0+oA4gJqp1R2HQVefpZGERERkeYRFhaGh4cHeXl59Z7Py8sjMjLyvOOzsrLYv38/U6ZMcTxXXV0NgKenJ7t27XK8Ly8vj6ioqHqf2bdv3waz+Pj44OOj0RBOKy4Fvv6n+o6JWOHoFig4CF7+0G2s1WlE3IZGjsl3c0yp1CqVIiIi7srb25sBAwaQllY3Gqi6upq0tDSGDh163vE9e/YkIyOD9PR0x3bttdcyevRo0tPTiYmJITY2lsjIyHqfWVhYyIYNGy74meIiYq8Guyec3AcnsqxOI9K61E6pjB8H3v7WZhFxIxo5JhdXctxs+AgqjomIiLi5mTNnMm3aNAYOHMjgwYOZO3cuJSUl3HHHHQD8+Mc/pkOHDsyZMwdfX1+SkpLqvT8kJASg3vMzZszg6aefJj4+ntjYWB599FGio6OZOnVqS52WNDWfQOg01FzNPGuF2YdMRJpfvSmV11mbRcTNqDgmF7dnOWBAZG8IirY6jYiIiDSjm2++mWPHjjF79mxyc3Pp27cvS5YscTTUP3jwIHZ74yYePPTQQ5SUlPCzn/2M06dPM2LECJYsWYKvr29znIK0lLixZnFs72cw+C6r04i0Dvk74GQWePhA/Hir04i4FZthGIbVIZpCYWEhwcHBFBQUEBQUZHUc9/HuNNjxEVz9EIz5rdVpREREmpTuH1yDrpMTys2A10aYfY8e3q8V80Rawso5sPqP0GMS3Ppfq9OIOL3G3D+o55g07GyFOVQezGWCRUREREQAIpKgTQRUnoGD661OI9I61PYbS9AqlSJNTcUxadjB9VBeCAHtIbqf1WlERERExFnYbOaqlaBVK0VawvE95rRKuyf00MAFkaam4pg0rHaVyvhUaGR/ERERERFxc3Fjzce9aRc/TkSu3I6PzcfYkeDX1tosIm5IFQ9p2O4l5qNWqRQRERGRb+s6Gmx2czRLwRGr04i4t0ytUinSnFQckws7vtdcCcXuBd1GW51GRERERJyNfyh0GGDuZ2n0mEizObUfcr4xi9E9J1udRsQtqTgmF1Y7aqzLCPAJtDaLiIiIiDgn9R0TaX6Zn5qPnYdDQJi1WUTclIpjcmGOKZVq9igiIiIiDagtjmWtgqqzlkYRcVs7NKVSpLmpOCbnKz1dtyR39/GWRhERERERJxbdz2wOXl4AR762Oo2I+yk4Aoc3mvs9r7E2i4gbU3FMzpe1AqrPQlh3CO1qdRoRERERcVZ2D+g2xtzX1EqRprdzgfkYMwSCoqzNIuLGVByT8+1eaj5qlUoRERER+S7dxpqPe9WUX6TJaUqlSItQcUzqq66CPcvMffUbExEREZHvEldTHDu6BUqOW5tFxJ0UH4ODX5j7CVOszSLi5lQck/qObILSk+AbbA7dFRERERG5mMBIiEgGDMhaaXUaEfexcwEY1WZvv5BOVqcRcWsqjkl9tatUxqWAh5e1WURERETENdSOHlPfMZGms+Nj8zHhWmtziLQCKo5JfY5+Y5pSKSIiIiKXKC7FfMxKg+pqa7OIuIMzJ2H/5+a++o2JNDsVx6TO6UOQtw1s9robHBERERGR7xIzBLzbQMkxyN1qdRoR17drMVSfhYgkaNfN6jQibk/FMamzp2bUWMwQ8A+1NouIiIiIuA5Pb4gdae5raqXIlcusWaVSUypFWoSKY1LHMaUy1docIiIiIuJ6HH3H0qzNIeLqygoha4W5n6jimEhLUHFMTBUlsG+1ua9+YyIiIiLSWLXFsUMboKzA2iwirmzPMqiqgHbx0L6n1WlEWgUVx8SUvQaqys0lgvUfYBERERFprLZdzF/mjaq6P7qKSOPt+Mh8TLwWbDZLo4i0FiqOiWn3EvOx+wT9B1hERERELk/tok7qOyZyeSpKYE/Nz4/6jYm0GBXHBAxD/cZERERE5Mo5imNp5j2miDTO3s/gbCmEdIaoPlanEWk1VBwTc7ntohzwCoDOI6xOIyIiIiKuqstw8PSFwsNwbJfVaURcz46aVSo1pVKkRak4JnWjxrqNBi9fa7OIiIiIiOvy8oPOw819Ta0UaZyz5XW/myVcZ20WkVZGxTE5p9+YplSKiIiIyBVS3zGRy5O1EiqKIDAaOgywOo1Iq6LiWGtXnA9HNpn78eOtzSIiIiIirq+2OHbgC6g4Y20WEVey42PzMWEK2PWrukhL0k9ca7dnmfkY3Q8CI63NIiIiIiKuLywegmOgqhwOrLM6jYhrqKqEXYvM/UStUinS0lQca+1qp1TGa0qliIiIiDQBmw3ixpr7mlopcmmy10DZaQhoD52GWp1GpNVRcaw1O1tuzmsH9RsTERERkaajvmMijZNZs0plz2vA7mFtFpFWqNHFsTVr1jBlyhSio6Ox2Wx89NFH3/meVatW0b9/f3x8fIiLi2P+/PkNHvvHP/4Rm83GjBkzGhtNGuvAOqgohjYRENXX6jQiIiIi4i5irwa7J5zYCyezrU4j4tyqq2DnQnNfUypFLNHo4lhJSQl9+vRh3rx5l3R8dnY2kydPZvTo0aSnpzNjxgzuvPNOli5det6xX331FX/961/p3bt3Y2PJ5dhd028sfrwaPoqIiIhI0/ENhpgh5n5WmrVZRJzdwfVQcgx8Q6DLVVanEWmVPBv7hokTJzJx4sRLPv61114jNjaW5557DoCEhATWrl3LCy+8QGpq3VS+4uJibrvtNv72t7/x9NNPNzaWNJZhwO7F5n73CdZmERERERH3EzfWnKmwNw0G3Wl1GhHnVbtKZc/J4OFlbRaRVqrZhwutX7+elJSUes+lpqayfv36es9Nnz6dyZMnn3dsQ8rLyyksLKy3SSMc3wOn9oOHN3QdZXUaEREREXE3tX3H9q2GsxXWZhFxVtXVkPmpuZ94nbVZRFqxZi+O5ebmEhERUe+5iIgICgsLKS0tBeDtt99m8+bNzJkz55I/d86cOQQHBzu2mJiYJs3t9mpXqexyFfi0sTaLiIiIiLifiGQICIfKEjj0pdVpRJzTka+hKAd8gjRoQcRCljeaOnToEA888AD//ve/8fX1veT3zZo1i4KCAsd26NChZkzphnbX9HzTlEoRERERaQ52uzm1ErRqpUhDaqdUdk8FTx9rs4i0Ys1eHIuMjCQvL6/ec3l5eQQFBeHn58emTZvIz8+nf//+eHp64unpyerVq/nLX/6Cp6cnVVVVF/xcHx8fgoKC6m1yiUpPmU0fAbqPtzaLiIiIiLiv2qmVe9WUX+Q8hgGZn5j7CVqlUsRKjW7I31hDhw5l0aJF9Z5bvnw5Q4cOBWDs2LFkZGTUe/2OO+6gZ8+ePPzww3h4eDR3xNZnbxoYVdA+Adp2sTqNiIiIiLirrqMBG+Rtg8IcCIqyOpGI88hJh9MHwcu/rpAsIpZodHGsuLiYvXv3Or7Ozs4mPT2d0NBQOnXqxKxZszhy5AhvvvkmAHfffTcvv/wyDz30ED/5yU9YsWIF7777LgsXLgQgMDCQpKSket8jICCAdu3anfe8NBHHlMrUix8nIiIiInIlAtpBh/5wZBNkpUG/H1qdSMR57KgZNRaXAt7+1mYRaeUaPa3y66+/pl+/fvTr1w+AmTNn0q9fP2bPng1ATk4OBw8edBwfGxvLwoULWb58OX369OG5557j73//O6mpKsxYouos7F1u7qvfmIiIiIg0N8fUSvUdE3E4d0qlVqkUsVyjR46NGjUKwzAafH3+/PkXfM+WLVsu+XusWrWqsbHkUh3+yuw55tcWOg6yOo2IiIiIuLu4FFj9J8haCdVVYFfbFBHyM+HEXvDw0YweESdg+WqV0sJ2LzEf48aBR7O3nBMRERGR1i66P/iGQNlpOLLZ6jQizqF21Fi3MeATaG0WEVFxrNVRvzERERERaUkentB1lLmvqZUiptp+Y4lapVLEGag41pqc2g/HMsHmAXFjrU4jIiIiIq2F+o6J1Dm+F/K3g90Teky0Oo2IoOJY67J7mfnYaajZc0xEREREpCXU/mH2yCY4c9LaLCJWy/zYfIy9Wr+XiTgJFcdak9p+Y5pSKSIiIiItKSgawnsBBmStsDqNiLV2aJVKEWej4lhrUV4M+z8397tPsDaLiIiIiLQ+taPH9qZZm0PESqcOQE462OzQ8xqr04hIDRXHWot9q6CqAtp2gbB4q9OIiIiISGtzbt+x6mprs4hYJfNT87HzcAgIszaLiDioONZaOKZUTgCbzdosIiIiItL6dPoeeAVAST7kbbM6jYg1MmumVCZolUoRZ6LiWGtQXQ17lpv76jcmIiIiIlbw9DEbkINWrZTWqfAoHNpg7idoSqWIM1FxrDXI/QaKc8G7jTl8V0RERETECuo7Jq1Z5gLzMWaIuUiFiDgNFcdag91Lzcduo82/2ImIiIiIWKG279ihL6Gs0NosIi1NUypFnJaKY63Buf3GRERERESsEhoLod2g+ixkr7E6jUjLKTkOB9aZ+wlTrM0iIudRcczdFeXC0S3mfvx4a7OIiIiIiJy7aqVIa7FzARjVENUX2na2Oo2IfIuKY+5uzzLzscMAaBNubRYREREREUdxLA0Mw9osIi1lR82UykRNqRRxRiqOubvafmOaUikiIiIizqDLcPDwgYKDcGKv1WlEml/pKchebe4nXGdtFhG5IBXH3FllGWStNPe7p1qbRUREREQEwDsAOg8z9zW1UlqDXYvNPnvhvSAszuo0InIBKo65swNrobIEAqMgsrfVaURERERETHFjzUcVx6Q10JRKEaen4pg7c0ypTAWbzdosIiIiIiK1avuO7V8LlaXWZhFpTuVFkLXC3E9QcUzEWak45q4MA3YvMffVb0xEREREnEn7nhDUAc6WwYF1VqcRaT67l0JVObSLg/AEq9OISANUHHNXx3bC6YPg6QuxI61OIyIiIiJSx2Y7Z2plmrVZRJrTjo/Nx4RrNZtHxImpOOauakeNxV4N3v7WZhERERER+bbaqZXqOybuquJM3b/f6jcm4tRUHHNX5/YbExERERFxNrEjweYBx3fDqQNWpxFpens/g8ozENIJovpanUZELkLFMXd05iQc2mDux6s4JiIiIiJOyC8EYgab+1maWiluKLNmlUpNqRRxeiqOuaO9n4FRDRFJEBJjdRoRERERkQtT3zFxV2fL62bzJF5nbRYR+U4qjrmj2n5j8eOtzSEiIiIicjG1fcf2rYazFdZmEWlK+1ZBeSEERkGHgVanEZHvoOKYu6k6W9f0sfsEa7OIiIiIiFxMZB/wD4OKIji80eo0Ik3HsUrlFLDr124RZ6efUndzaAOUFYBfKHTUXyhERERExInZ7edMrdSqleImqiph50JzX1MqRVyCimPu5twplXYPa7OIiIiIiHyX2qmVKo6Ju9j/OZSdhoD20Gmo1WlE5BKoOOZuaps+dtcqlSIiIiLiArqNAWyQmwFFeVanEblyO2pWqew5WQMWRFyEimPu5OQ+OL4L7J41NxkiIiIiIk4uIAyi+5r7WSssjSJyxaqrYOcCcz/hWmuziMglU3HMnexeZj52Ggp+IZZGERERERG5ZJpaKe7i4JdQcgx8QyD2aqvTiMglUnHMndT2G9MqlSIiIiLiSrrVNOXPWmGOvBFxVbWrVPaYBB5e1mYRkUum4pi7KC+C/WvNfRXHRERERMSVdBwEPsFQehKOpludRuTyVFdD5qfmvlapFHEpKo65i6yVUF0Jod0gLM7qNCIiIiIil87DE7qONPc1tVJc1ZFNUHQUvAOh22ir04hII6g45i4cq1Rq1JiIiIiIuCD1HRNXl1kzpbJ7Knj6WJtFRBpFxTF3UF0Ne2qLY6nWZhERERERuRxxNX3HjnwNZ05am0WksQwDdnxi7idqlUoRV6PimDs4usVcEcUnyFypUkRERETE1QR3hPYJYFTDvlVWpxFpnNytcPoAePrVjYIUEZfhaXUAaQK1q1R2GwOe3tZmERERERG5XHFj4Vgm7E2DpO9f0UdVnK1mXdZxPtuRhwF0CPEjKtiXqGA/okN8iQz2xcfTo2lyi9SuUhmfAt4B1mYRkUZTccwd1BbH1G9MRERERFxZXAqsf9nsO2YYYLM16u2VVdV8kXWChVuPsnR7HgWllRc9PqyND9EhvkQH+xFV8xgdUrffPtAHD3vjMkgrVG9K5VRLo4jI5VFxzNUVHjWH8GKD+HFWpxERERERuXydhoKXPxTnQt52iEz6zrecrapm/b4TLNyaw5LtuZw+U1cQax/ow8SkSEL8vck5XcrRglJyTpdxtKCUsspqjheXc7y4nK2HCy742Z52GxFBvmYBLcTPMeqs9jE62I8Qfy9sjSziiZs5thNO7AEPb4gfb3UaEbkMKo65utpVKjsOgoAwa7OIiIiIiFwJL1/ocpW52NTezxosjp2tqubLfSdZmHGUJdtyOXVOQSysjTcTk6KY3DuKQV1CLzjyyzAMTp2p5OjpUnIKyjh6buGs5rncwjLOVhscOV3KkdOlwKkLZvHz8nCMNIsKNotodQU0c9/fW792ubXaUWPdxoBvkLVZROSy6L/Srm63VqkUERERETcSl1JXHBsxw/F0VbXBhn0nWJCRw5JtuZwsqXC81i7AmwlJkUzuHcWQ2HbfORXSZrMRGuBNaIA3SR2CL3hMVbVBflEZR0+XkVNTODtyutTcrymoHS+uoLSyin3HSth3rKTB7xfs50VUsK/Z96ymcFbbAy06xI+IIF+8PbVWmsvKrCmOJWiVShFXpeKYK6ssrVvJR8UxERERaQLz5s3jz3/+M7m5ufTp04eXXnqJwYMHX/DYDz74gD/84Q/s3buXyspK4uPj+dWvfsWPfvQjxzG33347b7zxRr33paamsmTJkmY9D3FhcWPNx4NfUlVayMajlY4RYseL6wpioQHepPaK5JreUQyJDcXTo2mLSx52G1HB5lRKaHvBY8oqq8grrCma1RTRjpxTTDt6upSi8rMUlFZSUFrJztyiC36OzQbt2/gQFeJHh5riWV0xzY/oYF/C2vhgV/8z53MiC/K2gd0Teky0Oo2IXCYVx1zZ/rVwthSCOkDEd/djEBEREbmYd955h5kzZ/Laa68xZMgQ5s6dS2pqKrt27SI8PPy840NDQ/ntb39Lz5498fb2ZsGCBdxxxx2Eh4eTmlr3h7sJEybw+uuvO7728fFpkfMR11TVtiuVgZ3xLTrAr/88jw/O9Ha8FuLvxcSkSCYnR/O9rk1fEGssXy8POrcLoHO7hlcnLCqrrJu6WVM4O+qYvlnK0YIyKs5Wk19UTn5ROd8cuvDneHnYiKxdbbNmxFlt4Sw6xI/oYD+C/DzV/6yl1a5S2eUq8A+1NouIXDYVx1yZY5XK1Eav5CMiIiLybc8//zx33XUXd9xxBwCvvfYaCxcu5J///CePPPLIecePGjWq3tcPPPAAb7zxBmvXrq1XHPPx8SEyMrJZs4trq6422HTwFAu35rAoI4fppd2Z5nmAvhWbSPMbwIRe5pTJod3a4WVxQayxAn29CPT1ontE4AVfNwyDEyUVjoUCzu2DVvuYV1hGZZXBoZOlHDpZ2uD38vf2qFk44PyVN2sf/bw9mutUW6faKZWJ11mbQ0SuiIpjrsowzuk3NsHaLCIiIuLyKioq2LRpE7NmzXI8Z7fbSUlJYf369d/5fsMwWLFiBbt27eJPf/pTvddWrVpFeHg4bdu2ZcyYMTz99NO0a9euwc8qLy+nvLzc8XVhYeFlnJE4u+pqgy2HTrFgaw6LM3LJLSxzvPaVb3+msZwfBGdyy4yxeHu5b0HHZrMR1saHsDY+JHe8cP+zs1XmyDJz4YAyc+XN2v2akWgnSyo4U1HF3vxi9uYXN/j92vp71VsswLHyZk1RLSLI1+UKkJY5fRCObgGbHXpeY3UaEbkCKo65qvwdUHAIPP0g9mqr04iIiIiLO378OFVVVURERNR7PiIigp07dzb4voKCAjp06EB5eTkeHh688sorjBs3zvH6hAkT+P73v09sbCxZWVn85je/YeLEiaxfvx4PjwsXPObMmcMTTzzRNCcmTsUwDLYcOu0YIZZTUFcQC/T1ZHyi2UNseKer4LkX8Cs5DIX7oV0360I7AU8Pe00xy6/BY8oqq86ZvnnuKpx1xbSSiipOnank1JlKduRcuOhst0F4oK850qxm2mb9Apof7QK81f8MIPNT87HTMGjT3tosInJFVBxzVbVTKruOBK+G/ycpIiIi0pwCAwNJT0+nuLiYtLQ0Zs6cSdeuXR1TLm+55RbHscnJyfTu3Ztu3bqxatUqxo4de8HPnDVrFjNnznR8XVhYSExMTLOehzQfwzD45nABC7ceZVFGLkdO100LbOPjyfjECCb3jmJEfBg+nucUTDt9D7LXmKtWtvLi2KXw9fIgNiyA2LAL9z8zDIPCsrPnr7zp2C8jt6CMiqpqcgvLyC0sY8vB0xf8LG8Pe82qm+dP36zdD/L1asazdRI7aqdUapVKEVen4pirckyp1CqVIiIicuXCwsLw8PAgLy+v3vN5eXkX7Rdmt9uJi4sDoG/fvmRmZjJnzpzz+pHV6tq1K2FhYezdu7fB4piPj4+a9rs4wzDIOFLAwq05LNiaU68gFuDtwbjECCb3juaq+DB8G5oyGZdSVxwb8vMWSu6+bDYbwX5eBPt50TMy6ILHVFcbHC8pd6y8WbdwQF0/tPyiciqqqjlw4gwHTpxp8Pu18fE0i2f1pm/WjESrmcLZ4LV3BYU5cOhLcz9hirVZROSKqTjmikpOwKGN5n68imMiIiJy5by9vRkwYABpaWlMnToVgOrqatLS0rjvvvsu+XOqq6vr9Qv7tsOHD3PixAmioqKuNLI4GcMw2H60kAVbc1iYcbRe43h/bw9SEswRYiO7t7+0okhcCiyfDdmfQ2UZePk2Y3oBsNtthAf6Eh7oS5+YkAseU1lVTV5h2YVX3qx57tSZSorLz7Inv5g9F+l/1i7Au/6Is5rCWYeaYlp4oI/lK5I2aOcC87HjYAiKtjaLiFwxFcdc0d7lgAGRyRDcweo0IiIi4iZmzpzJtGnTGDhwIIMHD2bu3LmUlJQ4Vq/88Y9/TIcOHZgzZw5g9gYbOHAg3bp1o7y8nEWLFvGvf/2LV199FYDi4mKeeOIJbrjhBiIjI8nKyuKhhx4iLi6u3mqW4roMw2BHTiELt+awMCOn3kgiPy8PxiaEc03vKEZ2D2/8KonhiRAYBUU5cPAL6DamidPL5fDysNOxrT8d2/o3eMyZirPkFJSZK3CeLuVozfTNc1fjPFNRxYmSCk6UVLDtyIX7n3nYbUQE+jhGmnWoV0Az90MDvLHZLOh/tuNj81FTKkXcgopjrqi235hWqRQREZEmdPPNN3Ps2DFmz55Nbm4uffv2ZcmSJY4m/QcPHsRurxvFUVJSwr333svhw4fx8/OjZ8+evPXWW9x8880AeHh4sHXrVt544w1Onz5NdHQ048eP56mnntK0SRdmGAY7c4scBbHs4yWO13y97IztaY4QG9WjPf7eV/Drhs0GcWNhy1uwN03FMRfi7+1Jt/Zt6Na+zQVfNwyDgtLKc0afnbNwQM1CArkFZZytNsyvz1m44dt8PO2M7N6eJ67rRVRwC/ViLjkOB9aZ+5pSKeIWbIZhGFaHaAqFhYUEBwdTUFBAUNCF59C7hapKeKYrlBfCnWnQcaDViURERFxWq7l/cHG6TtYzDIPdecUs3HqUBRk57DtWVxDz8bQzpmc4k3tHMaZn+JUVxL5t+4fw3u3QvidM39B0nytOr7ra4HhxuWOxAHMVzppiWs3Xx4rqpnAH+nry2JRe3NC/Q/OPJNv0Bnz6C4jqAz9f07zfS0QuW2PuHzRyzNUcXG8WxvzDILq/1WlERERExI3tySuq6SGWw95zekd5e9oZ3aM9k3tHM7ZnOAE+zfRrRddRYLPDsZ1w+hCEaNXS1sJutxEe5Et4kC/9Gjim4mw1u/OK+N1H20g/dJoH3/uGRRk5zPl+MhFBzdijLrNmlcoETakUcRcqjrmac1eptDtpc0oRERERcVl784trpkweZXfeOQUxDzsje7TnmpoRYoG+Xs0fxq8tdBwEhzZAVhoMuL35v6e4DG9PO0kdgnn/nmH87fN9PL9sNyt25jPu+dU8cV0vpvZthlFkpadg3ypzP/G6pv1sEbGMimOuxtFvTE1sRURERKRpZB0rZlHNCLGduUWO5708bIzs3p7JvaMYmxBBUEsUxL4tLsUsju39TMUxuSAPu427R3ZjbM9wfvXeN2w9XMAv3/mGRRm5/P76JMIDm3AU2a4lUH3WXDAiLL7pPldELKXimCs5vhdO7AW7F3QdbXUaEREREXFh2cdLWJSRw4KtOWTm1K0W6OVh46r49kxOjiIlMYJgPwsKYueKGwsrfw/7Vpv9dz0sziNOKz4ikA/uGcZf1+xj7me7Wb4jj6/2n+TJ65KY0juqaUaRaUqliFtSccyV7KmZUtllOPiqGa2IiIiINM6BEyUszMhh4dYcth+tK4h52m2MiA9jcnIU4xMjCfZ3ogJUVD/wbwdnTsDhr6DzMKsTiRPz9LAzfXQcYxPC+dW737D9aCG/+O8WFmfk8NTUJMLaXMFKueVF5sqpAIkqjom4ExXHXIljSuUEa3OIiIiIiMs4dPKMoyCWcaTA8byH3cbwuDCuSY5ifK8IQvy9LUx5EXY7dBsDGe+ZUytVHJNL0DMyiI+mD+eVlVm8tGIPi7flsiH7JE9PTWJSctTlfeieZVBVDqHdzGmVIuI2VBxzFWUFcOALc1/9xkRERETkIg6fOsOimoLYN4frF8SGdWtnjhDrFUlogJMWxL4tLqWmOJYGY2dbnUZchJeHnQdS4klJNEeR7cwt4t5/b+aa3lE8eV1S4//931EzpTLxWmjqRv8iYikVx1xF1gqz8WO7eAjtanUaEREREXEyR06Xsrimh1j6odOO5+02GNqtHZOTo0ntFUG7K5lWZpVuY8zHnHQoPgZt2lsaR1xLr+hgPrlvBC+v2MO8VVks2JrDl/tO8Pvrk0ntFXlpH1Jxxhw5Buo3JuKGVBxzFbtr/kOsUWMiIiIiUuPo6VJzhFhGDlsOnnY8b7fBkNh2TO4dxYSkyCvrs+QM2oRDVB/I+cb8o3Gfm61OJC7G29POzPE9GJcYya/eS2d3XjE//9cmpvaN5vFre333tOKsNKg8A8GdILpfy4QWkRaj4pgrqK6q+yuF+o2JiIiItGq5BWWOgtimA6ccz9tsMLhLKNf0jiI1KZLwQF8LUzaDbmPN4tjez1Qck8uW3DGYT+8fwV/S9vDqqiw+Sj/KuqwTzLk+mZTEiIbfqCmVIm5NxTFXcGQznDkOPsHQ6XtWpxERERGRFpZXWMbimoLYV/vrF8QGdQ5lcu8oJiZFEh7kZgWxc8WlwNrnzRE81dVmo36Ry+Dj6cGvU3syLjGSB9/7hr35xdz55tfc0L8js6ckEuz3rdVaz5bXLY6mKZUibknFMVdQ+x/iuLHg4UTLaouIiIhIs8kvKmPJtlwWbM3hq/0nMYy61wZ2bltTEIsiMtiNC2LnihkM3oFw5oTZe6xDf6sTiYvrGxPCgvtH8MJnu/nbmn28v/kwa/ce44839GZ0j/C6A/ethvJCCIyCjoOsCywizUbFMVewe6n5qCmVIiIiIm7tWFE5S7bnsnDrUTZk1y+I9e8UwuTe0UxKjiQq2M+6kFbx8IKuI2HnAnPVShXHpAn4enkwa2IC4xMj+fV737DveAl3vP4VNw+M4bfXJBDk6wU7PjYP7nmNRiyKuCkVx5xdwWHIywCb3RxKLiIiIiJu5URxbUHMXEGv+pyCWN+YEK7pHcXE5Cg6hLTCgti3xaXUFMc+g5G/tjqNuJEBnduy6IGreHbpLv6xLpt3vj7E53uO8cz3Exixa6F5UOJ11oYUkWaj4pizqx011nEwBLSzNouIiIiINImTJRUsrSmIrd93gqpzKmJ9OgY7pkzGhPpbmNIJxY01Hw9vhNJT4NfW2jziVny9PPjdNYmkJpm9yA6cOMOrb7zJCO9TVPuHYe88zOqIItJMVBxzdo4planW5hARERGRK3KqpIJlO8weYl9k1S+IJXcwC2KTk1UQu6iQThDWA47vMvtA9ZpqdSJxQ4O6hLL4gat4Zsku4jb+A4BPy/vRft8phsWFWZxORJqDimPOrOIMZK8299VvTERERMTlFJypZOkOc4TYur3HOXtOQaxXdJCjINa5XYCFKV1MXIpZHNv7mYpj0mz8vT15/JqeVOz4Bsrg/dL+rPn7Bn48tDMPT+hJgI9+lRZxJ/qJdmbZa+BsGQR3gvAEq9OIiIiIyCUoKK1k+Y48Fm49ytq9x6msqiuIJUQFcU3vKCYlRxEbpoLYZYkbC1/OM5vyGwbYbFYnEnd1aAPeZccxfIPp2nsiazYe5c31B1i16xh/vrE3Q7qq7Y2Iu1BxzJntXmI+dk/V//RFREREnFhhWSWf7chj4dYc1uw5Vq8g1jMykMnJUUzqHUW39m0sTOkmOg8HTz8oOgr5mRCRaHUicVc1q1Taekzi8ev7Ma53DA/9bysHT57h5v/7kjuGd+Gh1J74eXtYHFRErpSKY87KMM7pN6YplSIiIiLOpqiskrTMfBZszWHN7mNUVFU7Xuse0YbJydFM7h1JXHighSndkJcvdBkBe5ebUytVHJPmUF0NmZ+a+zWrVA6PC2PJjKv4w6Kd/HfjQV5ft5+VO/P58w/6MKhLqIVhReRKqTjmrHIzzL+Gefmb//MXEREREcsVl58lLdMcIbZq9zEqztYVxLq1D+Ca3tFM7h1F9wgVxJpVXEpdcWz4L6xOI+7o6GYoPALebaDraMfTgb5ezPl+MhOSInnk/a3sP3GGm/66np8Oj+XB1B74emkUmYgrUnHMWdWOGus62vzrmIiIiIhYoqT8LCt25rNwaw4rd+VTfk5BrGtYANf0jmJy72i6R7TBplYYLSMuxXw8uB4qSsBb/dukidVMqaR76gV/HxvZvT1Lf3k1Ty/YwbtfH+bva7NZsTOfZ2/qQ/9ObVs4rIhcKRXHnNW5/cZEREREpEWdqTjLyp3HWJhxlBU78ymrrCuIdWnn7xgh1jMyUAUxK7TrBiGd4fQB2L9W98zStAwDMj8x9xOubfCwIF8vnrmxDxOTonjkg63sO17Cja9+wV1Xd+WXKd01ikzEhag45oyK8+HIJnM/fry1WURERERaidKKKlbtymdBRg4rMvMpraxyvNa5nT+Tk6OY3DuKxKggFcSsZrOZo8e+/oc5tVLFMWlKuRlwar+58EP8uO88fHTPcJbNGMkTC7bzweYj/HX1PtIy83nuB33oExPS7HFF5MqpOOaM9iwHDIjqC0FRVqcRERERcVtllVWs2nWMhRk5pGXmcaairiAWE+rH5ORorukdRa9oFcScTtzYuuKYSFOqnVIZN/aSp+wG+3vx/E19mZgUxW8+zGBvfjHff/UL7h7ZlV+MjcfHU6PIRJyZvbFvWLNmDVOmTCE6OhqbzcZHH330ne9ZtWoV/fv3x8fHh7i4OObPn1/v9Tlz5jBo0CACAwMJDw9n6tSp7Nq1q7HR3IdjSqVWqRQRERFpTq+v28/db23i02+Ocqaiig4hfvz86q58ct9w1vx6NI9M7ElSh2AVxpxR7NVg94ST++BEltVpxJ3UTqlMnNrot45LjGDZjKu5rm80VdUG81Zmce1L69h2pKBpM4pIk2p0caykpIQ+ffowb968Szo+OzubyZMnM3r0aNLT05kxYwZ33nknS5cudRyzevVqpk+fzpdffsny5cuprKxk/PjxlJSUNDae6ztbAVkrzf3umlIpIiIi0pwmJUfSIcSPu66K5aPpw1n78GhmTUqgd8cQFcScnU8gdBpq7metsDaLuI/8nXB8N3h4X/Z03bYB3rx4Sz9e+2F/2gV4syuviOvmreP55bvrrXArIs6j0dMqJ06cyMSJEy/5+Ndee43Y2Fiee+45ABISEli7di0vvPACqanmf2yWLFlS7z3z588nPDycTZs2cfXVVzc2oms7+AVUFEFAOET1szqNiIiIiFvr3C6AtQ+PViHMVcWNhf2fm1MrB99ldRpxB7WjxrqOBt+gK/qoCUlRDOoSyuxPtrNwaw5/SdvD8h15PPeDPiRGX9lni0jTavTIscZav349KSkp9Z5LTU1l/fr1Db6noMAcchoaGtrgMeXl5RQWFtbb3MLumhF13ceDvdkvj4iIiEirp8KYC4ur+T0jew2cLbc2i7iHHbVTKhtepbIx2rXxYd7/68/L/68fbf29yMwp5NqX1/KXtD1UVmkUmYizaPbqS25uLhEREfWei4iIoLCwkNLS0vOOr66uZsaMGQwfPpykpKQGP3fOnDkEBwc7tpiYmCbP3uIMA3YtNvfVb0xERERE5OIikqBNBFSegYMN//Fd5JKcyIK8DLB5QI9JTfrR1/SOZtkvR5LaK4Kz1QbPL9/N9a+sY1duUZN+HxG5PE43NGn69Ols27aNt99++6LHzZo1i4KCAsd26NChFkrYjE7shVPZ5vz2rqOsTiMiIiIi4txstrrRY1q1Uq5U7ZTK2KvBv+FZTJerfaAPr/1wAC/e0pdgPy+2HSnkmpc+Z97KvZzVKDIRSzV7cSwyMpK8vLx6z+Xl5REUFISfn1+95++77z4WLFjAypUr6dix40U/18fHh6CgoHqby6tdpbLLCLPBqIiIiIiIXFzcWPNxb5q1OcT1NfGUygux2Wxc17cDy395NSkJ4VRWGfx56S5uePUL9uRpFJmIVZq9ODZ06FDS0ur/j2r58uUMHTrU8bVhGNx33318+OGHrFixgtjY2OaO5Zwc/cY0pVJERERE5JJ0HQ02O+TvgIIjVqcRV3X6EBzdDNig5zXN/u3Cg3z5248H8vxNfQjy9eSbwwVMfmktf12dRVW10ezfX0Tqa3RxrLi4mPT0dNLT0wHIzs4mPT2dgwcPAuZ0xx//+MeO4++++2727dvHQw89xM6dO3nllVd49913+eUvf+k4Zvr06bz11lv85z//ITAwkNzcXHJzcy/Yk8xtlZ6GA1+Y+/HjLY0iIiIiIuIy/EOhwwBzP0ujx+QyZX5qPnYeBm3CW+Rb2mw2vt+/I8t+OZLRPdpTcbaaOYt3cuNrX5B1rLhFMoiIqdHFsa+//pp+/frRr18/AGbOnEm/fv2YPXs2ADk5OY5CGUBsbCwLFy5k+fLl9OnTh+eee46///3vpKamOo559dVXKSgoYNSoUURFRTm2d95550rPz3VkpYFRBe17QmgrHTknIiIiInI51HdMrlRtv7GE5ptS2ZDIYF/+efsgnrmxN4E+nmw5eJpJL37O3z/fp1FkIi3Es7FvGDVqFIbR8A/o/PnzL/ieLVu2NPiei31eq+GYUpl68eNERERERKS+uBRYNQf2rYKqs+DR6F9zpDUryoWDX5r7CVMsiWCz2bhpYAwj4sJ45IMM1uw+xtMLM1m6PZc/39iHLmEBluQSaS2cbrXKVqm6CvYsM/fVb0xEREREpHGi+4FfWygrgCObrE4jribzU8CAjoMguIOlUaJD/HjjjkH88fvJtPHx5Kv9p5jw4hrmr8umWqPIRJqNimPO4PBXUHoKfEOg42Cr04iIiIiIuBa7B3QbY+5raqU0loVTKi/EZrNxy+BOLJlxFcPj2lFWWc3jn+7g1r99ycETZ6yOJ+KWVBxzBruXmI/x4zQEXERERETkcqjvmFyOkhOwf525n+gcxbFaHdv689ZPh/D01CT8vT3YkH2SCS+u4V9fHtAoMpEmpuKYM3D0G9OUShERERGRy1I7cuzoFig5bm0WcR27FpoLo0X2hrZdrE5zHpvNxg+/15mlM67me11DOVNRxaMfbeOH/9jAoZMaRSbSVFQcs9qpA5C/A2znDAUXEREREZHGCYyEiGTAgKyVVqcRV7GjZkqlk40a+7aYUH/+c+f3eOLaXvh5efBF1gkmzF3DfzYc1AJ3Ik1AxTGr1Tbi7/Q98A+1NouIiIiIiCuLG2s+amqlXIrS0+YKpwAJ11mZ5JLY7TamDevC4geuYlCXtpRUVPGbDzP48T83cvR0qdXxRFyaimNWq+031j3V2hwiIiIiIq6utu9YVhpUV1ubRZzf7iVQXQntE6B9d6vTXLIuYQG8/bOhPHpNIj6edj7fc5zUF9bw7leHNIpM5DKpOGalihLI/tzcV78xEREREZErEzMEvNtAyTHI3Wp1GnF2LjKl8kI87DZ+OiKWRQ9cRf9OIRSVn+Wh97dyx/yvyC0oszqeiMtRccxK+1ZDVTmEdIYw1/lLhYiIiIiIU/L0htiR5r6mVsrFlBebIwwBElyvOFarW/s2vHf3MH4zqSfennZW7TrGuBdW8/6mwxpFJtIIKo5ZyTGlcgLYbNZmERERERFxB46+Y2nW5hDntmcZnC2D0K4Q0cvqNFfEw27jZ1d3Y9EvRtCnYzBFZWf51XvfcNebX5NfqFFkIpdCxTGrGAbsXmruq9+YiIiIiEjTqC2OHdoAZQXWZhHnlVkzpTLhWrcZqBAXHsj79wzjoQk98Paw81lmPuNeWMPH6Uc0ikzkO6g4ZpWcb6A4F7wCoMsIq9OIiIiIiLiHtl2gXTwYVWYbE5FvqyyF3cvM/UTnX6WyMTw97Nw7Ko5P7x9BcodgCkoreeDtdO5+axPHisqtjifitFQcs0rtqLFuo8HTx9osIiIiIiLupHbVSvUdkwvZmwaVJRDcCaL7WZ2mWfSIDOSDe4fxq3Hd8fKwsXR7HuNfWM2CrUetjibilFQcs8q5/cZERERERKTpOIpjaWY7E5FzOaZUTnGbKZUX4uVh5/6x8Xw8fQSJUUGcOlPJff/ZwvR/b+ZEsUaRiZxLxTErFOXB0c3mfvx4a7OIiIiIiLibLsPB0xcKD8Px3VanEWdytgJ21QxUSHTdVSobIzE6iI+mD+eBsfF42m0szMhh/AtrWJyRY3U0Eaeh4pgV9tTMb4/uD4ER1mYREREREXE3Xn7Qebi5r6mVcq7s1VBeAG0ioeNgq9O0GG9PO78c152Ppg+nZ2QgJ0oquOffm/nFf7dwqqTC6ngillNxzAqaUikiIiIi0rzUd0wuZMdH5mPCNWBvfb8OJ3UI5uP7hnPf6Dg87DY++eYo415Yw7LtuVZHE7FU6/uvgdXOlkPWSnO/e6q1WURERERE3FVtcWz/Oqg4Y20WcQ5VZ2HnInPfzVapbAwfTw8eTO3BB/cMIz68DceLy/nZvzYx8510Cs5UWh1PxBIqjrW0/WvNlVECoyCqj9VpRERERETcU1g8BMdAVTkcWGd1GnEGB9ZC6UnwbwedhlmdxnJ9YkL49P4R3D2yG3YbfLDlCONeWM2KnXlWRxNpcSqOtbTdS83H+PFuvTKKiIiIiIilbDaIG2vub/sAKsuszSPW21GzSmXPyeDhaW0WJ+Hr5cEjE3vyv3uG0bV9APlF5fxk/tf8+r1vKCjVKDJpPVQca0mGoX5jIiIiIiItpXZq5Tf/gWdi4T+3wFf/gNMHrc0lLa+6GnYuMPcTWu+Uyob079SWRb+4iruuisVmg/c2HWbC3DWs3n3M6mgiLULFsZZ0bBecPgAePtB1pNVpRERERETcW/cJMORus6VJ5RnYvRgWzoS5yTBvCCz7HWSvgbNarc/tHdoAxXngEwyxV1udxin5ennw28mJvPfzoXRp509OQRnT/rmRWR9spahMo8jEvak41pJqR43FXg3eAdZmERERERFxdx5eMPFPMDMT7l4LYx8ze03ZPODYTvjiJXhjCjzTFd75IWx6AwqPWp1amsOOj83HHhPB09vaLE5uYJdQFj9wNXcM7wLAfzceYsLcz1m757i1wUSakSZat6TafmNapVJEREREpOXYbBCZbG5XzYTSU+YK8nuWw97lUHIMMj81N4CIZIgfZ/YJ7jhI/alcnWHUXdtWvEplY/h5e/DYlF6k9orkof9t5eDJM/zwHxu4bUgnZk1KoI2PfibEvdgMwzCsDtEUCgsLCQ4OpqCggKCgIKvjnO/MSfhzNzCqYUYGhHSyOpGIiEir5/T3DwLoOkkzq66GnPS6Qtnhr4FzfkXyDYZuY81CWVwKtGlvVVK5XIc3wd/HgHcb+HUWePlancillJSf5U9LdvLm+gMAdGzrxzM39mZYtzCLk4lcXGPuH1TubSlZK8zCWHgvFcZERERERJyF3Q4d+pvbqIeh5ARkpcGeZbD3M3OU2fYPzA0gup9ZKIsfb+7bPazNL98ts2ZKZfx4FcYuQ4CPJ09el8SEJHMU2eFTpfy/v21g2tDOPDyxJ/7eKiuI69O/xS3FsUrleGtziIiIiIhIwwLaQe+bzK26Co5sMgtle5abI8yObjG31X8C/3bmaLL48dBtDPiHWp1evs0wYMcn5n7itdZmcXHDuoWxZMbVzFmUyb83HOSN9QdYuesYz/6gD4Nj9e++uDZNq2wJVWfNKZVlp+EnS6HT96xOJCIiIjj5/YM46DqJ0yjKM0eT7Vlm9iwrL6h7zWaHDgNrRpWNg8je5qg0sVZuBrw2Ajx9zSmVPm2sTuQWPt9zjIf/t5WjBWXYbHDHsFh+ndoDP2+NpBTnoWmVzubwRrMw5tfWbOgpIiIiIiKuJzAC+t1mblWVcGhj3fTLvG3mff/hjbDyaWgTAXHjzEJZt9Fm7zJpebWrVMalqDDWhK6Kb8+SX17NHxZm8vZXh/jnumxW7srn2R/0ZkBnjSIT16PiWEuonVIZP149CURERERE3IGHF3QZbm7jnoCCI2ZD/z3LzVFlxXmQ/pa52TzM2SO1K2CGJ5oraErzc0yp1CqVTS3I14s/3tCb1KRIZr2fQfbxEm58bT13XdWVmeO64+ul333FdWhaZUuYNwSO7YQb/wlJN1idRkRERGo49f2DOOg6ics5Ww4H15uFsj3L4Pju+q8HdagrlMWO1Iim5nJsF8wbDHYveChLo/eaUUFpJU8t2MH/Nh0GoFv7AJ79QR/6dWprcTJpzTSt0pmczDYLYzYPcwloERERERFxb54+0HWUuaX+Hk7trymULYfsNVB4BDbNNze7F3QeVrcCZli8RpU1ldpRY5rW2uyC/bx49gd9mJgUySMfZJB1rIQbXv2Cn4/sxoyUeHw8NYpMnJuKY81tzzLzsfMw8AuxNIqIiIiIiFigbRcYfJe5VZbC/nU1K2AuNQtn2avNbdlvIaRzXaGsywjw9rc6vevKrOk3lqBVKlvK2IQIlv+yLU98uoMPtxzh1VVZpGXm8ewP+tC7Y4jV8UQapOJYc6vtN9Y91docIiIiIiJiPS8/iE8xN+NPcCKrplfZMti/Fk4fgK/+Zm6evmaBrHYFzNCuVqd3HSf3mStV2jyg52Sr07QqIf7evHBzXyYkRfLbDzPYnVfM9a98wT0ju3H/2DiNIhOnpOJYcyovMv8HB9B9grVZRERERETEudhsEBZnbt+7BypKzGmXe5aZUzALDpkrYe79DBYD7eLqCmWdh5vTN+XCaqdUxl4F/lo90QqpvSIZ1CWUxz7ZzqffHOXllXv5rGYUWVIHTXMV56LiWHPatwqqKsy/8LSLszqNiIiIiIg4M+8A6DHR3AzD7F1c29T/4Ho4sdfcvnwFvPzNnmZxKWbBLCTG6vTOJbOmOKYplZYKDfDmpVv7MTEpkt99tI2duUVMnbeO+8bEMX10HF4edqsjigAqjjUvx5TKCWqqKSIiIiIil85mg/AEcxv+CygrNP/4XjuqrDgXdi0yN4D2CXUrYHb6Hnh4WRrfUgWH4cgmwAY9r7E6jQCTkqMYHBvKox9tY/G2XOZ+todl2/N47qY+JERpFWKxnopjzaW6GnbXNONXvzEREREREbkSvkGQeK25GYbZT6u2UHZ4IxzLNLcv/gI+QeaosvhxEDcOgqKsTt+yMj81HzsNhcAIa7OIQ1gbH165rT+fbs1h9sfb2JFTyLUvr+WBsfHcPbIbnhpFJhZScay55GyBknzwDoROw6xOIyIiIiIi7sJmg6je5nb1g3DmJOxbWTMFczmcOW5OK6ydWhiZXLcCZoeB4OHmvwbW9htL1JRKZ2Oz2bi2TzTf6xrKbz/cxvIdeTy7bDdLa0aRdY8ItDqitFJu/l9FC+1eaj7GjQFPb2uziIiIiIiI+/IPhaQbzK262vxDfW2h7Mgmc5RZbgZ8/hz4hkDcWLNQ1m0stGlvdfqmVZRn9mcDSJhibRZpUHigL//3owF8nH6Uxz7ZTsaRAq75y1p+Oa47d10Vq1Fk0uJUHGsu5/YbExERERERaQl2O3QYYG6jHoGS47A3zZyCufczKDsN2943N2wQ3a9uVFl0P/P9rmznp4BhjpAL7mh1GrkIm83G1H4dGNqtHb/5IIO0nfn8aclOlm7P5dkf9CEuvI3VEaUVUXGsORQehZxvAJs5x19ERERERMQKAWHQ52ZzqzprjiTbs8zccrfC0c3mtvqP4B9Ws/rlOOg2xhyR5mo0pdLlRAT58vdpA3l/8xGe+HQ76YdOM+kvn/Pr8T34yYhYPOxa3E6an4pjzWFPTSP+jgPdb5iyiIiIiIi4Jg9P6DTE3MY+CkW55miyPcsga6XZq2zr2+Zms0PHQXUrYEb2NnudObMzJ2H/WnM/QcUxV2Kz2bhxQEeGx7XjkfczWL37GL9flMmS7bn88fvJxKsXmTQzFceag1apFBERERERZxcYCf1+aG5VlXBoQ90KmPk7zK8PbYAVT0ObSIhPMQtlXUeBb7DV6c+3cyEYVeYCBKGxVqeRyxAV7Mf8Owbx7teHeGpBJpsOnGLcC2tI6hDExKQoJiVHERsWYHVMcUM2wzAMq0M0hcLCQoKDgykoKCAoKMi6IJVl8EwsVJ6Bn39uriAjIiIiTslp7h/konSdRCxQcLiuqf++VVBZUvea3RNivlc3qiw8wTlGlf37B2Zxb/TvYOSvrU4jV+jI6VIe+3g7K3bmUX1O1aJnZCCTkqOYlBxJXLhGlEnDGnP/oOJYU9vzGfz7BgiMhpk7nON/EiIiInJBTnP/IBel6yRisbPlcOCLmmLZMjixp/7rQR3rCmWxV4OPBY3US0/Dn+OguhKmfwXtu7d8BmkWJ4rLWbYjj0UZOXyRdYKqcypl8eFtmJgcxeTkKLpHtMGm37/lHI25f3DxpUickGOVylQVxkRERMTlzJs3jy5duuDr68uQIUPYuHFjg8d+8MEHDBw4kJCQEAICAujbty//+te/6h1jGAazZ88mKioKPz8/UlJS2LNnTwOfKCJOydMHuo2GCX+A+7+GX6TDpGfNYpinLxQehk2vw9u3mrNo3rwO1s+D43ugpcZi7F5qFsba91RhzM20a+PDrYM78a+fDuHr36bwzI29Gd2jPV4eNvbkF/OXtD2kzl3D2OdX8+zSXWw/WoCbjAGSFqSRY03JMGBubyg4CLe+Az0mWJNDRERELolT3D84kXfeeYcf//jHvPbaawwZMoS5c+fy3nvvsWvXLsLDw887ftWqVZw6dYqePXvi7e3NggUL+NWvfsXChQtJTTV7r/7pT39izpw5vPHGG8TGxvLoo4+SkZHBjh078PX1vaRcuk4iTqyy1GyCX7sC5qn99V9v28UsosWPhy4jwMuveXK8fRvsXABXPwRjfts830OcSkFpJWmZeSzKyGXN7mNUVFU7Xuvczr+mR1kkyR2CNaKsldK0SqtumvJ2wKtDzb+ePJQN3v7W5BAREZFL4hT3D05kyJAhDBo0iJdffhmA6upqYmJiuP/++3nkkUcu6TP69+/P5MmTeeqppzAMg+joaH71q1/x4IMPAlBQUEBERATz58/nlltuueBnlJeXU15e7vi6sLCQmJgYXScRZ2cYcGJv3fTLA+ugqqLudU9f6HJVTbFsXNM1zS8vhj93g7NlcPdasyG/tCpFZZWs2JnP4oxcVu7Kp/xsXaGsQ4gfk5IjmZgcRd+OIdjtKpS1Fo25z9NqlU2pdkpl7EgVxkRERMSlVFRUsGnTJmbNmuV4zm63k5KSwvr167/z/YZhsGLFCnbt2sWf/vQnALKzs8nNzSUlJcVxXHBwMEOGDGH9+vUNFsfmzJnDE088cYVnJCItzmaDsHhzG3qvWbTKXlO3AmbhYdi73NwWA+3i6wplnYeZ0zcvx97lZmGsbSxEJDXpKYlrCPT14rq+HbiubwdKys+ycpdZKFuxM58jp0v52+fZ/O3zbKKCfZmQFMmk5CgGdGqrQpk4qDjWlHYvNR+7p1qbQ0RERKSRjh8/TlVVFREREfWej4iIYOfOnQ2+r6CggA4dOlBeXo6HhwevvPIK48aNAyA3N9fxGd/+zNrXLmTWrFnMnDnT8XXtyDERcTE+baDnJHMzDMjPNAtlez+Dg+vNxv4n9sCX88ArALqONAtlceMgpBE/8zs+MR8Tr1XfZyHAx5NrekdzTe9oSiuqWL07n0UZuaRl5pFTUMbr6/bz+rr9hAf6MCEpkolJUQyODcVDhbJWTcWxplJyAg7XNKxVcUxERERaicDAQNLT0ykuLiYtLY2ZM2fStWtXRo0addmf6ePjg4/PZY4gERHnZLNBRKK5jZgBZQWwb1XdqLLiPNi1yNwAwhPrVsCMGQIeXhf+3MrSukEKide1xJmIC/Hz9mBCUhQTkqIoq6zi8z3HWZyRw/LMPPKLynlz/QHeXH+AsDbejO8VyaSkKL7XNRRPD61d2NqoONZU9n4GRjVEJENwR6vTiIiIiDRKWFgYHh4e5OXl1Xs+Ly+PyMjIBt9nt9uJi4sDoG/fvmRmZjJnzhxGjRrleF9eXh5RUVH1PrNv375NfxIi4jp8g81iVuJ1UF0NeRl1hbLDX0H+DnNb9yL4BEHXUXVTMAPP+W9S1gqoLIHgGIjub9npiPPz9fJgXGIE4xIjKD9bxRd7T7AoI4dlO/I4XlzBfzYc5D8bDtLW34vxiZFMTI5kWLcwvD1VKGsNVBxrKrX9xjRqTETErVRVVVFZWWl1DLlMXl5eeHh4WB3DJXh7ezNgwADS0tKYOnUqYDbkT0tL47777rvkz6murnY004+NjSUyMpK0tDRHMaywsJANGzZwzz33NPUpiIirstshqo+5Xf1rOHPSLHrtqelPduYEZH5ibgCRvetWwNz+oflcwhRNqZRL5uPpweie4YzuGc4fqqpZn3WCxdtyWLo9j5MlFbzz9SHe+foQQb6ejEuMZFJyJCPiw/Dx1D2Fu1JxrClUVcLeNHO/+wRrs4iISJMwDIPc3FxOnz5tdRS5QiEhIURGRmoZ90swc+ZMpk2bxsCBAxk8eDBz586lpKSEO+64A4Af//jHdOjQgTlz5gBm4/yBAwfSrVs3ysvLWbRoEf/617949dVXAbDZbMyYMYOnn36a+Ph4YmNjefTRR4mOjnYU4EREzuMfCsk3mlt1NRzdUjOqbJm5n7vV3D5/tu49Cddal1dcmpeHnau7t+fq7u156rpqNmafZNG2HJZsy+N4cTnvbz7M+5sPE+jjydiEcCYmRzGye3t8vVQocycqjjWFg19CeQH4h0EHDeUVEXEHtYWx8PBw/P39VVhxQYZhcObMGfLz8wHqTeuTC7v55ps5duwYs2fPJjc3l759+7JkyRJHQ/2DBw9it9dNLykpKeHee+/l8OHD+Pn50bNnT9566y1uvvlmxzEPPfQQJSUl/OxnP+P06dOMGDGCJUuW4Ovr2+LnJyIuyG6HjgPMbfQsKD4GWWk1jf3ToOy0uUplzBCrk4ob8PSwMywujGFxYTxxbRJf7z/J4m25LN6WQ15hOR+lH+Wj9KP4e3swpmc4k5OjGNUjHD9vFcpcnc0wDMPqEE2hsLCQ4OBgCgoKCAoKatlvvvS3sP5l6PP/4PpXW/Z7i4hIk6uqqmL37t2Eh4fTrl07q+PIFTpx4gT5+fl07979vCmWlt4/yCXTdRKRC6o6a44gC+oAgRHffbzIZaquNthy6BSLMnJZnJHD0YIyx2t+Xh6M7tmeiUlRjOkZToCPxiA5i8bcP+iqNYXa1VHUb0xExC3U9hjz9/e3OIk0hdrrWFlZqf5jIiLuxMNTM3ekRdjtNgZ0DmVA51B+NzmBbw4XsDgjh4UZORw+VcqijFwWZeTi42lnZPf2TEqOYkxCOEG+DayyKk5HxbErdSILTuwBuyd0G2N1GhERaUKaSukedB1FRESkqdhsNvrGhNA3JoRHJvZk+9FCFmXksCgjh/0nzrBsRx7LduTh7WHnqvgwJiZHMS4hgmB/FcqcmYpjV6p21Fjn4eCrYf4iIiIiIiIirYHNZiOpQzBJHYL5dWoPduYWOUaUZR0rIW1nPmk78/G02xgeF8ak5EjGJUYSGuBtdXT5FhXHrtSe2imVWqVSREREREREpDWy2WwkRAWREBXEzPE92J1XxKKMHBZn5LIrr4jVu4+xevcxfvPhNoZ2bcfE5EhSe0US1sbH6ugC2L/7EGlQWSHsX2fuq9+YiIi4mS5dujB37twm+axVq1Zhs9k4ffp0k3yeiIiIiDPrHhHIjJTuLP3l1Xw2cyQPju9OYlQQVdUGa/ce57cfbmPw7z/jlv9bz5vr95NfWPbdHyrNRiPHrsS+lVBdCe3ioF03q9OIiIgwatQo+vbt2yRFra+++oqAgIArDyUiIiLSisWFt+G+MfHcNyae/cdLWLwtl8Xbcth6uIAv953ky30neeyT7Qzs3JaJSVFMTI4kKtjP6titiopjV2K3plSKiIhrMQyDqqoqPD2/+xagffv2LZBIREREpPXoEhbAPaO6cc+obhw6eYYl23JZtC2HLQdP89X+U3y1/xRPLthB/04hTEqOYkJSJB3bagX15qZplZeruvqc4pimVIqIuDPDMDhTcdaSzTCMS855++23s3r1al588UVsNhs2m4358+djs9lYvHgxAwYMwMfHh7Vr15KVlcV1111HREQEbdq0YdCgQXz22Wf1Pu/b0yptNht///vfuf766/H39yc+Pp5PPvnksv+5vv/++/Tq1QsfHx+6dOnCc889V+/1V155hfj4eHx9fYmIiODGG290vPa///2P5ORk/Pz8aNeuHSkpKZSUlFx2FhEREZGWFhPqz11Xd+XDe4fzxSNjmH1NIoO6tMVmg80HT/P0wkxG/Gkl1728ltdWZ3HghO51motGjl2uo5vhzHHwCYJOQ61OIyIizai0sorE2Ust+d47nkzF3/vS/nf94osvsnv3bpKSknjyyScB2L59OwCPPPIIzz77LF27dqVt27YcOnSISZMm8fvf/x4fHx/efPNNpkyZwq5du+jUqVOD3+OJJ57gmWee4c9//jMvvfQSt912GwcOHCA0NLRR57Vp0yZuuukmHn/8cW6++Wa++OIL7r33Xtq1a8ftt9/O119/zS9+8Qv+9a9/MWzYME6ePMnnn38OQE5ODrfeeivPPPMM119/PUVFRXz++eeNKiSKiIiIOJPoED9+MiKWn4yIJa+wjKXbc1mUkcPG7JN8c7iAbw4X8MfFO+kVHcSk5CgmJkXStX0bq2O7DRXHLtfuJeZj3Fjw8LI2i4iICBAcHIy3tzf+/v5ERkYCsHPnTgCefPJJxo0b5zg2NDSUPn36OL5+6qmn+PDDD/nkk0+47777Gvwet99+O7feeisAf/jDH/jLX/7Cxo0bmTChcS0Gnn/+ecaOHcujjz4KQPfu3dmxYwd//vOfuf322zl48CABAQFcc801BAYG0rlzZ/r16weYxbGzZ8/y/e9/n86dOwOQnJzcqO8vIiIi4qwignz58dAu/HhoF44VlbNsRy6LM3JZv+8E248Wsv1oIX9euouekYFMTIpiUnIk8RGBVsd2aSqOXa7a4pj6jYmIuD0/Lw92PGnNFHo/L48m+ZyBAwfW+7q4uJjHH3+chQsXOopNpaWlHDx48KKf07t3b8d+QEAAQUFB5OfnNzpPZmYm1113Xb3nhg8fzty5c6mqqmLcuHF07tyZrl27MmHCBCZMmOCYztmnTx/Gjh1LcnIyqampjB8/nhtvvJG2bds2OoeIiIiIM2sf6MNtQzpz25DOnCypYPmOXBZl5LJu73F25haxM7eIFz7bTVx4GyYlRTIxOYqekYHYbDaro7sUFccuR8ERyM0AbBA37jsPFxER12az2S55aqOz+vaqkw8++CDLly/n2WefJS4uDj8/P2688UYqKiou+jleXvVHS9tsNqqrq5s8b2BgIJs3b2bVqlUsW7aM2bNn8/jjj/PVV18REhLC8uXL+eKLL1i2bBkvvfQSv/3tb9mwYQOxsbFNnkVERETEGYQGeHPzoE7cPKgTBWcqWZ6Zx+KMHD7fc5y9+cX8ZcVe/rJiL7FhAUxMimRSchS9ooNUKLsEash/OfbU9J2JGQwB7azNIiIicg5vb2+qqqq+87h169Zx++23c/3115OcnExkZCT79+9v/oA1EhISWLdu3XmZunfvjoeHOVrO09OTlJQUnnnmGbZu3cr+/ftZsWIFYBblhg8fzhNPPMGWLVvw9vbmww8/bLH8IiIiIlYK9vfixgEd+cftg/j60RTm3tyX8YkReHvayT5ewiursrjmpbVc/eeVzFmUSfqh0+rPehGu/Wdwq2iVShERcVJdunRhw4YN7N+/nzZt2jQ4qis+Pp4PPviAKVOmYLPZePTRR5tlBFhDfvWrXzFo0CCeeuopbr75ZtavX8/LL7/MK6+8AsCCBQvYt28fV199NW3btmXRokVUV1fTo0cPNmzYQFpaGuPHjyc8PJwNGzZw7NgxEhISWiy/iIiIiLMI8vViar8OTO3XgeLys6zcmc/ibTms2JnPoZOl/HXNPv66Zh8dQvyYkBTJpORI+sW0xW7XiLJaKo41VsUZ2LfK3Fe/MRERcTIPPvgg06ZNIzExkdLSUl5//fULHvf888/zk5/8hGHDhhEWFsbDDz9MYWFhi+Xs378/7777LrNnz+app54iKiqKJ598kttvvx2AkJAQPvjgAx5//HHKysqIj4/nv//9L7169SIzM5M1a9Ywd+5cCgsL6dy5M8899xwTJ05ssfwiIiIizqiNjydT+kQzpU80ZyrOsnrXMRZtyyUtM48jp0v5x9ps/rE2m4ggHyYmmateDuwSikcrL5TZDDcZV1dYWEhwcDAFBQUEBQU13zfavRT+cxMEx8CMDNDcXRERt1NWVkZ2djaxsbH4+vpaHUeu0MWuZ4vdP8gV0XUSERG5MmWVVazZfYzF23L5bEceReVnHa+FtfFhQlIEk5KjGNwlFE8P9+jA1Zj7B40cayzHKpWpKoyJiIiIiIiIiNPz9fJgfK9IxveKpPxsFev2HmdRRi7LtudyvLict748yFtfHqRdgDfje5lTL7/XtR1eblIo+y4qjjWGYZzTb0xTKkVERGrdfffdvPXWWxd87Yc//CGvvfZaCycSERERkQvx8fRgTM8IxvSMoOL6ZNbvO8HijByWbs/lREkF/914kP9uPEiIvxfjEyOYmBzF8G5heHu6b6FMxbHGyNsGhUfAyx+6XGV1GhEREafx5JNP8uCDD17wNU2DExEREXFO3p52RnZvz8ju7Xl6ahJf7jvJom05LN1mFsre/fow7359mEBfT8YlRjApKYoR8WH4enlYHb1JqTjWGLVTKruOAi/1oBEREakVHh5OeHi41TFERERE5DJ5etgZER/GiPgwnrouiY3ZJ1m8LYfF23I5VlTOB5uP8MHmI7Tx8WRsQjgTk6IY1aO9WxTKVBxrDMeUylRrc4iIiIiIiIiINBMPu42h3doxtFs7Hp/Si00HT7EoI4fFGbnkFpbxcfpRPk4/ir+3B6N7hjMpKYrRPdvj7+2aZaZGTxhds2YNU6ZMITo6GpvNxkcfffSd71m1ahX9+/fHx8eHuLg45s+ff94x8+bNo0uXLvj6+jJkyBA2btzY2GjNq+Q4HP7a3I8fb20WEREREREREZEWYLfbGNQllMem9OKLR8bwwb3DuOuqWDqE+HGmooqFW3OY/p/N9H9qOXf/axMfpx+h+JzVMF1Bo4tjJSUl9OnTh3nz5l3S8dnZ2UyePJnRo0eTnp7OjBkzuPPOO1m6dKnjmHfeeYeZM2fy2GOPsXnzZvr06UNqair5+fmNjdd89iwHDIjqA0HRVqcREREREREREWlRdruN/p3a8tvJiax9eDSf3Decu0d2o1OoP2WV1SzZnssDb6fT/6nl3PnG13yw+TAFpZVWx/5ONsMwjMt+s83Ghx9+yNSpUxs85uGHH2bhwoVs27bN8dwtt9zC6dOnWbLE7OE1ZMgQBg0axMsvvwxAdXU1MTEx3H///TzyyCMX/Nzy8nLKy8sdXxcWFhITE0NBQUHzNP59dxrs+AhGPgyjf9P0ny8iIk6jrKyM7OxsYmNj8fVVj0lXd7HrWVhYSHBwcPPdP0iT0HUSERFxboZhsCOnkMUZuSzKyGHf8RLHa14eNkbEhTExOYrxiRGE+Hu3SKbG3D80+2TQ9evXk5KSUu+51NRUZsyYAUBFRQWbNm1i1qxZjtftdjspKSmsX7++wc+dM2cOTzzxRLNkPs/ZCshaYe7Hq9+YiIiIiIiIiEgtm81Gr+hgekUH86vx3dmdV2z2KNuWw+68YlbuOsbKXcf4TU0vs0k1hbJ2bXysjg5cxrTKxsrNzSUiIqLecxERERQWFlJaWsrx48epqqq64DG5ubkNfu6sWbMoKChwbIcOHWqW/AAcXA/lhRDQHqL7Nd/3ERERcXH79+/HZrORnp5udRQRERERsYDNZqNHZCC/HNedZb8cyWczr+ZX47qTEBXE2WqDz/ccZ9YHGQz6/Wf8v799ye68Iqsju+5qlT4+Pvj4tFCFcc8y8zE+FezNXk8UERG5bKNGjaJv377MnTu3ST7v9ttv5/Tp05e0AI+IiIiIyLfFhQdy/9hA7h8bT/bxEhZvM1e9zDhSwPp9Jwjx97I6YvMXxyIjI8nLy6v3XF5eHkFBQfj5+eHh4YGHh8cFj4mMjGzueJdm7GzoNgYCwqxOIiIiIiIiIiLikmLDArh3VBz3jorj0MkzbDpwivBA63v8NvswqKFDh5KWllbvueXLlzN06FAAvL29GTBgQL1jqqurSUtLcxxjOU8fiBtrrlQpIiKtj2FARYk1WyPWzbn99ttZvXo1L774IjabDZvNxv79+9m2bRsTJ06kTZs2RERE8KMf/Yjjx4873ve///2P5ORk/Pz8aNeuHSkpKZSUlPD444/zxhtv8PHHHzs+b9WqVY3+x7d69WoGDx6Mj48PUVFRPPLII5w9W7e8d0PfH2DVqlUMHjyYgIAAQkJCGD58OAcOHGh0BhERERFxLjGh/kzt18HqGMBljBwrLi5m7969jq+zs7NJT08nNDSUTp06MWvWLI4cOcKbb74JwN13383LL7/MQw89xE9+8hNWrFjBu+++y8KFCx2fMXPmTKZNm8bAgQMZPHgwc+fOpaSkhDvuuKMJTlFEROQKVZ6BP0Rb871/cxS8Ay7p0BdffJHdu3eTlJTEk08+CYCXlxeDBw/mzjvv5IUXXqC0tJSHH36Ym266iRUrVpCTk8Ott97KM888w/XXX09RURGff/45hmHw4IMPkpmZSWFhIa+//joAoaGhjYp/5MgRJk2axO23386bb77Jzp07ueuuu/D19eXxxx+/6Pc/e/YsU6dO5a677uK///0vFRUVbNy4EZvN1rh/hiIiIiIiF9Ho4tjXX3/N6NGjHV/PnDkTgGnTpjF//nxycnI4ePCg4/XY2FgWLlzIL3/5S1588UU6duzI3//+d1JT61Z9vPnmmzl27BizZ88mNzeXvn37smTJkvOa9IuIiEjDgoOD8fb2xt/f39Ga4Omnn6Zfv3784Q9/cBz3z3/+k5iYGHbv3k1xcTFnz57l+9//Pp07dwYgOTnZcayfnx/l5eWX3erglVdeISYmhpdffhmbzUbPnj05evQoDz/8MLNnzyYnJ6fB73/y5EkKCgq45ppr6NatGwAJCQmXlUNEREREpCGNLo6NGjUK4yJTPObPn3/B92zZsuWin3vfffdx3333NTaOiIhI8/PyN0dwWfW9r8A333zDypUradOmzXmvZWVlMX78eMaOHUtycjKpqamMHz+eG2+8kbZt217R962VmZnJ0KFD6432Gj58OMXFxRw+fJg+ffo0+P1DQ0O5/fbbSU1NZdy4caSkpHDTTTcRFRXVJNlERERERKAFeo6JiIi4PJvNnNpoxXaFUwiLi4uZMmUK6enp9bY9e/Zw9dVX4+HhwfLly1m8eDGJiYm89NJL9OjRg+zs7Cb6h3dx3/X9X3/9ddavX8+wYcN455136N69O19++WWLZBMRERGR1kHFMRERETfi7e1NVVWV4+v+/fuzfft2unTpQlxcXL0tIMDsZWaz2Rg+fDhPPPEEW7Zswdvbmw8//PCCn9dYCQkJrF+/vt6o83Xr1hEYGEjHjh2/8/sD9OvXj1mzZvHFF1+QlJTEf/7zn8vOIyIiIiLybSqOiYiIuJEuXbqwYcMG9u/fz/Hjx5k+fTonT57k1ltv5auvviIrK4ulS5dyxx13UFVVxYYNG/jDH/7A119/zcGDB/nggw84duyYo7dXly5d2Lp1K7t27eL48eNUVlY2Ks+9997LoUOHuP/++9m5cycff/wxjz32GDNnzsRut1/0+2dnZzNr1izWr1/PgQMHWLZsGXv27FHfMRERERFpUo3uOSYiIiLO68EHH2TatGkkJiZSWlpKdnY269at4+GHH2b8+PGUl5fTuXNnJkyYgN1uJygoiDVr1jB37lwKCwvp3Lkzzz33HBMnTgTgrrvuYtWqVQwcOJDi4mJWrlzJqFGjLjlPhw4dWLRoEb/+9a/p06cPoaGh/PSnP+V3v/sdwEW/f15eHjt37uSNN97gxIkTREVFMX36dH7+8583xz86EREREWmlbMbFuuu7kMLCQoKDgykoKCAoKMjqOCIi4sLKysrIzs4mNjYWX19fq+PIFbrY9dT9g2vQdRIREZHGasz9g6ZVioiIiIiIiIhIq6XimIiIiFyyP/zhD7Rp0+aCW+1UTBERERERV6KeYyIiInLJ7r77bm666aYLvubn59fCaURERERErpyKYyIiInLJQkNDCQ0NtTqGiIiIiEiT0bRKERGRBrjJmjWtnq6jiIiIiFyMimMiIiLf4uXlBcCZM2csTiJNofY61l5XEREREZFzaVqliIjIt3h4eBASEkJ+fj4A/v7+2Gw2i1NJYxmGwZkzZ8jPzyckJAQPDw+rI4mIiIiIE1JxTERE5AIiIyMBHAUycV0hISGO6ykiIiIi8m0qjomIiFyAzWYjKiqK8PBwKisrrY4jl8nLy0sjxkRERETkolQcExERuQgPDw8VV0RERERE3Jga8ouIiIiIiIiISKul4piIiIiIiIiIiLRaKo6JiIiIiIiIiEir5TY9xwzDAKCwsNDiJCIiIuIqau8bau8jxDnpPk9EREQaqzH3eW5THCsqKgIgJibG4iQiIiLiaoqKiggODrY6hjRA93kiIiJyuS7lPs9muMmfSqurqzl69CiBgYHYbLYm//zCwkJiYmI4dOgQQUFBTf75zkbn6950vu5N5+vedL5NyzAMioqKiI6Oxm5Xtwlnpfu8pqXzdW86X/em83VvOt+m1Zj7PLcZOWa32+nYsWOzf5+goKBW8S9pLZ2ve9P5ujedr3vT+TYdjRhzfrrPax46X/em83VvOl/3pvNtOpd6n6c/kYqIiIiIiIiISKul4piIiIiIiIiIiLRaKo5dIh8fHx577DF8fHysjtIidL7uTefr3nS+7k3nK9L0Wtu/Zzpf96bzdW86X/em87WO2zTkFxERERERERERaSyNHBMRERERERERkVZLxTEREREREREREWm1VBwTEREREREREZFWS8UxERERERERERFptVQcO8e8efPo0qULvr6+DBkyhI0bN170+Pfee4+ePXvi6+tLcnIyixYtaqGkTaMx5zt//nxsNlu9zdfXtwXTXr41a9YwZcoUoqOjsdlsfPTRR9/5nlWrVtG/f398fHyIi4tj/vz5zZ6zqTT2fFetWnXetbXZbOTm5rZM4Cs0Z84cBg0aRGBgIOHh4UydOpVdu3Z95/tc9ef3cs7XlX9+X331VXr37k1QUBBBQUEMHTqUxYsXX/Q9rnptofHn68rX9kL++Mc/YrPZmDFjxkWPc+VrLNbRfZ7u82rpPk/3ec5K93m6zzuXK1/bC3H2+zwVx2q88847zJw5k8cee4zNmzfTp08fUlNTyc/Pv+DxX3zxBbfeeis//elP2bJlC1OnTmXq1Kls27athZNfnsaeL0BQUBA5OTmO7cCBAy2Y+PKVlJTQp08f5s2bd0nHZ2dnM3nyZEaPHk16ejozZszgzjvvZOnSpc2ctGk09nxr7dq1q971DQ8Pb6aETWv16tVMnz6dL7/8kuXLl1NZWcn48eMpKSlp8D2u/PN7OecLrvvz27FjR/74xz+yadMmvv76a8aMGcN1113H9u3bL3i8K19baPz5gute22/76quv+Otf/0rv3r0vepyrX2Oxhu7zdJ9XS/d5us9zZrrP033et7nqtf02l7jPM8QwDMMYPHiwMX36dMfXVVVVRnR0tDFnzpwLHn/TTTcZkydPrvfckCFDjJ///OfNmrOpNPZ8X3/9dSM4OLiF0jUfwPjwww8vesxDDz1k9OrVq95zN998s5GamtqMyZrHpZzvypUrDcA4depUi2Rqbvn5+QZgrF69usFjXP3n91yXcr7u8vNbq23btsbf//73C77mTte21sXO112ubVFRkREfH28sX77cGDlypPHAAw80eKw7XmNpfrrP031eLd3nuTbd553PXX5+a+k+r467XFtXuc/TyDGgoqKCTZs2kZKS4njObreTkpLC+vXrL/ie9evX1zseIDU1tcHjncnlnC9AcXExnTt3JiYm5jsr3K7Mla/tlejbty9RUVGMGzeOdevWWR3nshUUFAAQGhra4DHudI0v5XzBPX5+q6qqePvttykpKWHo0KEXPMadru2lnC+4x7WdPn06kydPPu/aXYg7XWNpGbrP033euVz52l4J3ee55jXWfV597nRtdZ93YVZeYxXHgOPHj1NVVUVERES95yMiIhqcj5+bm9uo453J5Zxvjx49+Oc//8nHH3/MW2+9RXV1NcOGDePw4cMtEblFNXRtCwsLKS0ttShV84mKiuK1117j/fff5/333ycmJoZRo0axefNmq6M1WnV1NTNmzGD48OEkJSU1eJwr//ye61LP19V/fjMyMmjTpg0+Pj7cfffdfPjhhyQmJl7wWHe4to05X1e/tgBvv/02mzdvZs6cOZd0vDtcY2lZus8z6T7PpPs83ee5Ct3nnc8drq3u8y7Oymvs2ezfQdzC0KFD61W0hw0bRkJCAn/961956qmnLEwmV6pHjx706NHD8fWwYcPIysrihRde4F//+peFyRpv+vTpbNu2jbVr11odpUVc6vm6+s9vjx49SE9Pp6CggP/9739MmzaN1atXN3gj4eoac76ufm0PHTrEAw88wPLly126wayIq3P1/5ZIw3Sf57p0n6f7PFe/tq52n6fiGBAWFoaHhwd5eXn1ns/LyyMyMvKC74mMjGzU8c7kcs7327y8vOjXrx979+5tjoiWaujaBgUF4efnZ1GqljV48GCXu/G47777WLBgAWvWrKFjx44XPdaVf35rNeZ8v83Vfn69vb2Ji4sDYMCAAXz11Ve8+OKL/PWvfz3vWHe4to05329ztWu7adMm8vPz6d+/v+O5qqoq1qxZw8svv0x5eTkeHh713uMO11halu7zTLrPM+k+T/d5rkD3ebrPuxBXu7audp+naZWY/4IOGDCAtLQ0x3PV1dWkpaU1OP936NCh9Y4HWL58+UXnCzuLyznfb6uqqiIjI4OoqKjmimkZV762TSU9Pd1lrq1hGNx33318+OGHrFixgtjY2O98jytf48s5329z9Z/f6upqysvLL/iaK1/bhlzsfL/N1a7t2LFjycjIID093bENHDiQ2267jfT09PNumMA9r7E0L93n6T7vXK58bZuK7vOcl+7zdJ93Ma52bV3uPq/ZW/67iLffftvw8fEx5s+fb+zYscP42c9+ZoSEhBi5ubmGYRjGj370I+ORRx5xHL9u3TrD09PTePbZZ43MzEzjscceM7y8vIyMjAyrTqFRGnu+TzzxhLF06VIjKyvL2LRpk3HLLbcYvr6+xvbt2606hUtWVFRkbNmyxdiyZYsBGM8//7yxZcsW48CBA4ZhGMYjjzxi/OhHP3Icv2/fPsPf39/49a9/bWRmZhrz5s0zPDw8jCVLllh1Co3S2PN94YUXjI8++sjYs2ePkZGRYTzwwAOG3W43PvvsM6tOoVHuueceIzg42Fi1apWRk5Pj2M6cOeM4xp1+fi/nfF355/eRRx4xVq9ebWRnZxtbt241HnnkEcNmsxnLli0zDMO9rq1hNP58XfnaNuTbqxi52zUWa+g+T/d5tXSfp/s8Z6b7PN3nucu1bYgz3+epOHaOl156yejUqZPh7e1tDB482Pjyyy8dr40cOdKYNm1avePfffddo3v37oa3t7fRq1cvY+HChS2c+Mo05nxnzJjhODYiIsKYNGmSsXnzZgtSN17tEtbf3mrPb9q0acbIkSPPe0/fvn0Nb29vo2vXrsbrr7/e4rkvV2PP909/+pPRrVs3w9fX1wgNDTVGjRplrFixwprwl+FC5wrUu2bu9PN7Oefryj+/P/nJT4zOnTsb3t7eRvv27Y2xY8c6biAMw72urWE0/nxd+do25Ns3Te52jcU6us/Tfd6579F9nmvQfZ7u89zl2hqG7vMMw7nv82yGYRhNPx5NRERERERERETE+annmIiIiIiIiIiItFoqjomIiIiIiIiISKul4piIiIiIiIiIiLRaKo6JiIiIiIiIiEirpeKYiIiIiIiIiIi0WiqOiYiIiIiIiIhIq6XimIiIiIiIiIiItFoqjomIiIiIiIiISKul4piISA2bzcZHH31kdQwRERERaWK6zxORi1FxTEScwu23347NZjtvmzBhgtXRREREROQK6D5PRJydp9UBRERqTZgwgddff73ecz4+PhalEREREZGmovs8EXFmGjkmIk7Dx8eHyMjIelvbtm0Bcyj8q6++ysSJE/Hz86Nr167873//q/f+jIwMxowZg5+fH+3ateNnP/sZxcXF9Y755z//Sa9evfDx8SEqKor77ruv3uvHjx/n+uuvx9/fn/j4eD755BPHa6dOneK2226jffv2+Pn5ER8ff95NnoiIiIicT/d5IuLMVBwTEZfx6KOPcsMNN/DNN99w2223ccstt5CZmQlASUkJqamptG3blq+++or33nuPzz77rN5N0auvvsr06dP52c9+RkZGBp988glxcXH1vscTTzzBTTfdxNatW5k0aRK33XYbJ0+edHz/HTt2sHjxYjIzM3n11VcJCwtruX8AIiIiIm5K93kiYilDRMQJTJs2zfDw8DACAgLqbb///e8NwzAMwLj77rvrvWfIkCHGPffcYxiGYfzf//2f0bZtW6O4uNjx+sKFCw273W7k5uYahmEY0dHRxm9/+9sGMwDG7373O8fXxcXFBmAsXrzYMAzDmDJlinHHHXc0zQmLiIiItBK6zxMRZ6eeYyLiNEaPHs2rr75a77nQ0FDH/tChQ+u9NnToUNLT0wHIzMykT58+BAQEOF4fPnw41dXV7Nq1C5vNxtGjRxk7duxFM/Tu3duxHxAQQFBQEPn5+QDcc8893HDDDWzevJnx48czdepUhg0bdlnnKiIiItKa6D5PRJyZimMi4jQCAgLOG/7eVPz8/C7pOC8vr3pf22w2qqurAZg4cSIHDhxg0aJFLF++nLFjxzJ9+nSeffbZJs8rIiIi4k50nycizkw9x0TEZXz55ZfnfZ2QkABAQkIC33zzDSUlJY7X161bh91up0ePHgQGBtKlSxfS0tKuKEP79u2ZNm0ab731FnPnzuX//u//rujzRERERET3eSJiLY0cExGnUV5eTm5ubr3nPD09Hc1Q33vvPQYOHMiIESP497//zcaNG/nHP/4BwG233cZjjz3GtGnTePzxxzl27Bj3338/P/rRj4iIiADg8ccf5+677yY8PJyJEydSVFTEunXruP/++y8p3+zZsxkwYAC9evWivLycBQsWOG7aRERERKRhus8TEWem4piIOI0lS5YQFRVV77kePXqwc+dOwFxh6O233+bee+8lKiqK//73vyQmJgLg7+/P0qVLeeCBBxg0aBD+/v7ccMMNPP/8847PmjZtGmVlZbzwwgs8+OCDhIWFceONN15yPm9vb2bNmsX+/fvx8/Pjqquu4u23326CMxcRERFxb7rPExFnZjMMw7A6hIjId7HZbHz44YdMnTrV6igiIiIi0oR0nyciVlPPMRERERERERERabVUHBMRERERERERkVZL0ypFRERERERERKTV0sgxERERERERERFptVQcExERERERERGRVkvFMRERERERERERabVUHBMRERERERERkVZLxTEREREREREREWm1VBwTEREREREREZFWS8UxERERERERERFptVQcExERERERERGRVuv/A8grE5fcStqoAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Those (COURSE curves, not these!!) are some excellent looking loss curves!\n",
        "\n",
        "It looks like the loss for both datasets (train and test) is heading in the right direction.\n",
        "\n",
        "The same with the accuracy values, trending upwards.\n",
        "\n",
        "That goes to show the power of **transfer learning**. Using a pretrained model often leads to pretty good results with a small amount of data in less time.\n",
        "\n",
        "I wonder what would happen if you tried to train the model for longer? Or if we added more data?\n",
        "\n",
        "> **Question:** Looking at the loss curves, does our model look like it's overfitting or underfitting? Or perhaps neither? Hint: Check out notebook [04. PyTorch Custom Datasets part 8. What should an ideal loss curve look like?](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like) for ideas."
      ],
      "metadata": {
        "id": "qDoSGDiCCe32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "puS8gR4kDGsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Make predictions on images from the test set\n",
        "\n",
        "It looks like our model performs well quantitatively but how about qualitatively?\n",
        "\n",
        "Let's find out by making some predictions with our model on images from the test set (these aren't seen during training) and plotting them.\n",
        "\n",
        "*Visualize, visualize, visualize!*\n",
        "\n",
        "One thing we'll have to remember is that for our model to make predictions on an image, the image has to be in *same* format as the images our model was trained on.\n",
        "\n",
        "This means we'll need to make sure our images have:\n",
        "* **Same shape** - If our images are different shapes to what our model was trained on, we'll get shape errors.\n",
        "* **Same datatype** - If our images are a different datatype (e.g. `torch.int8` vs. `torch.float32`) we'll get datatype errors.\n",
        "* **Same device** - If our images are on a different device to our model, we'll get device errors.\n",
        "* **Same transformations** - If our model is trained on images that have been transformed in certain way (e.g. normalized with a specific mean and standard deviation) and we try and make preidctions on images transformed in a different way, these predictions may be off.\n",
        "\n",
        "> **Note:** These requirements go for all kinds of data if you're trying to make predictions with a trained model. Data you'd like to predict on should be in the same format as your model was trained on.\n",
        "\n",
        "To do all of this, we'll create a function `pred_and_plot_image()` to:\n",
        "\n",
        "1. Take in a trained model, a list of class names, a filepath to a target image, an image size, a transform and a target device.\n",
        "2. Open an image with [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.open).\n",
        "3. Create a transform for the image (this will default to the `manual_transforms` we created above or it could use a transform generated from `weights.transforms()`).\n",
        "4. Make sure the model is on the target device.\n",
        "5. Turn on model eval mode with `model.eval()` (this turns off layers like `nn.Dropout()`, so they aren't used for inference) and the inference mode context manager.\n",
        "6. Transform the target image with the transform made in step 3 and add an extra batch dimension with `torch.unsqueeze(dim=0)` so our input image has shape `[batch_size, color_channels, height, width]`.\n",
        "7. Make a prediction on the image by passing it to the model ensuring it's on the target device.\n",
        "8. Convert the model's output logits to prediction probabilities with `torch.softmax()`.\n",
        "9. Convert model's prediction probabilities to prediction labels with `torch.argmax()`.\n",
        "10. Plot the image with `matplotlib` and set the title to the prediction label from step 9 and prediction probability from step 8.\n",
        "\n",
        "> **Note:** This is a similar function to [04. PyTorch Custom Datasets section 11.3's](https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function) `pred_and_plot_image()` with a few tweaked steps."
      ],
      "metadata": {
        "id": "fefQkGlnCezb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Take in a trained model, class names, image path, image size, a transform and target device\n",
        "def pred_and_plot_image(model: torch.nn.Module,\n",
        "                        image_path: str,\n",
        "                        class_names: List[str],\n",
        "                        image_size: Tuple[int, int] = (224, 224),\n",
        "                        transform: torchvision.transforms = None,\n",
        "                        device: torch.device=device):\n",
        "\n",
        "\n",
        "    # 2. Open image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # 3. Create transformation for image (if one doesn't exist)\n",
        "    if transform is not None:\n",
        "        image_transform = transform\n",
        "    else:\n",
        "        image_transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    ### Predict on image ###\n",
        "\n",
        "    # 4. Make sure the model is on the target device\n",
        "    model.to(device)\n",
        "\n",
        "    # 5. Turn on model evaluation mode and inference mode\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
        "      transformed_image = image_transform(img).unsqueeze(dim=0)\n",
        "\n",
        "      # 7. Make a prediction on image with an extra dimension and send it to the target device\n",
        "      target_image_pred = model(transformed_image.to(device))\n",
        "\n",
        "    # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "    # 9. Convert prediction probabilities -> prediction labels\n",
        "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "    # 10. Plot image with predicted label and probability\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n",
        "    plt.axis(False);"
      ],
      "metadata": {
        "id": "_lsKCgBuCeu3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What a good looking function!\n",
        "\n",
        "Let's test it out by making predictions on a few random images from the test set.\n",
        "\n",
        "We can get a list of all the test image paths using `list(Path(test_dir).glob(\"*/*.jpg\"))`, the stars in the `glob()` method say \"any file matching this pattern\", in other words, any file ending in `.jpg` (all of our images).\n",
        "\n",
        "And then we can randomly sample a number of these using Python's [`random.sample(populuation, k)`](https://docs.python.org/3/library/random.html#random.sample) where `population` is the sequence to sample and `k` is the number of samples to retrieve."
      ],
      "metadata": {
        "id": "eic_bJLODo6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random list of image paths from test set\n",
        "import random\n",
        "num_images_to_plot = 3\n",
        "test_image_path_list = list(Path(test_dir).glob(\"*/*.png\")) # get list all image paths from test data\n",
        "test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n",
        "                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n",
        "\n",
        "# Make predictions on and plot the images\n",
        "for image_path in test_image_path_sample:\n",
        "    pred_and_plot_image(model=model,\n",
        "                        image_path=image_path,\n",
        "                        class_names=class_names,\n",
        "                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
        "                        image_size=(224, 224))"
      ],
      "metadata": {
        "id": "F37OglRxCemu",
        "outputId": "1a2eec5a-d29d-4e74-f74d-c535ac52db1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-26b4b43a0fcf>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Make predictions on and plot the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_image_path_sample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     pred_and_plot_image(model=model,\n\u001b[0m\u001b[1;32m     11\u001b[0m                         \u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-226face381d4>\u001b[0m in \u001b[0;36mpred_and_plot_image\u001b[0;34m(model, image_path, class_names, image_size, transform, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m# 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mtransformed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m# 7. Make a prediction on image with an extra dimension and send it to the target device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 224, 224] doesn't match the broadcast shape [3, 224, 224]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Making predictions on a custom image\n",
        "\n",
        "It looks like our model does well qualitatively on data from the test set.\n",
        "\n",
        "But how about on our own custom image?\n",
        "\n",
        "That's where the real fun of machine learning is!\n",
        "\n",
        "Predicting on your own custom data, outisde of any training or test set.\n",
        "\n",
        "To test our model on a custom image, let's import the old faithful `pizza-dad.jpeg` image (an image of my dad eating pizza).\n",
        "\n",
        "We'll then pass it to the `pred_and_plot_image()` function we created above and see what happens."
      ],
      "metadata": {
        "id": "q3wHjLmwD286"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download custom image\n",
        "import requests\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n",
        "\n",
        "# Download the image if it doesn't already exist\n",
        "if not custom_image_path.is_file():\n",
        "    with open(custom_image_path, \"wb\") as f:\n",
        "        # When downloading from GitHub, need to use the \"raw\" file link\n",
        "        #request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
        "        print(f\"Downloading {custom_image_path}...\")\n",
        "        f.write(request.content)\n",
        "else:\n",
        "    print(f\"{custom_image_path} already exists, skipping download.\")\n",
        "\n",
        "# Predict on custom image\n",
        "pred_and_plot_image(model=model,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names)"
      ],
      "metadata": {
        "id": "6EF2D3VdEEaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two thumbs up!\n",
        "\n",
        "Looks like our model got it right again!\n",
        "\n",
        "But this time the prediction probability is higher than the one from TinyVGG (`0.373`) in [04. PyTorch Custom Datasets section 11.3](https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function).\n",
        "\n",
        "This indicates our `efficientnet_b0` model is *more* confident in its prediction where as our TinyVGG model was par with just guessing."
      ],
      "metadata": {
        "id": "x82qdEYIELB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main takeaways\n",
        "* **Transfer learning** often allows to you get good results with a relatively small amount of custom data.\n",
        "* Knowing the power of transfer learning, it's a good idea to ask at the start of every problem, \"does an existing well-performing model exist for my problem?\"\n",
        "* When using a pretrained model, it's important that your custom data be formatted/preprocessed in the same way that the original model was trained on, otherwise you may get degraded performance.\n",
        "* The same goes for predicting on custom data, ensure your custom data is in the same format as the data your model was trained on.\n",
        "* There are [several different places to find pretrained models](https://www.learnpytorch.io/06_pytorch_transfer_learning/#where-to-find-pretrained-models) from the PyTorch domain libraries, HuggingFace Hub and libraries such as `timm` (PyTorch Image Models)."
      ],
      "metadata": {
        "id": "K4NMSM5KER_n"
      }
    }
  ]
}